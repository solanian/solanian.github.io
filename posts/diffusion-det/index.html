<!DOCTYPE html> <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}}); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> </script><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="DiffusionDet 코드까지 깊게 살펴보기" /><meta property="og:locale" content="en" /><meta name="description" content="DiffusionDet: Diffusion Model for Object Detection" /><meta property="og:description" content="DiffusionDet: Diffusion Model for Object Detection" /><link rel="canonical" href="https://solanian.github.io/posts/diffusion-det/" /><meta property="og:url" content="https://solanian.github.io/posts/diffusion-det/" /><meta property="og:site_name" content="I want to know everything" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-12-11T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="DiffusionDet 코드까지 깊게 살펴보기" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-12-11T00:00:00+09:00","datePublished":"2022-12-11T00:00:00+09:00","description":"DiffusionDet: Diffusion Model for Object Detection","headline":"DiffusionDet 코드까지 깊게 살펴보기","mainEntityOfPage":{"@type":"WebPage","@id":"https://solanian.github.io/posts/diffusion-det/"},"url":"https://solanian.github.io/posts/diffusion-det/"}</script><title>DiffusionDet 코드까지 깊게 살펴보기 | I want to know everything</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="I want to know everything"><meta name="application-name" content="I want to know everything"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="/assets/css/academicons.css"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">I want to know everything</a></div><div class="site-subtitle font-italic">academic notes, thoughts, reviews</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="https://drive.google.com/file/d/1rIuUuGCkeIdNCTUC6p3cKCzRFvuR7Fxk/view?usp=sharing" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>CURRICULUM VITAE</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/solanian" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['sorld0603','snu.ac.kr'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/dongseong-seo-64a3471aa" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://scholar.google.com/citations?user=B7-29akAAAAJ" aria-label="googlescholar" target="_blank" rel="noopener"> <i class="ai ai-google-scholar-square"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>DiffusionDet 코드까지 깊게 살펴보기</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>DiffusionDet 코드까지 깊게 살펴보기</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1670684400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Dec 11, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/solanian">Dongseong Seo</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3789 words"> <em>21 min</em> read</span></div></div></div><div class="post-content"><h1 id="diffusiondet-diffusion-model-for-object-detection">DiffusionDet: Diffusion Model for Object Detection</h1><p>Author: Shoufa Chen Conference / Journal: Arxiv PrePrint Nickname: DiffusionDet Year: 2022</p><p>paper: <a href="https://arxiv.org/abs/2211.09788">DiffusionDet: Diffusion Model for Object Detection</a></p><p>code: <a href="https://github.com/ShoufaChen/DiffusionDet">GitHub - ShoufaChen/DiffusionDet: PyTorch implementation of DiffusionDet</a></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled.png" alt="Untitled" data-proofer-ignore></p><h1 id="introduction">Introduction</h1><p><strong>DiffusionDet</strong>은 object detection문제를 noisy한 box를 object box로 diffusion process를 통해서 denoising하는 generation task로 새로 정의한 논문이다.</p><p><strong>논문에서 주장하는 main contribution 및 모델의 장점</strong></p><ul><li><p>pre-define anchor, 학습된 query말고 random box도 object candidate로써의 역할을 한다는 사실을 발견했다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%201.png" alt="Untitled" data-proofer-ignore></p><li>이전의 패러다임과 완전 다르게 generative한 방식으로 object detection을 해냈다.<li>Random box를 object candidate로 사용하기 때문에 train에서 사용한 random box의 수와 inference에서 사용할 random box의 수가 일치하지 않아도 된다<li>Iteration을 통해서 점점 refine 되기 때문에 필요한 속도와 정확도를 맞춰서 성능을 조절할 수 있다. 즉 학습된 하나의 모델이 가지는 flexibilty가 높다고 한다.</ul><h1 id="preliminary---diffusion-model">Preliminary - Diffusion Model</h1><p>Diffusion model에 대해서 아는 사람도 있겠지만 모르는 사람을 위해서 CVPR 2022 diffusion tutorial에 좋은 자료가 있어서 여기서 아주 기본적인 부분들만 가져와서 소개해봤다.</p><p><strong>Reference: CVPR 2022 diffusion tutorial</strong></p><p>Tutorial page: <a href="https://cvpr2022-tutorial-diffusion-models.github.io/">Denoising Diffusion-based Generative Modeling: Foundations and Applications</a></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%202.png" alt="Untitled" data-proofer-ignore></p><p>재밌어 보이는 그림이라서 가져옴 현재 generative task에서 각 model들이 표현할 수 있는 수준을 한 장으로 보여주는 그림. 왜 diffusion model이 요즘 핫한지 쉽게 이해가 됨</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%203.png" alt="Untitled" data-proofer-ignore></p><p>Diffusion Model의 motivation: 연기의 확산(diffusion)을 생각해 보면 연기가 생성되는 지점부터 쭉 퍼져나가서 모든 공간에 random하게 퍼지게 되는데 연기가 움직이는 step 하나하나를 계산하면 역으로 연기가 시작된 지점을 알 수 있듯, noisy한 image도 어떤 선명한 image로 부터 순차적으로 작은 noise가 더해져 왔으며 이 noise를 계산하면 거꾸로 noisy한 image로 부터 원래의 선명한 image를 얻을 수 있지 않겠냐는 아이디어에서 왔다.</p><p>diffusion model은 forward process와 reverse process로 나뉜다.</p><p>forward: noise를 더해가는 과정, reverse: noise를 제거해 가는 과정</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%204.png" alt="Untitled" data-proofer-ignore></p><p>forward process: step 마다 정해진 schedule에 따라 noise를 더해 random image 생성</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%205.png" alt="Untitled" data-proofer-ignore></p><p>reverse process: 각 step마다 더해진 noise를 학습된 network를 이용해 predict하여 이를 역으로 계산해서 denoising을 해나감</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%206.png" alt="Untitled" data-proofer-ignore></p><p>Generation task를 할 때는 실제로 predict한 noise에서 sampling을 해서 그 값으로 denoising을 함</p><h1 id="method">Method</h1><h2 id="architecture"><span class="mr-2">Architecture</span><a href="#architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%207.png" alt="Untitled" data-proofer-ignore></p><p>diffusion model은 iterative하게 돌아가야 하기 때문에 raw image를 사용하게 되면 computation cost가 매우 증가해서 image encoder와 detection decoder를 사용하였다. Encoder를 통해 크기가 작은 feature를 뽑고 이 feature를 이용해 diffusion process를 거치고 원하는 step에서 detection decoder를 통해서 output을 뽑는다. 결과적으로 encoder와 decoder는 한 번씩 쓰이며 raw image에 비해 빠른 속도로 diffusion process를 돌릴 수 있도록 했다.</p><h2 id="code-detail"><span class="mr-2">Code Detail</span><a href="#code-detail" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>code detail은 가장 좋은 성능을 보이는 image encoder를 swin transformer를 쓴 config를 기준으로 분석하였다.<li>File path: <code class="language-plaintext highlighter-rouge">configs/diffdet.coco.swinbase.yaml</code></ul><h3 id="dataloader"><span class="mr-2">Dataloader</span><a href="#dataloader" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Dataloader 단에서 불러오는 정보는 <code class="language-plaintext highlighter-rouge">file_name</code>, <code class="language-plaintext highlighter-rouge">height</code>, <code class="language-plaintext highlighter-rouge">width</code>, <code class="language-plaintext highlighter-rouge">image_id</code>, <code class="language-plaintext highlighter-rouge">image</code>, <code class="language-plaintext highlighter-rouge">instances</code>이다. 다른 정보는 자명하니 <code class="language-plaintext highlighter-rouge">instances</code>만 살펴보면 <code class="language-plaintext highlighter-rouge">image_height</code>, <code class="language-plaintext highlighter-rouge">image_width</code>, gt 정보가 들어있다. <code class="language-plaintext highlighter-rouge">height</code>, <code class="language-plaintext highlighter-rouge">width</code>와 <code class="language-plaintext highlighter-rouge">image_height</code>, <code class="language-plaintext highlighter-rouge">image_width</code>의 차이는 input으로 들어가는 size와 original image의 size의 차이 이다.</p><h3 id="preprocessing"><span class="mr-2">Preprocessing</span><a href="#preprocessing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%208.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%209.png" alt="Untitled" data-proofer-ignore></p><p>Preprocessing 단에서는 크게 복잡한 과정은 없고 <code class="language-plaintext highlighter-rouge">images</code>에 image를 normalize하고 batch에 속하는 image들의 크기를 32의 배수가 되도록 padding도 하고 크기도 맞춰서 저장하고 <code class="language-plaintext highlighter-rouge">images_whwh</code>에 image의 원래 크기를 $bs\times4$로 저장한다. 굳이 whwh로 두 번이나 저장하는 이유는 이 후에 학습과정에 사용할 box들의 $(c_x, c_y, w, h)$ 값에 편하게 곱해주기 위함이다.</p><h3 id="training"><span class="mr-2">Training</span><a href="#training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>전체적인 training과정은 아래와 같다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2010.png" alt="Untitled" data-proofer-ignore></p><p>이 Pseudo code를 말로 풀어 설명하면 image에서 feature를 뽑고 gt_boxes를 train에서 사용할 random box의 수와 맞추기 위해 padding을 해준다. 그 후에 box들에 noise를 더해서 corrupt된 box 들을 얻고 이 corrupt된 box로 부터 다시 gt box를 predict하고 loss를 구한다.</p><p>코드 상에서 어떻게 구현 되었는지를 하나하나 살펴보았다.</p><ul><li><p>Image encoder</p><p>Image feature를 추출하는 부분이다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2011.png" alt="Untitled" data-proofer-ignore></p><p>swin transformer(backbone)를 사용하여 1/4, 1/8, 1/16, 1/32 크기의 feature를 뽑아 <code class="language-plaintext highlighter-rouge">features</code>에 저장한다.</p><li><p>Pad gt boxes &amp; forward process</p><p>학습에 사용할 gt data를 준비하고 forward process를 이용해 corrupted box를 생성하는 부분이다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2012.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2013.png" alt="Untitled" data-proofer-ignore></p><p>label 정보를 크기를 보정해서 <code class="language-plaintext highlighter-rouge">targets</code>, noise로 corrupted된 box를 <code class="language-plaintext highlighter-rouge">x_boxes</code>로, 사용된 noise와 step 수를 <code class="language-plaintext highlighter-rouge">noises</code>, <code class="language-plaintext highlighter-rouge">t</code>로 저장한다. 여기서 noise를 어떻게 더하고 gt box padding을 하는지를 보면</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2014.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2015.png" alt="Untitled" data-proofer-ignore></p><p>우선 처음에 step 수랑 noise를 random하게 생성한다.</p><p>그리고 $N_{train}$(num_proposal) 보다 gt box가 많다면 random sample을 하고 gt box가 적다면 기존 box에 약간의 noise를 더해 나머지 box를 생성한다.</p><p>그리고 box들의 좌표를 image 중심에서 구석으로 기준점을 바꿔주고 signal scale을 곱해준다. 여기서 signal scaling을 하는 이유는 diffusion task가 이 SNR scale에 민감해서 효과적인 scale을 찾아줘야 하기 때문이며 object detection task가 generation task에 비해서 상대적으로 높은 signal scaling value에서 잘 작동하는걸 찾았다고 한다. (왜 그런지는 못 찾은 듯)</p><p>그리고 여기에 forward process에서 noise를 sample해서 더해준다. 이 forward process 함수는 <code class="language-plaintext highlighter-rouge">q_sample</code>인데 여기서 <code class="language-plaintext highlighter-rouge">sqrt_alphas_cumprod_t</code> 와 <code class="language-plaintext highlighter-rouge">sqrt_one_minus_alphas_cumprod_t</code> 는 t-step 까지 더해진 noise를 한번에 구하는 식인 아래의 식을 이용해 구한 것이다. 각각 mean의 변화와 variance의 변화를 의미한다. 여기서 사용된 $\alpha$는 cosine schedule을 사용했다고 한다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2016.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2017.png" alt="Untitled" data-proofer-ignore></p><p>이렇게 구한 noise를 box의 $(c_x,c_y,w,h)$에 적용하여 box corruption을 하고 다시 image 중심 좌표계로 옮겨 준다. 이렇게 corrupt된 box와 함께 여기서 사용된 noise와 step수를 저장한다.</p><p>여기서 $\alpha, \beta$ 같은 parameter들은 학습되는 변수가 아닌 미리 설정한 scheduling에 따라서 정해지기 때문에 <code class="language-plaintext highlighter-rouge">register_buffer</code>에 미리 저장해두고 <code class="language-plaintext highlighter-rouge">extract</code>를 통해서 가져온다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2018.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2019.png" alt="Untitled" data-proofer-ignore></p><li><p>Reverse process &amp; detection decoder</p><p>이전 단계에서 구한 noisy box와 image feature를 이용해 detection 결과를 뽑는 detection head이다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2020.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2021.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2022.png" alt="Untitled" data-proofer-ignore></p><p>우선 <code class="language-plaintext highlighter-rouge">time_mlp</code>에서 transformer의 position embedding과 유사하게 각 noise prediction마다 time step을 구분 할 수 있도록 time step을 embedding 해준다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2023.png" alt="Untitled" data-proofer-ignore></p><p>그 후에 parameter로 주는 <code class="language-plaintext highlighter-rouge">num_heads</code>의 수 만큼 reverse process를 적용해서 매 step 마다 predicted <code class="language-plaintext highlighter-rouge">bboxes</code>와 <code class="language-plaintext highlighter-rouge">proposal_features</code>를 얻을 수 있고 이 값들은 다음 step에서도 쓰여서 결과를 refine해 나간다. 여기서 reverse process의 noise prediction을 하는데에는 Sparse R-CNN이 쓰였다</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2024.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2025.png" alt="Untitled" data-proofer-ignore></p><p><code class="language-plaintext highlighter-rouge">rcnn_head</code>를 자세히 살펴보면 처음에 <code class="language-plaintext highlighter-rouge">pooler</code>를 통해서 image backbone에서 나온 feature를 이전 step에서 구한 box를 이용해 crop하여 7x7 크기의 <code class="language-plaintext highlighter-rouge">roi_features</code>를 구한다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2026.png" alt="Untitled" data-proofer-ignore></p><p>feature를 denoising하는 과정(reverse process)은 <code class="language-plaintext highlighter-rouge">self_attention</code>, <code class="language-plaintext highlighter-rouge">instance_interatcion</code>, <code class="language-plaintext highlighter-rouge">object_feature_extraction</code> 이 세 과정을 거친다. <code class="language-plaintext highlighter-rouge">self_attention</code>에서는 <code class="language-plaintext highlighter-rouge">proposed_feature</code>들 중에서 빈도가 높은 feature들을 더해주고 <code class="language-plaintext highlighter-rouge">instance_interaction</code>을 통해 이전 단계에서 predict box 영역 내부의 feature와 유사한 feature들을 더해주고 <code class="language-plaintext highlighter-rouge">object_feature_extraction</code>이용해 object가 존재할 확률이 높은 부분의 feature들을 더해준다.</p><p>이렇게 feature들을 더해주는 과정들을 다른 diffusion model의 구현코드를 본 적이 없다면 reverse process라고 한 번에 알아볼 수 없을 수도 있다. (내가 그랬다.)</p><p>Diffusion model의 reverse process는 아래의 식과 같은데 distribution을 predict하고 여기서 sampling을 해서 denoising을 하게 되는데 실제로 predict해야 하는 부분은 mean의 noise만 predict 하면 되고 나머지 값 들은 매 time step에 따라서 미리 scheduling 되어 있다.</p>\[q(x_{t-1}|x_t,t_0)=\mathcal{N}(x_{t-1};\tilde{\mu}_t(x_t,x_0),\tilde\beta_t\bf{I}) \\ \text{where} \ \mu_\theta(x_t, t)=\frac{1}{\sqrt{1-\beta_t}}\left({x_t-\frac{1}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t,t)}\right)\]<p>각각의 과정들이 결국 noise prediction 후 denoising을 위해서 더해주는 과정과 같다고 보면 된다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2027.png" alt="Untitled" data-proofer-ignore></p><p>이 denoising 된 t step에서의 feature에 time step을 구분해주기 위해서 이전에 계산했던 time embedding 값에 따라서 scale, shift 해준다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2028.png" alt="Untitled" data-proofer-ignore></p><p>그 후에 <code class="language-plaintext highlighter-rouge">cls_layer</code>, <code class="language-plaintext highlighter-rouge">reg_layer</code>를 통해서 class와 box_delta를 구해서 refine된 결과와 feature를 저장하고 지정해둔 step 수 만큼 reverse process를 반복한다. 이 코드 상에서는 6번 반복 했다.</p><li><p>Training Loss</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2029.png" alt="Untitled" data-proofer-ignore></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2030.png" alt="Untitled" data-proofer-ignore></p><p>training loss는 논문에서 설명한 바에 따르면 predict한 $N_{train}$개의 box를 가지고 각각의 ground truth 마다 cost가 낮은 top-$k$개의 유사한 box들을 matching 하고 이 matching된 pair들의 loss의 평균을 사용한다. matching은 optimal transport assignment method를 사용했으며 여기서 사용한 $k=5$이다. 이 알고리즘의 자세한 설명은 이 논문의 정리와는 맞지 않는 것 같아 더 알아보고 싶다면 아래 논문과 코드 부분을 참고하면 될 것 같다.</p><p>reference paper: <a href="https://arxiv.org/abs/2103.14259">OTA: Optimal Transport Assignment for Object Detection</a></p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2031.png" alt="Untitled" data-proofer-ignore></p></ul><h3 id="inference"><span class="mr-2">Inference</span><a href="#inference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Inference 과정도 naive하게 보면 training과 크게 다르지 않으나 object detection이 정답이 정해져있는 task이다 보니 deterministic하게 generation이 이뤄져야 결과가 robust 할 것이라 기대할 수 있고 수렴 속도 향상을 위해서 inference 단의 sampling 과정에서의 diffusion process는 DDIM(<a href="https://arxiv.org/abs/2010.02502">Denoising Diffusion Implicit Models</a>)을 사용한 것으로 추측된다. 해당 부분 역시 CVPR 2022 Tutorial를 참고하거나 DDIM 논문을 참고하면 될 것 같다. 직접적으로 연관된 부분은 나중에 코드와 함께 설명하겠다. 전체적인 과정의 pseudo code는 아래와 같다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2032.png" alt="Untitled" data-proofer-ignore></p><p>Image encoder에서 feature를 뽑고 corrupted box를 random하게 생성한다. 사용할 time step을 결정하고 detection decoder와 reverse process를 time step만큼 반복하며 box를 refine한다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2033.png" alt="Untitled" data-proofer-ignore></p><p>코드를 살펴보면 feature를 뽑는 과정은 특별할 것도 없고 training에서 설명 했으니 넘어가겠다. 그 후에 바로 <code class="language-plaintext highlighter-rouge">ddim_sample</code> 함수를 통해서 box prediction을 진행한다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2034.png" alt="Untitled" data-proofer-ignore></p><p>여기서는 $N_{eval}$만큼 random box를 생성하고 예측하고 싶은 time step $T$를 원하는 간격으로 쪼개서 time pair들을 생성한다. 논문에서는 $N_{eval}=500$, $T=1000$, sampling_timestep=1을 사용하였다. 총 500개의 box를 1 timestep간격으로 1000번 반복하게 되는 것이다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2035.png" alt="Untitled" data-proofer-ignore></p><p>앞에서 구한 time pair들에 대해서 training에서와 같은 방식으로 <code class="language-plaintext highlighter-rouge">model_prediction</code>에서 reverse process를 거친다. 그 결과로 나온 predict한 box가 너무 random에 가까운 결과를 뽑은 경우에는 다음 step으로 넘겨도 크게 도움이 되지 않을 뿐더러 오히려 성능을 저해하는 요인이 될 수 있다. 그래서 <code class="language-plaintext highlighter-rouge">box renewal</code>에서 class score를 가지고 desired와 un-desired로 구분해서 desired로 분류된 box들만 다음 step으로 넘기는 filtering을 해준다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2036.png" alt="Untitled" data-proofer-ignore></p><p>여기서 걸러낸 predicted box를 바로 사용할 수도 있으나 ddim으로 한번 reverse process를 거치면서 sampling을 해준다. ddim과 training에서 사용하던 diffusion process의 차이는 markovian process인지를 가정하는지 안 하는지의 차이이다. ddim은 non-markovian이라는 가정하에 noisy한 data의 초기상태와 바로 직전의 data의 상태를 기반으로 noise를 prediction 하기 때문에 같은 time step에서는 noisy data의 초기 상태와 1:1로 deterministic한 결과를 뽑아낼 수 있다. 이러한 특성 때문에 보다 빠르게 수렴하다보니 <code class="language-plaintext highlighter-rouge">model_prediction</code>의 결과로 나온 predicted box가 noisy하더라도 한번 ddim을 거치고 나면 refine된 효과를 얻을 수 있을 것으로 보여서 추가로 사용한 것으로 보인다. 논문에서도 추가로 ddim을 사용하고 안 하고의 성능차이가 꽤 있었다고 한다.</p><p>ddim에서의 reverse process 식은 아래와 같다.</p>\[p(\bf{x}_{t-1}|\bf{x}_t)=\mathcal{N}\left(\sqrt{\bar\alpha_{t-1}}\hat{\bf{x}}_0+\sqrt{1-\bar\alpha_{t-1}-\tilde\sigma^2_i}\cdot\frac{\bf{x}_t-\sqrt{\bar{\alpha}_t}\hat{\bf{x}}_0}{\sqrt{1-\bar\alpha_t}}, \tilde\sigma^2_i\bf{I}\right)\]<p>여기서 위의 training section에서 언급했듯 diffusion의 time step $t$에서의 data $x_t=\sqrt{\bar\alpha_t}x_0+\sqrt{(1-\bar\alpha_t)}\epsilon$ 이므로</p>\[\epsilon=\frac{x_t-\sqrt{\bar\alpha_t}x_0}{\sqrt{1-\bar\alpha_t}}, x_0=\frac{x_t-\sqrt{1-\bar\alpha_t}\epsilon}{\sqrt{\bar\alpha_t}}\]<p>이다. ddim의 reverse process식에 reparameterization trick과 위의 식을 이용하면 아래와 같은 식을 얻을 수 있다.</p>\[x_{t-1}=\sqrt{\bar\alpha_{t-1}}\hat{\bf{x}}_0+\sqrt{1-\bar\alpha_{t-1}-\tilde\sigma^2_i}\epsilon_\theta^t(\bf{x}_t)+\sigma_t\epsilon_t\]<p>여기서 $\epsilon_\theta^t(\bf{x}_t)$가 predicted noise이다. 이 식을 통해서 refine된 bbox prediction 결과를 얻는다. 코드는 위의 식을 보면 이해가 될 것이다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2037.png" alt="Untitled" data-proofer-ignore></p><p>여기에 <code class="language-plaintext highlighter-rouge">use_ensemble</code>을 하게 되면 원래 predicted box의 결과 ddim의 결과로 나온 predicted box와 ensemble을 하게 되고 <code class="language-plaintext highlighter-rouge">detector_postprocess</code>를 거쳐 이미지 상에서 상대 위치로 나온 결과를 다시 절대 좌표로 바꿔서 결과를 얻게 된다.</p><h1 id="experiments">Experiments</h1><h2 id="results"><span class="mr-2">Results</span><a href="#results" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2038.png" alt="Untitled" data-proofer-ignore></p><p>첫번째 그림은 초기에 proposal 하는 box 수에 따라서 성능을 비교한건데 DiffusionDet은 proposal box를 늘리는 만큼 성능이 올라가지만 DETR은 오히려 box의 수를 늘리면 degenration이 발생한다고 하는데 가장 성능이 좋은 box 수를 민감하게 잡아야 하는 DETR에 비해 적절하게 성능하고 속도간의 trade-off만 맞추면 되는 부분이 확실히 이점이 있는 것으로 보인다.</p><p>두번째 그림은 sample step을 거칠 수록 refinement의 효과를 보인다는 것을 나타낸 그림이다. box 수를 많이 쓰는게 overhead를 많이 잡아먹지 않는다면 step수를 늘리는 것보다 box수를 더 쓰는게 나아 보인다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2039.png" alt="Untitled" data-proofer-ignore></p><p>Detection 성능은 훌륭했다.</p><h2 id="ablation-study"><span class="mr-2">Ablation Study</span><a href="#ablation-study" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2040.png" alt="Untitled" data-proofer-ignore></p><p>signal scale factor, box renewal thresthold, gt box padding 방법론, DDIM과 box renewal의 사용 유무, $N_{train},N_{eval}$에 따른 성능 비교를 ablation study한 결과도 같이 첨부했다. 실제 구현시에 각 요소들의 효과를 고려하는데 참고하면 좋을 것 같다.</p><p>그리고 box 수와 step 수에 따른 fpx와 정확도 차이가 나오는데 앞에서 box 수를 늘리는데에 큰 overhead가 없다면 step 수 보다는 box 수를 늘리는게 나을 것 같다고 했는데 box를 100개 쓰나 300개 쓰나 fps는 약간 느려지긴 하는데 큰 차이가 없다. 성능 향상을 위해선 box 수를 늘리는걸 우선으로 삼아도 좋을 듯 하다.</p><p><img data-src="../../assets/img/DiffusionDet%20Diffusion%20Model%20for%20Object%20Detection%20bd53fdfeb296488baaa776718b9cebb6/Untitled%2041.png" alt="Untitled" data-proofer-ignore></p><p>아무래도 generation model이다 보니 random seed에 따른 성능 차이에 대해서 불안감이 있을 수 있는데 이에 대해서도 테스트를 한 결과 성능이 왔다갔다 하긴 하는데 이 정도면 꽤나 robust하게 성능을 내고 있다고 봐도 무방할 정도로 편차가 크지 않게 나왔다.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/object-detection/'>Object Detection</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >Deep Learning</a> <a href="/tags/model/" class="post-tag no-text-decoration" >Model</a> <a href="/tags/vision/" class="post-tag no-text-decoration" >Vision</a> <a href="/tags/diffusion/" class="post-tag no-text-decoration" >Diffusion</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=DiffusionDet+%EC%BD%94%EB%93%9C%EA%B9%8C%EC%A7%80+%EA%B9%8A%EA%B2%8C+%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0+-+I+want+to+know+everything&url=https%3A%2F%2Fsolanian.github.io%2Fposts%2Fdiffusion-det%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=DiffusionDet+%EC%BD%94%EB%93%9C%EA%B9%8C%EC%A7%80+%EA%B9%8A%EA%B2%8C+%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0+-+I+want+to+know+everything&u=https%3A%2F%2Fsolanian.github.io%2Fposts%2Fdiffusion-det%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fsolanian.github.io%2Fposts%2Fdiffusion-det%2F&text=DiffusionDet+%EC%BD%94%EB%93%9C%EA%B9%8C%EC%A7%80+%EA%B9%8A%EA%B2%8C+%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0+-+I+want+to+know+everything" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/why_devcontainer/">Why devcontainer?</a><li><a href="/posts/review-detr/">DETR에서 PETRv2 까지</a><li><a href="/posts/region-growing-segmentation/">Region Growing Segmentation 정리</a><li><a href="/posts/stand_alone_self_attention/">Stand-Alone Self-Attention in Visual Models 정리</a><li><a href="/posts/A_comprehensive_review_of_3D_point_cloud_descriptors/">3D pointcloud descriptors 총 정리</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/pointcloud/">Pointcloud</a> <a class="post-tag" href="/tags/slam/">SLAM</a> <a class="post-tag" href="/tags/vision/">Vision</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/model/">Model</a> <a class="post-tag" href="/tags/rigid/">Rigid</a> <a class="post-tag" href="/tags/calibration/">calibration</a> <a class="post-tag" href="/tags/diffusion/">Diffusion</a> <a class="post-tag" href="/tags/docker/">Docker</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/review-detr/"><div class="card-body"> <em class="small" data-ts="1660575600" data-df="ll" > Aug 16, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>DETR에서 PETRv2 까지</h3><div class="text-muted small"><p> 오랜만에 다시 공부하게 된 딥러닝의 세계는 듣던대로 정말 모든 분야를 transformer가 장악하고 있었다. 회사에서 새로 만드려는 fusion module에 사용할 수 있는 유용한 것들이 있나 survey를 하다가 detr계열의 논문들에 다들 관심이 있어해서 이걸로 다시 공부를 시작해 보려고 한다. 해당 논문들의 변천사를 정리해보자면 이렇다. ...</p></div></div></a></div><div class="card"> <a href="/posts/stand_alone_self_attention/"><div class="card-body"> <em class="small" data-ts="1560006000" data-df="ll" > Jun 9, 2019 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Stand-Alone Self-Attention in Visual Models 정리</h3><div class="text-muted small"><p> paper: Stand-Alone Self-Attention in Visual Models Abstract 현대 컴퓨터 비전에서 convolution은 fundamental building block으로 역할을 수행해 왔다. 최근 몇몇 연구에서 long range dependency 문제를 해결하기 위해 convolution을 넘어서야 한다는 주장...</p></div></div></a></div><div class="card"> <a href="/posts/flownet-3d/"><div class="card-body"> <em class="small" data-ts="1569596400" data-df="ll" > Sep 28, 2019 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>FlowNet3D 정리</h3><div class="text-muted small"><p> paper: FlowNet3D: Learning Scene Flow in 3D Point Clouds code: https://github.com/xingyul/flownet3d 이 논문은 dynamic envorionment에서 point의 3D motion(scene flow)를 찾기 위해 제안된 논문이다. 그래서 FlowNet3D를 제안해 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/review-detr/" class="btn btn-outline-primary" prompt="Older"><p>DETR에서 PETRv2 까지</p></a><div class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></div></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/solanian">Dongseong Seo</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/pointcloud/">Pointcloud</a> <a class="post-tag" href="/tags/slam/">SLAM</a> <a class="post-tag" href="/tags/vision/">Vision</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/model/">Model</a> <a class="post-tag" href="/tags/rigid/">Rigid</a> <a class="post-tag" href="/tags/calibration/">calibration</a> <a class="post-tag" href="/tags/diffusion/">Diffusion</a> <a class="post-tag" href="/tags/docker/">Docker</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
