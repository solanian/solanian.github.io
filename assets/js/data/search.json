[ { "title": "DiffusionDet 코드까지 깊게 살펴보기", "url": "/posts/diffusion-det/", "categories": "Object Detection", "tags": "Deep Learning, Model, Vision, Diffusion", "date": "2022-12-11 00:00:00 +0900", "snippet": "DiffusionDet: Diffusion Model for Object DetectionAuthor: Shoufa ChenConference / Journal: Arxiv PrePrintNickname: DiffusionDetYear: 2022paper: DiffusionDet: Diffusion Model for Object Detectioncode: GitHub - ShoufaChen/DiffusionDet: PyTorch implementation of DiffusionDetIntroductionDiffusionDet은 object detection문제를 noisy한 box를 object box로 diffusion process를 통해서 denoising하는 generation task로 새로 정의한 논문이다.논문에서 주장하는 main contribution 및 모델의 장점 pre-define anchor, 학습된 query말고 random box도 object candidate로써의 역할을 한다는 사실을 발견했다. 이전의 패러다임과 완전 다르게 generative한 방식으로 object detection을 해냈다. Random box를 object candidate로 사용하기 때문에 train에서 사용한 random box의 수와 inference에서 사용할 random box의 수가 일치하지 않아도 된다 Iteration을 통해서 점점 refine 되기 때문에 필요한 속도와 정확도를 맞춰서 성능을 조절할 수 있다. 즉 학습된 하나의 모델이 가지는 flexibilty가 높다고 한다.Preliminary - Diffusion ModelDiffusion model에 대해서 아는 사람도 있겠지만 모르는 사람을 위해서 CVPR 2022 diffusion tutorial에 좋은 자료가 있어서 여기서 아주 기본적인 부분들만 가져와서 소개해봤다.Reference: CVPR 2022 diffusion tutorialTutorial page: Denoising Diffusion-based Generative Modeling: Foundations and Applications재밌어 보이는 그림이라서 가져옴 현재 generative task에서 각 model들이 표현할 수 있는 수준을 한 장으로 보여주는 그림. 왜 diffusion model이 요즘 핫한지 쉽게 이해가 됨Diffusion Model의 motivation: 연기의 확산(diffusion)을 생각해 보면 연기가 생성되는 지점부터 쭉 퍼져나가서 모든 공간에 random하게 퍼지게 되는데 연기가 움직이는 step 하나하나를 계산하면 역으로 연기가 시작된 지점을 알 수 있듯, noisy한 image도 어떤 선명한 image로 부터 순차적으로 작은 noise가 더해져 왔으며 이 noise를 계산하면 거꾸로 noisy한 image로 부터 원래의 선명한 image를 얻을 수 있지 않겠냐는 아이디어에서 왔다.diffusion model은 forward process와 reverse process로 나뉜다.forward: noise를 더해가는 과정, reverse: noise를 제거해 가는 과정forward process: step 마다 정해진 schedule에 따라 noise를 더해 random image 생성reverse process: 각 step마다 더해진 noise를 학습된 network를 이용해 predict하여 이를 역으로 계산해서 denoising을 해나감Generation task를 할 때는 실제로 predict한 noise에서 sampling을 해서 그 값으로 denoising을 함MethodArchitecturediffusion model은 iterative하게 돌아가야 하기 때문에 raw image를 사용하게 되면 computation cost가 매우 증가해서 image encoder와 detection decoder를 사용하였다. Encoder를 통해 크기가 작은 feature를 뽑고 이 feature를 이용해 diffusion process를 거치고 원하는 step에서 detection decoder를 통해서 output을 뽑는다. 결과적으로 encoder와 decoder는 한 번씩 쓰이며 raw image에 비해 빠른 속도로 diffusion process를 돌릴 수 있도록 했다.Code Detail code detail은 가장 좋은 성능을 보이는 image encoder를 swin transformer를 쓴 config를 기준으로 분석하였다. File path: configs/diffdet.coco.swinbase.yamlDataloaderDataloader 단에서 불러오는 정보는 file_name, height, width, image_id, image, instances이다. 다른 정보는 자명하니 instances만 살펴보면 image_height, image_width, gt 정보가 들어있다. height, width와 image_height, image_width의 차이는 input으로 들어가는 size와 original image의 size의 차이 이다.PreprocessingPreprocessing 단에서는 크게 복잡한 과정은 없고 images에 image를 normalize하고 batch에 속하는 image들의 크기를 32의 배수가 되도록 padding도 하고 크기도 맞춰서 저장하고 images_whwh에 image의 원래 크기를 $bs\\times4$로 저장한다. 굳이 whwh로 두 번이나 저장하는 이유는 이 후에 학습과정에 사용할 box들의 $(c_x, c_y, w, h)$ 값에 편하게 곱해주기 위함이다.Training전체적인 training과정은 아래와 같다.이 Pseudo code를 말로 풀어 설명하면 image에서 feature를 뽑고 gt_boxes를 train에서 사용할 random box의 수와 맞추기 위해 padding을 해준다. 그 후에 box들에 noise를 더해서 corrupt된 box 들을 얻고 이 corrupt된 box로 부터 다시 gt box를 predict하고 loss를 구한다.코드 상에서 어떻게 구현 되었는지를 하나하나 살펴보았다. Image encoder Image feature를 추출하는 부분이다. swin transformer(backbone)를 사용하여 1/4, 1/8, 1/16, 1/32 크기의 feature를 뽑아 features에 저장한다. Pad gt boxes &amp; forward process 학습에 사용할 gt data를 준비하고 forward process를 이용해 corrupted box를 생성하는 부분이다. label 정보를 크기를 보정해서 targets, noise로 corrupted된 box를 x_boxes로, 사용된 noise와 step 수를 noises, t로 저장한다. 여기서 noise를 어떻게 더하고 gt box padding을 하는지를 보면 우선 처음에 step 수랑 noise를 random하게 생성한다. 그리고 $N_{train}$(num_proposal) 보다 gt box가 많다면 random sample을 하고 gt box가 적다면 기존 box에 약간의 noise를 더해 나머지 box를 생성한다. 그리고 box들의 좌표를 image 중심에서 구석으로 기준점을 바꿔주고 signal scale을 곱해준다. 여기서 signal scaling을 하는 이유는 diffusion task가 이 SNR scale에 민감해서 효과적인 scale을 찾아줘야 하기 때문이며 object detection task가 generation task에 비해서 상대적으로 높은 signal scaling value에서 잘 작동하는걸 찾았다고 한다. (왜 그런지는 못 찾은 듯) 그리고 여기에 forward process에서 noise를 sample해서 더해준다. 이 forward process 함수는 q_sample인데 여기서 sqrt_alphas_cumprod_t 와 sqrt_one_minus_alphas_cumprod_t 는 t-step 까지 더해진 noise를 한번에 구하는 식인 아래의 식을 이용해 구한 것이다. 각각 mean의 변화와 variance의 변화를 의미한다. 여기서 사용된 $\\alpha$는 cosine schedule을 사용했다고 한다. 이렇게 구한 noise를 box의 $(c_x,c_y,w,h)$에 적용하여 box corruption을 하고 다시 image 중심 좌표계로 옮겨 준다. 이렇게 corrupt된 box와 함께 여기서 사용된 noise와 step수를 저장한다. 여기서 $\\alpha, \\beta$ 같은 parameter들은 학습되는 변수가 아닌 미리 설정한 scheduling에 따라서 정해지기 때문에 register_buffer에 미리 저장해두고 extract를 통해서 가져온다. Reverse process &amp; detection decoder 이전 단계에서 구한 noisy box와 image feature를 이용해 detection 결과를 뽑는 detection head이다. 우선 time_mlp에서 transformer의 position embedding과 유사하게 각 noise prediction마다 time step을 구분 할 수 있도록 time step을 embedding 해준다. 그 후에 parameter로 주는 num_heads의 수 만큼 reverse process를 적용해서 매 step 마다 predicted bboxes와 proposal_features를 얻을 수 있고 이 값들은 다음 step에서도 쓰여서 결과를 refine해 나간다. 여기서 reverse process의 noise prediction을 하는데에는 Sparse R-CNN이 쓰였다 rcnn_head를 자세히 살펴보면 처음에 pooler를 통해서 image backbone에서 나온 feature를 이전 step에서 구한 box를 이용해 crop하여 7x7 크기의 roi_features를 구한다. feature를 denoising하는 과정(reverse process)은 self_attention, instance_interatcion, object_feature_extraction 이 세 과정을 거친다. self_attention에서는 proposed_feature들 중에서 빈도가 높은 feature들을 더해주고 instance_interaction을 통해 이전 단계에서 predict box 영역 내부의 feature와 유사한 feature들을 더해주고 object_feature_extraction이용해 object가 존재할 확률이 높은 부분의 feature들을 더해준다. 이렇게 feature들을 더해주는 과정들을 다른 diffusion model의 구현코드를 본 적이 없다면 reverse process라고 한 번에 알아볼 수 없을 수도 있다. (내가 그랬다.) Diffusion model의 reverse process는 아래의 식과 같은데 distribution을 predict하고 여기서 sampling을 해서 denoising을 하게 되는데 실제로 predict해야 하는 부분은 mean의 noise만 predict 하면 되고 나머지 값 들은 매 time step에 따라서 미리 scheduling 되어 있다.\\[q(x_{t-1}|x_t,t_0)=\\mathcal{N}(x_{t-1};\\tilde{\\mu}_t(x_t,x_0),\\tilde\\beta_t\\bf{I}) \\\\ \\text{where} \\ \\mu_\\theta(x_t, t)=\\frac{1}{\\sqrt{1-\\beta_t}}\\left({x_t-\\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t,t)}\\right)\\] 각각의 과정들이 결국 noise prediction 후 denoising을 위해서 더해주는 과정과 같다고 보면 된다. 이 denoising 된 t step에서의 feature에 time step을 구분해주기 위해서 이전에 계산했던 time embedding 값에 따라서 scale, shift 해준다. 그 후에 cls_layer, reg_layer를 통해서 class와 box_delta를 구해서 refine된 결과와 feature를 저장하고 지정해둔 step 수 만큼 reverse process를 반복한다. 이 코드 상에서는 6번 반복 했다. Training Loss training loss는 논문에서 설명한 바에 따르면 predict한 $N_{train}$개의 box를 가지고 각각의 ground truth 마다 cost가 낮은 top-$k$개의 유사한 box들을 matching 하고 이 matching된 pair들의 loss의 평균을 사용한다. matching은 optimal transport assignment method를 사용했으며 여기서 사용한 $k=5$이다. 이 알고리즘의 자세한 설명은 이 논문의 정리와는 맞지 않는 것 같아 더 알아보고 싶다면 아래 논문과 코드 부분을 참고하면 될 것 같다. reference paper: OTA: Optimal Transport Assignment for Object Detection InferenceInference 과정도 naive하게 보면 training과 크게 다르지 않으나 object detection이 정답이 정해져있는 task이다 보니 deterministic하게 generation이 이뤄져야 결과가 robust 할 것이라 기대할 수 있고 수렴 속도 향상을 위해서 inference 단의 sampling 과정에서의 diffusion process는 DDIM(Denoising Diffusion Implicit Models)을 사용한 것으로 추측된다. 해당 부분 역시 CVPR 2022 Tutorial를 참고하거나 DDIM 논문을 참고하면 될 것 같다. 직접적으로 연관된 부분은 나중에 코드와 함께 설명하겠다. 전체적인 과정의 pseudo code는 아래와 같다.Image encoder에서 feature를 뽑고 corrupted box를 random하게 생성한다. 사용할 time step을 결정하고 detection decoder와 reverse process를 time step만큼 반복하며 box를 refine한다.코드를 살펴보면 feature를 뽑는 과정은 특별할 것도 없고 training에서 설명 했으니 넘어가겠다. 그 후에 바로 ddim_sample 함수를 통해서 box prediction을 진행한다.여기서는 $N_{eval}$만큼 random box를 생성하고 예측하고 싶은 time step $T$를 원하는 간격으로 쪼개서 time pair들을 생성한다. 논문에서는 $N_{eval}=500$, $T=1000$, sampling_timestep=1을 사용하였다. 총 500개의 box를 1 timestep간격으로 1000번 반복하게 되는 것이다.앞에서 구한 time pair들에 대해서 training에서와 같은 방식으로 model_prediction에서 reverse process를 거친다. 그 결과로 나온 predict한 box가 너무 random에 가까운 결과를 뽑은 경우에는 다음 step으로 넘겨도 크게 도움이 되지 않을 뿐더러 오히려 성능을 저해하는 요인이 될 수 있다. 그래서 box renewal에서 class score를 가지고 desired와 un-desired로 구분해서 desired로 분류된 box들만 다음 step으로 넘기는 filtering을 해준다.여기서 걸러낸 predicted box를 바로 사용할 수도 있으나 ddim으로 한번 reverse process를 거치면서 sampling을 해준다. ddim과 training에서 사용하던 diffusion process의 차이는 markovian process인지를 가정하는지 안 하는지의 차이이다. ddim은 non-markovian이라는 가정하에 noisy한 data의 초기상태와 바로 직전의 data의 상태를 기반으로 noise를 prediction 하기 때문에 같은 time step에서는 noisy data의 초기 상태와 1:1로 deterministic한 결과를 뽑아낼 수 있다. 이러한 특성 때문에 보다 빠르게 수렴하다보니 model_prediction의 결과로 나온 predicted box가 noisy하더라도 한번 ddim을 거치고 나면 refine된 효과를 얻을 수 있을 것으로 보여서 추가로 사용한 것으로 보인다. 논문에서도 추가로 ddim을 사용하고 안 하고의 성능차이가 꽤 있었다고 한다.ddim에서의 reverse process 식은 아래와 같다.\\[p(\\bf{x}_{t-1}|\\bf{x}_t)=\\mathcal{N}\\left(\\sqrt{\\bar\\alpha_{t-1}}\\hat{\\bf{x}}_0+\\sqrt{1-\\bar\\alpha_{t-1}-\\tilde\\sigma^2_i}\\cdot\\frac{\\bf{x}_t-\\sqrt{\\bar{\\alpha}_t}\\hat{\\bf{x}}_0}{\\sqrt{1-\\bar\\alpha_t}}, \\tilde\\sigma^2_i\\bf{I}\\right)\\]여기서 위의 training section에서 언급했듯 diffusion의 time step $t$에서의 data $x_t=\\sqrt{\\bar\\alpha_t}x_0+\\sqrt{(1-\\bar\\alpha_t)}\\epsilon$ 이므로\\[\\epsilon=\\frac{x_t-\\sqrt{\\bar\\alpha_t}x_0}{\\sqrt{1-\\bar\\alpha_t}}, x_0=\\frac{x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon}{\\sqrt{\\bar\\alpha_t}}\\]이다. ddim의 reverse process식에 reparameterization trick과 위의 식을 이용하면 아래와 같은 식을 얻을 수 있다.\\[x_{t-1}=\\sqrt{\\bar\\alpha_{t-1}}\\hat{\\bf{x}}_0+\\sqrt{1-\\bar\\alpha_{t-1}-\\tilde\\sigma^2_i}\\epsilon_\\theta^t(\\bf{x}_t)+\\sigma_t\\epsilon_t\\]여기서 $\\epsilon_\\theta^t(\\bf{x}_t)$가 predicted noise이다. 이 식을 통해서 refine된 bbox prediction 결과를 얻는다. 코드는 위의 식을 보면 이해가 될 것이다.여기에 use_ensemble을 하게 되면 원래 predicted box의 결과 ddim의 결과로 나온 predicted box와 ensemble을 하게 되고 detector_postprocess를 거쳐 이미지 상에서 상대 위치로 나온 결과를 다시 절대 좌표로 바꿔서 결과를 얻게 된다.ExperimentsResults첫번째 그림은 초기에 proposal 하는 box 수에 따라서 성능을 비교한건데 DiffusionDet은 proposal box를 늘리는 만큼 성능이 올라가지만 DETR은 오히려 box의 수를 늘리면 degenration이 발생한다고 하는데 가장 성능이 좋은 box 수를 민감하게 잡아야 하는 DETR에 비해 적절하게 성능하고 속도간의 trade-off만 맞추면 되는 부분이 확실히 이점이 있는 것으로 보인다.두번째 그림은 sample step을 거칠 수록 refinement의 효과를 보인다는 것을 나타낸 그림이다. box 수를 많이 쓰는게 overhead를 많이 잡아먹지 않는다면 step수를 늘리는 것보다 box수를 더 쓰는게 나아 보인다.Detection 성능은 훌륭했다.Ablation Studysignal scale factor, box renewal thresthold, gt box padding 방법론, DDIM과 box renewal의 사용 유무, $N_{train},N_{eval}$에 따른 성능 비교를 ablation study한 결과도 같이 첨부했다. 실제 구현시에 각 요소들의 효과를 고려하는데 참고하면 좋을 것 같다.그리고 box 수와 step 수에 따른 fpx와 정확도 차이가 나오는데 앞에서 box 수를 늘리는데에 큰 overhead가 없다면 step 수 보다는 box 수를 늘리는게 나을 것 같다고 했는데 box를 100개 쓰나 300개 쓰나 fps는 약간 느려지긴 하는데 큰 차이가 없다. 성능 향상을 위해선 box 수를 늘리는걸 우선으로 삼아도 좋을 듯 하다.아무래도 generation model이다 보니 random seed에 따른 성능 차이에 대해서 불안감이 있을 수 있는데 이에 대해서도 테스트를 한 결과 성능이 왔다갔다 하긴 하는데 이 정도면 꽤나 robust하게 성능을 내고 있다고 봐도 무방할 정도로 편차가 크지 않게 나왔다." }, { "title": "DETR에서 PETRv2 까지", "url": "/posts/review-detr/", "categories": "Object Detection", "tags": "Deep Learning, Model, Vision, Transformer", "date": "2022-08-16 00:00:00 +0900", "snippet": "오랜만에 다시 공부하게 된 딥러닝의 세계는 듣던대로 정말 모든 분야를 transformer가 장악하고 있었다. 회사에서 새로 만드려는 fusion module에 사용할 수 있는 유용한 것들이 있나 survey를 하다가 detr계열의 논문들에 다들 관심이 있어해서 이걸로 다시 공부를 시작해 보려고 한다.해당 논문들의 변천사를 정리해보자면 이렇다.DETR: single image 기반인 2D positional embedding을 한 2D object detectionDETR3D: multi-view image + object query로 reference point를 뽑아서 image에 projection하는 트릭을 사용한 3D object detectionPETR: DETR3D의 reference point를 사용하지 않고 camera view와 일치하는 부분에 3D space상에서구한 3D positional embedding을 가져와서 바로 사용함.PETRv2: PETR에 이전 frame의 image도 사용 + 3D PE에 image feature를 추가로 활용하여 보다 robust한 결과를 얻고자 한 것DETRpaper: [2005.12872] End-to-End Object Detection with Transformers (arxiv.org)DETR은 기존의 bounding box regression + classification으로 나누고 중간중간에 hand-designed component를 사용한 object detection 문제를 transformer의 self-attention을 이용해 set prediction problem으로 간소하게 바꿔서 해결하였다. DETR은 transformer의 non-local computation을 통해서 large object에 대해서는 보다 좋은 성능을 보였지만 작은 object에 대해서 성능이 안 좋아서 앞으로 보완해야할 문제라고 했다.DETR ModelObject detection set prediction lossDETR는 N개의 fixed-size set을 prediction한다. 여기서 이 N은 image 안에 있는 object를 여러개 추정하기 위해서 정한 숫자이므로 이미지상에서 존재할 수 있는 object의 수 보다 커야한다. Ground truth의 경우 object의 수 보다 N이 더 크다면 $\\emptyset$으로 padding한다. Training을 할 때 가장 어려웠던 부분이 ground truth에 대해 predicted object의 score를 class, position, size에 따라 매기는 것이었다고 한다. DETR에서의 loss는 ground truth object와 predicted object 사이에 optimal bipartite matching을 구하고 여기서의 object-specific loss가 optimize되도록 학습된다.\\[\\hat\\sigma=argmin_{\\sigma\\in\\mathfrak{G}}\\sum_{i}^N\\mathcal{L}_{match}(y_i,\\hat y_{\\sigma(i)})\\\\ \\mathcal{L}_{match}(y_i,\\hat y_{\\sigma(i)})=-\\mathbf{1}_{\\{c_i\\neq\\emptyset\\}}\\hat p_{\\sigma(i)}(c_i)+\\mathbf{1}_{\\{c_i\\neq\\emptyset\\}}\\mathcal{L}_\\mathrm{box}(b_i,\\hat b_{\\sigma(i)})\\]우선 optimial bipartite matching을 구하기 위해서 위의 식을 통해 optimal permutation index set인 $\\hat\\sigma$를 구한다. bbox와 label이 일치할수록 $\\mathcal{L}_{match}$의 값이 작아지고 이 값의 합이 최소가 되는 permutation을 구하는 것이다. 이 과정은 modern detector들에서 ground truth object에 대해 match proposal이나 anchor와 같은 heuristic assignmet rule을 사용한 것 과 같다.그 다음에는 이전에 계산한 모든 pair에 대해서 hungarian loss를 계산한다.\\[\\mathcal{L}_{Hungarian}(y,\\hat y)=\\sum_{i=1}^{N}[-log\\hat p_{\\hat\\sigma(i)}(c_i)+\\mathbf1_{\\{c_i\\neq\\emptyset\\}}\\mathcal{L}_{box}(b_i,\\hat b_{\\hat\\sigma(i)})]\\]실제로 $c_i\\neq\\emptyset$에 대해서 class imbalance 문제 때문에 log-probabilty term을 1/10으로 down-weight 하였다고 한다. 이 부분은 Faster R-CNN에서 positive/negative proposal을 subsampling으로 balance를 맞추는 부분과 유사하다.Bounding box lossmatching cost와 hungarian loss의 두번째 텀은 둘다 \\(\\mathcal{L}_{box}\\)인데 많은 detector들이 box prediction을 initial guess를 활용하는 반면 여기서는 직접적으로 prediction을 한다. 이 방법은 구현을 쉽게 해주지만 loss의 scale 문제가 있는데 아래와 같이 IoU loss와 L1 loss의 linear combination으로 loss를 정의하여 L1 loss의 scale문제를 좀 완화시켰다. $\\lambda_{iou},\\lambda_{L1}$은 hyperparameter이다.\\[\\mathcal{L}_{box}(b_i,\\hat b_{\\sigma(i)})=\\lambda_{iou}\\mathcal{L}_{iou}(b_i,\\hat b_{\\sigma(i)})+\\lambda_{L1}\\Vert b_i-\\hat b_{\\sigma(i)}\\Vert_1\\]DETR architectureDETR의 구조는 크게 3개의 component로 이뤄져 있다. CNN backbone, Encoder-Decoder transformer, simple Feed Forward Network 이다.BackboneCNN backbone(ResNet사용)은 $3\\times H_0 \\times w_0$의 image를 input으로 받아 $2048\\times \\frac{H_0}{32}\\times \\frac{W_0}{32}$의 activation map을 output으로 내보낸다.Transformer encoder먼저 $1\\times1$ convolution으로 channel dimension을 $d$로 줄이고 encoder가 sequence를 input으로 받기 때문에 feature map의 spatial dimension을 1 dimension으로 펼친다. 그 결과로 $d\\times HW$의 feature map이 나온다. 각각의 encoder layer는 standard architecture를 가지고 multi-head self-attention 모듈과 feed forward network로 구성되어 있다. Transformer architecture는 permutation-invariant 하기 때문에 각 attention layer마다 fixed positional encoding도 같이 해주었다.Transformer decoderDecoder 또한 transformer의 standard architecture를 따르며 d size의 N개의 embedding 변환한다. Original tranformer와의 차이는 N개의 object를 각 decoder layer에서 parallel하게 decode 한다는 것이다. Decoder 또한 permutation-invariant 하기 때문에 N input embedding이 서로 달라야만 다른 결과를 나타낼 수 있다. 이 input embedding은 학습된 positional encoding이며 이를 object query라고 부른다. 이 object query를 attention layer의 input에 더한다.Prediction feed-forward networks (FFN)최종 prediction은 3-layer perceptron으로 이뤄져 있고 activation function으로는 ReLU를 사용했다. FFN은 normalized center coordinate, height, width와 class label을 예측한다. 우리가 N개의 bounding box를 예측하고 보통 이 N이 이미지 상의 object수 보다 크도록 잡기 때문에 object가 잡히지 않은 box를 empty set으로 두도록 special class가 추가로 사용 되었다. 이 class는 기존의 object detection에서 “background”와 유사한 역할을 한다.Auxiliary decoding lossesTraining 과정 중 decoder에 추가로 auxiliary losses를 사용하면 model이 각 class마다 제대로 된 수의 object를 output으로 내보내는데에 도움이 된다. 이를 위해 각각의 decoder 뒤에 서로의 paramter를 공유하는 FFN prediction과 hungarian loss를 추가 하였다고 한다. 여기에 다른 decoder layer의 prediction FFN에 대한 입력을 normalize하기 위해 추가적으로 shared layer-norm을 사용했다.Code Reviewfacebookresearch/detr: End-to-End Object Detection with Transformers (github.com) 을 참고하여 코드를 같이 살펴 보았다.전체 모델은 models/detr.py 에 있는 build 함수에서 정의되는데 argument를 받아서 처음에 model을 build하고 시작한다. backbone의 경우 resnet50을 default로 사용했고 transformer의 경우 standard transformer architecture를 사용한 것이 맞다. Object query의 경우 100개를 사용 했으며가장 중요한 decoder part를 위주로 보자면 Multi-Head Self-Attentiontgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0] Multi-Head Attentiontgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos), key=self.with_pos_embed(memory, pos), value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]이렇게 크게 두가지 attention layer로 구성되어 있는데 첫번째는 self attention이라는 이름에 맞게 zero initialized된 object query에 positional embedding을 한 값과 원래 값을 self-attention을 한번 해줌으로써 서로 다른 object를 잘 찾도록 query를 한번 학습하는 역할을 하는 것으로 보인다.그 후에 Multi-Head Attention에서 진짜로 object query와 encode output, positional encoding을 사용해 attention을 하여 각각의 query가 관심있어 하는 object의 위치를 찾는 역할을 한다.그 외의 나머지 부분은 크게 읽으면서 헷갈린다거나 그랬던 부분이 없어서 넘어가려고 한다.DETR3Dpaper: [2110.06922] DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries (arxiv.org)DETR3D는 기존의 image 상에서 직접 3D bounding box를 찾거나 depth prediction을 하던 방식들과 다르게 3D space 상에서 바로 3D bounding box를 찾는 방식을 제안한다.이 논문의 main contribution은 다음과 같다. 최초의 3D set-to-set prediction으로 multi-camera detection을 하여 3D object detection 과정을 간소화 하였다. Backward geometric projection 모듈을 도입하여 2D feature extraction과 3D bounding box preditcion을 정확하게 연결지어 3D information을 관련된 모든 frame에서 사용할 수 있도록 하였다. NMS같은 추가적인 post-processing이 필요가 없으며 camera overlap region에서 다른 모델들에 비해 큰 성능 향상을 보였다.Multi-view 3D Object DetectionDETR 3D는 3개의 핵심 component로 이뤄져 있는데 shared ResNet으로 camera image들에서 feature를 추출하는 backbone, geometry-aware manner에서 2D feature와 3D bounding box set을 연결하는 detection head, 이 부분이 논문의 main contribution이라고 주장한다. Detection head의 각 layer는 data로 부터 학습된 object query로 부터 시작하고 각 object query는 3D location의 정보를 담고 있는데 이는 나중에 camera plane에 project 되어 bilinear interpolation을 통해 image feature를 가져오는데에 사용된다. DETR과 유사하게 multi-head attention을 사용해 object들의 interaction 정보를 모두 합쳐서 object query를 refinement한다. 이 layer를 여러번 거치면서 feature sampling과 object query refinement 과정이 반복된다. 마지막으로 set-to-set loss를 사용하여 network를 학습한다.Feature LearningInput으로는 K개의 image와 camera matrix$(3\\times4)$를 사용하며 bounding box의 position, size, heading angle, BEV상의 velocity, class label을 추정하도록 학습한다. 각 image들은 ResNet과 FPN을 사용하여 4개의 feature set으로 변환되며 각 feature set은 이미지들은 feature level과 관련이 있다. Multi-scale feature를 통해서 다양한 크기의 object를 인식할 수 있도록 하였다.Detection Head기존의 bottom-up 방법들은 각 이미지에서 detection 결과로 나온 bounding box들을 redundant를 제거하고 통합하는 방식으로 multi-camera image를 처리했는데 여기엔 크게 두 가지 문제가 있다고 한다. 첫 번째는 dense bounding box prediction은 정확한 depth estimation이 필요한데 이 자체가 매우 challenging한 문제이며 두 번째는 NMS 기반 redundancy 제거 및 통합 방식이 non-parallelizable해서 inference에 큰 overhead가 생긴다고 한다. 이 논문에서는 이 문제들을 top-down 방식으로 해결 했다고 한다.DETR 3D는 iterative하며 2D feature map에서 bounding box를 estimate하는 set-based computation을 하는 layer를 L개 사용한다. 각 layer는 다음과 같은 step을 따른다 object query들과 관련된 bonunding box set의 center를 predict한다. 이 center들을 camera transformation matrix들을 사용해 모든 feature map에 projection 한다. bilinear interpolation을 사용해 feature들을 가져와 object query에 합친다. multi-head attention을 사용해 object interation을 계산한다.DETR에서 아이디어를 얻어 각 layer $l\\in{0,…,L-1}$는 object query\\(\\mathcal{Q}_l=\\{q_{l1},...,q_{lM*}\\}\\subset\\mathbb{R}^C\\)를 통해 작동하며 새로운 set \\(\\mathcal{Q}_{l+1}\\)을 생성해 낸다. Object query $q_{li}$로부터 predict된 center point(reference point) $c_{li}\\in\\mathbb{R}^3$는 다음과 같다.\\[c_{li}=\\Phi^{ref}(q_{li})\\]$\\Phi^{ref}$는 neural network이다. 그 다음 최종 bounding box를 refine하고 predict하기 위해 $c_{li}$와 관련있는 image feature를 가져오기 위해 camera transformation matrix를 가지고 각 image들에 projection 한다.\\[c_{li}^*=c_{li}\\oplus1 \\qquad c_{lmi}=T_mc_{li}^*\\]$\\oplus$는 concatenation, $c_{lmi}$는 m번째 camera에 project된 point를 의미한다. Feature map의 크기에 의한 영향을 없애기 위해 $c_{lmi}$를 $[-1,1]$로 normalize한다. 그 후에 image feature는 다음과 같은 값으로 collect 된다.\\[f_{lkmi}=f^{bilinear}(\\mathcal{F}_{km},c_{lmi})\\]$f_{lkmi}$는 l번째 layer에서 얻은 i번째 point의 m번째 camera의 image에서 얻은 k번째 level feature이다. 주어진 point가 모든 camera의 image에서 보일 수 없는 경우도 있기에 invalid point를 filtering하기 위해 heuristic 정보가 필요한데 이를 위해서 binary value $\\sigma_{lkmi}$를 정의하였다. 직관적으로 point가 valid하면 1, invalid면 0이 부여될 것임을 알 수 있다. 그래서 최종 feature $f_{li}$와 다음 layer의 object query $q_{(l+1)i}$은 다음과 같이 계산된다.\\[f_{li}=\\frac{1}{\\sum_k\\sum_m\\sigma_{lkmi}+\\epsilon}\\sum_k\\sum_mf_{lkmi}\\sigma_{lkmi}\\quad and \\quad q_{(l+1)i}=f_{li}+q_{li}\\]$\\epsilon$은 0으로 나뉘는 것을 방지하기 위한 값이다. 결과적으로 각 object query $q_{li}$에 대해서 $\\Phi^{reg}l, \\Phi^{cls}$를 거쳐서 bounding box $\\hat b{li}$와 label $\\hat c_{li}$을 predict 한다.\\[\\hat b_{li}=\\Phi^{reg}_l(q_{li})\\quad and \\quad \\hat c_{li}=\\Phi^{cls}_l(q_{li})\\]Lossloss는 DETR에서 사용한 loss와 같다.총평DETR 3D라는 이름답게 object query를 3D버전으로 뽑는 컨셉의 논문이었다. 주목할만한 점은 object query과 관련된 image의 feature를 object query에 계속 더해주면서 더 object query를 정확하게 뽑고자 하는 부분인데 결과를 보니 생각보다 이게 효과가 있어 보였다.PETRpaper: [2203.05625] PETR: Position Embedding Transformation for Multi-View 3D Object Detection (arxiv.org)PETR은 3D coordinate의 position information을 image feature안에 embed하여 3D position aware feature를 생성해서 object query가 3D position-aware feature를 사용하여 더 좋은 성능을 보이도록 한 논문이다. 이 논문에서는 DETR3D에 3가지 문제점이 있다고 주장하는데 reference point가 정확하지 않은 점, projected 된 point의 feature만 수집되면서 global view의 representaion을 학습하지 못하는 점, 복잡한 sampling 절차가 실제로 사용하기에 어려운 점을 문제점으로 꼽았다.이 논문의 main contribution은 다음과 같다. 3D coordinate를 인코딩하여 multiview-feature를 3D domain으로 변환하고 object query들이 3D position-aware feature와 interaction을 하면서 update가 되는 새로운 multi-view 3D object detection framework인 PETR을 제안했다. Simple implicit function을 통해 3D position 정보를 2D multi-view feature에 인코딩 하여 3D position-aware feature를 생성하였고 이러한 representation이 3D object detection에 최초로 사용 했다고 한다.Method우선 전체적인 구조를 설명하면 input으로 N view의 image를 받아 각각 ResNet을 거쳐 image 마다 2D feature를 뽑는다. 3D coordinate generator에서는 camera frustum space를 3D meshgrid로 discretize하고 이 meshgrid를 camera parameter를 이용해 3D world space로 transform한다. 이 3D coordinate와 2D multi-view feature가 3D position encoder의 input으로 들어가 output으로 3D position-aware feature를 내보내게 된다. 이 3D position-aware feature는 query generator로 부터 생성된 object query와 interact 하며 transformer decoder로 들어가게 된다. 그 결과로 나온 값 들은 object의 class와 3D bounding box를 predict하는데 사용된다.3D Coordinates Generator2D image들과 3D space 사이의 관계를 구하기 위해서 camera frustum space의 point를 3D space로 변환해야 한다. camera frustum space의 point와 3D space의 point가 1대1 대응이기 때문에 각 camera frustum space의 point가 3D space상에서 어디에 위치하는 point인지를 아래와 같이 3D projection을 거꾸로 함으로써 계산할 수 있다.\\[p^{3d}_{i,j}=K_i^{-1}p_j^m\\]$K_i\\in R^{4\\times4}$는 i번째 view의 3D world→camera frustum space로의 변환이다. 그리고 아래 식과 같이 3D coordinate를 normalize를 추가로 해준다.\\[\\begin{cases}x_{i,j}=(x_{i,j}-x_{min})/(x_{max}-x_{min}) \\\\ y_{i,j}=(y_{i,j}-y_{min})/(y_{max}-y_{min}) \\\\ z_{i,j}=(z_{i,j}-z_{min})/(z_{max}-z_{min}) \\end{cases}\\]여기서 min, max값들은 3D world space 상에서의 RoI이다. 근데 여기서 이 3D coordinate tensor를 3D position encode의 input으로 넣어줄 때 transpose를 한다고만 써있는데 갑자기 depth에 4가 그림에서나 식에서 곱해져있다 왜 그런지를 찾아봤는데 논문에는 나와 있지 않고 코드를 봐야 알 수가 있었다. 코드(mmdet3d_plugin/models/dense_heads/petr_head.py의 position_embedding function)를 보고 이해한 바로는 단순히 3D coordinate에 대한 placeholder 개념으로 정의하는게 아니라 각 image의 모든 pixel로 부터 3D coordinate상의 pseudo lidar point를 생성하고 $H\\times W\\times D$형태의 voxel 안에 point를 부여하는 식이라 image pixel을 pseudo lidar point로 변환 했을때의 homogeneous coordinate상의 좌표 $[x,y,z,1]$가 각 voxel의 값으로 들어가서 input tensor의 크기는 $H\\times W\\times (D\\times4)$가 된다.3D Position Encoder3D position encoder의 목적은 3D position information과 2D image feature를 associate하여 3D feature를 얻는 것이다. MetaSR과 유사하게 3D position encoder는 다음과 같은 식으로 나타낼 수 있다.\\[F_i^{3d}=\\psi(F_i^{2d},P_i^{3d}),\\quad i=1,2,...,N\\]여기서 $\\psi$는 position encoding function이다. 우선 3D coordinate가 MLP를 거쳐 3D position embedding으로 변환되고 2D feature가 $1\\times1$ convolution layer를 거친 후 3D PE에 대해져서 3D position-aware feature가 만들어 진다. 그 후 transformer decoder의 key component로 사용하기 위해서 이 feature를 편다. Analysis on 3D PE3D PE의 효과를 증명하기 위해 front view에서 3개의 PE를 무작위로 선택하고 이 3개의 PE와 모든 multi-view PE 사이의 similarity를 구했는데 위와 같은 그림이 나왔다. 실제로 3D PE가 3D space 상에서 서로 다른 view에서의 position들의 상관관계를 내포하고 있다.Query Generator and Decoder Query Generator 3D scene은 워낙 search space가 넓고 sparse하다 보니 수렴이 어려워서 이를 완화하기 위해서 3D world space에 학습 가능한 anchor point들을 uniform distribution$[0,1]$로 initialize 하였다. 그리고 이 anchor point들을 small MLP를 통과시켜서 initial object query $Q_0$를 생성한다. 실제로 3D space에서 anchor point를 사용하는 것은 PETR의 수렴성을 보장할 수 있다. 이 anchor point의 효과는 실험을 통해서 알아냈다고 한다. Decoder Decoder는 DETR과 같이 standard transformer decoder를 L개의 layer로 사용하였다. Decoder layer에서의 interaction은 다음과 같이 식으로 나타낼 수 있다.\\[Q_l=\\Omega_l(F^{3d},Q_{l-1})\\] 각 layer마다 object query는 multi-head attention과 FFN을 통해 3D position-aware feature와 interact하고 여러 iteration을 거치고 난 후에 update된 object query는 corresponding object를 predict 할 수 있는 만큼 high-level repersentation을 갖게 된다. Head and LossDETR3D와 유사한 loss를 사용한다. 다만 regression에서 predict하는 값은 anchor points에 대한 relative offset이다.총평DETR3D에서 reference point를 object query로부터 predict한다는 점이 reference point를 정확하게 뽑기 어려울 것 같은 생각이 들어 매우 미심 쩍었는데 이 논문에서도 마찬가지로 비슷한 생각을 가지고 이를 해결하려고 했다. 실제로 positional embedding이 제대로 효과가 있는 것인 실험 결과도 있어서 이 방식의 positional embedding은 믿어도 될 것 같아 보인다.PETRv2paper: [2206.01256] PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images (arxiv.org)PETRv2는 PETR에서 이전 frame의 temporal 정보를 사용하여 3D object detection 성능을 향상 시켰다고 했다. 3D PE가 다른 frame의 object의 position을 temporal alignment 해주고 추가적으로 feature-guided position encoder를 사용하여 3D PE의 data adaptability를 증가시켰다고 한다. 여기에 추가적으로 segmentation patch를 query로 추가하여 BEV map상에서의 segmentation task에도 적용했다고 한다.이 논문의 main contribution은 다음과 같다. position embedding의 개념을 temporal representation learning으로 확장하였고 temporal alignment를 3D PE에서의 pose transformation을 이용해 맞췄다고 한다. 여기에 feature guided position encoder를 추가하여 3D PE를 2D image feature를 사용하여 reweight 했다고 한다. BEV segmentation task를 PETR framework에 추가하였고 좋은 성능을 보였다고 한다.Method전체적인 framework은 다음과 같다. PERT처럼 3D coordinate와 2D image feature를 뽑고 $t-1$ frame에서의 3D coordinate를 pose transformation(A)을 사용해 현재 frame $t$로 변환하고 인접한 frame들의 2D feature과 3D coordinate는 각각 concatenated(C)되고 feautre-guided position encoder(FPE)의 input으로 들어간다. 그리고 FPE는 transformer decoder에 들어가는 key와 value를 생성한다. 그 후에 detection query와 segmentation query를 각각 PETR논문에서 언급된 learnable 3D anchor point와 fixed BEV point로 initialize하여 transformer decoder에 들어가 multi-view image feature와 interaction하여 final prediction 결과를 내보낸다.Temporal Modeling여기서는 3D PE를 temporal modeling으로 확장하기 위한 3D coordinate alignment와 FPE에 대한 부분으로 나눠서 설명한다. 3D Coordinates Alignment frame $t$에서의 camera coordinate를 $c(t)$, lidar coorinate를 $l(t)$, ego coordinate를 $e(t)$라고 하고 global coordinate를 $g$라 하자. 여기서 $l(t)$를 default 3D space로 삼으면 i번째 camera에서 project된 3D point $P_i^{l(t)}(t)$는 다음과 같이 나타낼 수 있다.\\[P_i^{l(t)}(t)=T_{c_i(t)}^{l(t)}K_i^{-1}P^m(t)\\] 여기서 $P^m(t)$는 camera frustum space의 meshgrid의 point set이고 $K_i\\in R^{4\\times4}$는 intrinsic matrix이다. $t-1$의 3D point들의 coordinate를 $t$로 변환하면 다음과 같다.\\[P^{l(t)}_i(t-1)=T_{l(t-1)}^{l(t)}P_i^{l(t-1)}(t-1) \\\\ T^{l(t)}_{l(t-1)}=T^{l(t)}_{e(t)}T^{e(t)}_g{T^{e(t-1)}_g}^{-1}{T^{l(t-1)}_{e(t-1)}}^{-1}\\] 이렇게 align된 point set $[P_i^{l(t)}(t-1), P_i^{l(t)}(t)]$을 3D position embedding을 생성하는데 사용한다. Feature-guided Position Encoder PETR에서는 3D coordinate를 3D PE로 변환하는데 그 식은 아래와 같다.\\[PE_i^{3d}=\\psi(P_i^{l(t)}(t))\\] $\\psi$는 small MLP이고 식에서 처럼 PETR에서의 3D PE는 input image와 무관하다. 그러나 여기서 저자들은 image feature가 depth와 같은 유의미한 가이던스를 주기 때문에 3D PE가 2D feature로부터 만들어져야 한다고 주장 했고 그래서 vision prior를 사용한 feature-guided position encoder를 제안하였다. 그 식은 아래와 같다.\\[PE_i^{3d}(t)=\\xi(F_i(t))*\\psi(P_i^{l(t)}(t))\\] $\\xi$도 small MLP, $F_i(t)$는 i번째 camera의 image feature이다. 위의 그림처럼 2D feature는 $1\\times1$convolution으로 project된 후 MLP $\\xi$을 거치고 3D coordinates는 MLP $\\psi$를 거쳐서 attention weight을 내보내게 되고 마지막에 이 weight끼리 곱해져서 3D PE를 생성하게 된다. 이 3D PE에 2D feature가 더해져서 transformer decoder의 key값이 되고 projected 2D feature는 value 값이 된다. BEV segmentationBEV segmentation에서 PETR에 seg query를 추가하였는데 BEV map은 위의 그림처럼 small patch들로 나눌 수 있는데 이 patch들의 label을 직접 predict하는 식으로 segmentation head를 설계하였다. 위의 그림처럼 seg query들은 BEV space상에서 고정된 위치의 anchor point로 부터 initialize되고 MLP를 거쳐 학습된 seg query들을 생성한다. 이 seg query는 transformer decoder의 input으로 들어가 image feature와 interaction을 한 후에 segmentation head를 거쳐 BEV patch의 각 pixel들의 label을 predict 한다. segmantation loss는 cross-entropy loss를 사용하였다.\\[l_{seg}=\\frac{1}{N}\\sum_{j=0}^C\\sum_{i=0}^Nw\\hat{y}_ilog(y_i)+(1-\\hat{y}_i)log(l-y_i)\\]$N$는 ground truth에 존재하는 pixel 개수이고 $C$는 object category의 수 이다.Robust AnalysisPETRv2에서는 safety와 reliability를 위해 sensor error와 system bias에 대한 robustness를 평가해야 한다고 생각해서 다양한 조건에서 실험을 해야한다고 했는데 여기서 예시로 든 대표적인 3가지 sensor error는 아래와 같다. 나중에 개발할 때 이 부분을 참고해서 같이 실험을 하면 좋을 것 같다. Extrinsic noises: extrinsic calibration 값의 부정확함 Camera miss: camera의 break down이나 occlusion에 의해서 multi-view setting 중 특정 camera를 사용 할 수 없는 상태 Camera time delay: 야간과 같은 상황에서 camera 노출시간이 길어지는 것에 의한 time delay총평사실 읽으면서 PETR에서 성능을 조금이라도 향상 시키려고 온갖 기교들을 때려박고 segmentation task정도를 추가했다 정도라는 생각이 들었다. 위의 결과에서 볼 수 있듯이 CA랑 FPE 둘 다 효과가 있긴 했다." }, { "title": "Why devcontainer?", "url": "/posts/why_devcontainer/", "categories": "Setting", "tags": "Docker", "date": "2022-08-12 00:00:00 +0900", "snippet": "Devcontainer란?Devcontainer 공식문서: Developing inside a Container using Visual Studio Code Remote DevelopmentVScode에서 지원하는 도커 내에서 개발하는데 도움을 주는 개발환경 세팅 툴프로젝트 내부의 devcontainer.json 파일을 설정해 주는 것만으로 다른 사람과 쉽게 프로젝트와 개발 환경 공유가 가능해짐.그냥 컨테이너 빌드해서 공유하면 되지 않나?docker conatiner를 따로 생성하고 실행하는 것을 관리하다보면 몇 가지 작업들을 특히나 포팅을 연속으로 여러가지를 하게 되면 일일이 docker naming도 해줘야하고 image 따로 container도 관리해야 하는게 많아짐. 또한 작업 중에 사용환경도 같이 업데이트가 되고 하다보면 버전 컨트롤이 어려워짐. 그리고 무엇보다 다른 팀원들과 공유를 할때 그 팀원이 docker container를 생성할때 사용하는 flag도 shell script의 형태라든지 어떻게든지 공유를 하긴 해야함.→ 이러한 도커, 도커 설정, 엮여있는 프로젝트와 업데이트 된 환경도 git으로 관리할 수 있도록 도와주는것이 devcontainer이다. 무엇보다도 다른 사람과 공유되었을때 vscode에서 Rebuild and reopen in container 이 버튼 하나면 누구나 별다른 설정 필요 없이 같은 개발 환경에서 작업이 가능해진다. 또한 엄청난 용량을 가지는 docker를 docker hub에서 공유할 필요 없이 간단한 docker script로 공유가 가능해져서 docker 관리 차원에서도 효율적이다.어떻게 쓰는건가?간단하게 프로젝트 최상위 폴더에 .devcontainer/devcontainer.json 이 파일을 만들고 설정을 해주면 된다. 팀 내에서는 Dockerfile이 어떤식으로 만들어졌는지 공유 되는 것이 좋기도 하고 docker hub의 용량 관리의 효율을 위해 base image외에는 docker build file로 공유하는 것이 좋다. 여기서는 docker build file을 공유했기 때문에 직접 빌드해서 container를 생성하기 위해서 devcontainer안에 \"bulld\": {\"dockerfile\": \"Dockerfile\"} 이라는 옵션을 넣고 .devcontainer/Dockerfile 이 경로에 아래와 같은 docker build file을 작성했다.ARG VERSION=21.06FROM nvcr.io/nvidia/pytorch:${VERSION}-py3RUN apt-get update &amp;&amp; \\ apt-get install -qq -y \\ zsh \\ wget \\ git \\ build-essential \\\tcurlRUN apt-get install -qq -y libgl1-mesa-glx# COMMONRUN pip install wandb plotly open3d############################### FOR BEVfusion ################################## install OPENMPIRUN wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.4.tar.gz &amp;&amp; \\\ttar xvzf openmpi-4.0.4.tar.gz &amp;&amp; \\\tcd openmpi-4.0.4 &amp;&amp; \\\t./configure --prefix=/opt/openmpi --enable-mpi-threads &amp;&amp; \\\tmake all &amp;&amp; \\\tmake install &amp;&amp; \\\tcd .. &amp;&amp; \\\trm -rf openmpi-4.0.4 openmpi-4.0.4.tar.gz &amp;&amp; \\\techo \"OPENMPI_PATH=/opt/openmpi\\nMPI_HOME=\\${OPENMPI_PATH}\\nif ! echo \\${PATH} | /bin/grep -q \\${OPENMPI_PATH}/bin ; then\\n\\tPATH=\\${OPENMPI_PATH}/bin:\\${PATH}\\nfi\" &gt; /etc/profile.d/openmpi.sh &amp;&amp; \\\techo \"/opt/openmpi/lib\" &gt; /etc/ld.so.conf.d/openmpi.conf &amp;&amp; \\\tldconfigRUN pip install mmcv-full==1.4.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.9.0/index.htmlRUN pip install mpi4py==3.0.3 pillow==8.4.0 tqdm torchpack nuscenes-devkit opencv-python==4.5.5.64 mmdet==2.22.0################################################################################ Comment below lines if you don't want to use ZshRUN wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh || true \\\t&amp;&amp; git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions \\\t&amp;&amp; git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting \\\t&amp;&amp; git clone https://github.com/zsh-users/zsh-completions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-completions \\\t&amp;&amp; git clone https://github.com/supercrabtree/k ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kADD .zshrc /rootENV SHELL /bin/zsh작성한 dockerfile을 기반으로 container를 생성하기 위해서 아래와 같이 devcontainer.json에서 옵션을 설정해 주었다. 이 옵션들에 대한 자세한 내용은 다음 사이트에서 볼 수 있다.devcontainer.json reference{\t\"name\": \"bevfusion\",\t\"build\": {\t\t\"dockerfile\": \"Dockerfile\",\t},\t\"containerEnv\": {\t\t\"DISPLAY\": \"${localEnv:DISPLAY}\",\t\t\"XAUTHORITY\": \"${localEnv:XAUTHORITY}\",\t\t\"NVIDIA_VISIBLE_DEVICES\": \"all\",\t\t\"NVIDIA_DRIVER_CAPABILITIES\": \"compute,utility,graphics\",\t\t\"QT_X11_NO_MITSHM\": \"1\"\t},\t\"mounts\": [\t\t\"source=/tmp/.X11-unix,target=/tmp/.X11-unix,type=bind,consistency=cached\",\t\t\"source=/data,target=/data,type=bind\", // for local data mount\t],\t\"runArgs\": [\t\t\"--gpus=all\",\t\t\"--security-opt=seccomp=unconfined\",\t\t\"--network=host\",\t\t\"--privileged\",\t\t\"--ipc=host\",\t],\t\"settings\": {\t\t\"editor.detectIndentation\": false,\t\t\"editor.insertSpaces\": false,\t},\t\"extensions\": [\t\t\"ms-python.python\",\t\t\"kevinrose.vsc-python-indent\",\t\t\"ms-python.pylint\",\t],}이 파일들이 있는 프로젝트를 vscode로 열면 vscode의 화면 아래에 다음과 같은 창이 뜰텐데 Reopen in container 만 눌러주면 서버에서든 로컬에서든 도커에서 native환경처럼 작업할 수 있게 된다.Dockerfile과 devcontainer.json을 프로젝트와 같이 관리하면서 얻는 장점은 공동작업이나 다른 사람이 작업한것을 나중에 이어받아서 작업할때 복잡하게 누가 누구의 세팅을 도와줄 필요없이 실행이 가능하다는 점이 있다. 특히나 손뗀지 오래된 코드의 경우 나중에 다시 돌릴 때 이걸 어떻게 돌렸지 싶은 생각이 들면서 엄두가 안나서 버려지는 경우가 많은데 Reopen in container 하나로 과거의 세팅을 불러올 수 있다.그리고 dockerfile 내부에 dependency에 대한 정보가 다 들어가 있으니 documentation에 들어가는 노고가 줄게 된다. 공동작업중에 코드와 함께 dependency가 업데이트 되었을 때도 dockerfile을 같이 업데이트 해주면 rebuild 하나로 서로 골머리 아플일 없이 행복할 수 있다." }, { "title": "Targetless Calibration of LiDAR-IMU System Based on Continuous-time Batch Estimation 정리", "url": "/posts/Targetless-Calibration-of-LiDAR-IMU-System-Based-on-Continuous-time-Batch-Estimation/", "categories": "calibration", "tags": "calibration", "date": "2022-06-08 15:43:00 +0900", "snippet": "Targetless Calibration of LiDAR-IMU System Based on Continuous-time Batch Estimation링크: https://arxiv.org/pdf/2007.14759.pdfLI-Calib의 전체 pipeline요약이 논문의 알고리즘을 간단하게 요약하자면 IMU로 부터 얻은 measurement 값과 중간 과정에서 구하는 extrinsic 값을 이용해 lidar sensor의 distortion을 보정해서 map을 생성하고 생성된 map에서 얻은 plane들을 구해 lidar frame에서의 point들이 각각의 point가 속한 map 상의 plane과의 거리를 cost로 삼아 더 나은 extrinsic 값을 얻고 이를 반복하면서 extrinsic 값을 계속 개선해 나가는 것이다.Methods전체 pipeline이 돌아가는 과정을 살펴보면 다음과 같다.InitializationInitialization 과정은 처음에는 extrinsic 값에 대한 정보가 전혀 없기 때문에 랜덤한 초기값을 줘서 lidar distortion을 보정하면 map 자체가 만들어질 수 없기 때문에 초기에 어느정도 사용 할만한 extrinsic 값을 구하는 과정이다. IMU의 초기 orientation을 (0, 0, 1)로 두고 IMU measurement 값을 이용해 b-spline curve를 얻는다. Lidar에서는 ndt로 odometry를 돌려서 어느정도 pose의 변화가 생길때 까지의(코드 상에서 odometry로 구한 pose들의 covariance의 minimum eigen value가 0.25이상이 될 때 까지) trajectory를 구한다. 위에서 구한 두 pose trajectory를 이용해 lidar로 부터 얻은 pose들의 orientation과 같은 timepoint에서의 imu로 부터 얻은 orientation을 b-spline curve를 통해 일종의 interpolation 된 값을 가져와서 둘 사이의 extrinsic rotation을 initialize 한다. 자세한 내용은 아래와 같다.\\[^{I_k}_{I_{k+1}}q\\otimes^{I}_{L}q=^{I}_{L}q\\otimes^{L_k}_{L_{k+1}}q \\\\\\] 위의 식을 기반으로 하여 각각의 measurement마다 \\(\\begin{pmatrix}\\begin{bmatrix}^{I_k}_{I_{k+1}}q\\end{bmatrix}_L-\\begin{bmatrix}^{L_k}_{L_{k+1}}q\\end{bmatrix}_R\\end{pmatrix}^I_Lq=0\\) 이라는 cost를 정의하고 outlier를 처리하기 위해 각각의 cost마다 weight을 ${\\alpha_k}$로 heuristic하게 줘서 전체 식을 minimize하는 방식으로 optimization을 한다. 이 과정을 통해서 inertial to lidar extrinsic 값중에 rotation 값들만 얻게 된다.Data AssociationMap에서 구한 plane들이 과정은 Initialization 과정을 통해서 구한 extrinsic값과 IMU measurement로 구한 b-spline을 이용해서 lidar scan의 rotation으로 인한 distortion을 보정하고 frame들의 point를 쌓아서 map을 만든다. 그 후에 map에서 plane에 가까운 point들을 추출하고 map에서의 각각의 plane마다 plane에 속한 point가 lidar frame에서의 어떤 point와 일치하는지를 association하여 optimization 과정 중에 바로 사용할 수 있도록 준비하는 과정이다. 코드를 봤을때 plane추출은 전체 scene을 grid로 나누고 각 grid마다 RANSAC을 사용해 plane을 구하고 각 plane마다 plane-likeness를 구하여 plane이 아닌 부분을 걸러냈다. 그 결과는 위의 그림과 같다.이 과정을 통해 각 plane의 point들이 lidar frame상에서 어떤 point와 같은 point인지 association 되었는지를 구할 수 있다.Batch OptimizationExtrinsic 값을 refine하기 위해서는 optimization을 계속 해줘야하는데 optimization에 사용되는 cost function은 크게 세부분으로 나뉜다. angular velocity의 residual($r_{\\omega}^{k}$), acceleration의 residual($r_{a}^k$), point-to-plane distance의 residual($r_{\\mathcal{L}}^{j}$)이다. angular velocity와 acceleartion의 경우 단순하게 모든 timepoint에서 IMU의 measurement와 lidar의 measurement를 extrinsic 값을 이용해 한쪽을 transform하고 그 둘의 차이를 minimize하는 것이다. point-to-plane distance는 map frame상에서 plane과 plane에 속한 point들의 lidar frame상에서의 위치에서의 거리를 minimize 하는 것이다. 식으로 나타내면 아래와 같다.\\[\\hat{x}=argmin\\begin{Bmatrix}\\sum_{k\\in\\mathcal{A}}||r_a^k||^2_{\\Sigma_{a}} + \\sum_{k\\in\\mathcal{W}}||r_\\omega^k||^2_{\\Sigma_{\\omega}} + \\sum_{j\\in\\mathcal{L}}||r_\\mathcal{L}^j||^2_{\\Sigma_{L}} \\end{Bmatrix} \\\\r_a^k=^{I_k}a_m-^Ia(t_k)-b_a \\\\r_\\omega^k=^{I_k}\\omega_m-^I\\omega(t_k)-b_g \\\\^{L_0}p_i=^I_L{R^T}\\ \\ ^{I_0}_{I_j}R\\ \\ ^I_LR\\ \\ ^{L_j}p_i+^{L_0}p_{L_j} \\\\^{L_0}p_{L_j}=^I_L{R^T}\\ \\ ^{I_0}_{I_j}R \\ \\ ^Ip_L + ^I_L{R^T}\\ \\ ^{I_0}p_{I_j} - ^I_L{R^T}\\ \\ ^Ip_L \\\\r^j_\\mathcal{L}=\\begin{bmatrix}{^{L_0}p_i^T} &amp; {1}\\end{bmatrix}\\pi_j\\]이전에 구한 extrinsic 값과 association값을 이용해 optimization과정을 거치면 더 나은 extrinsic 값을 얻을 수 있게 된다.RefinementRefinement과정은 앞에서 나왔던 과정들을 그저 반복한다는 것에 대한 설명인데 결국은 cost들이 extrinsic 값에 dependent하기 때문에 더 나은 extrinsic 값을 얻게 된다면 angular velocity, acceleration residual은 수식에서 자명하게 줄어들 것이다. 그리고 point-to-plane distance residual의 경우 더 나아진 extrinsic 값을 이용해 lidar scan을 undistortion을 하고 이 point들을 쌓아서 mapping을 하게 되면 원래 plane인 point들에 대해서 더 명확하게 plane에 가깝게 mapping될 것이며 이 plane과 실제 point와의 거리는 lidar scan의 noise가 없다면 0이 되어야 한다. 이런식으로 data association -&gt; optimization -&gt; data association -&gt;optimization…을 반복하면서 extrinsic 값을 개선해 나가는 과정이다.총평실제 구현해서 돌린 체감상 ndt를 point를 하나도 안 버리고 처음부터 끝까지 쌓으니까 상당히 오래걸리며 mapping을 하고 그 값을 그대로 다 들고 있으니 메모리도 엄청 잡아먹는다.저자가 제공한 데이터셋 외에도 테스트를 해봤는데 우선 global 좌표계 기준으로 z방향의 오차는 전혀 못 잡았다. 그리고 NDT로 mapping의 성능에 크게 의존적인데다가 NDT가 모든 lidar의 configuration에 대해서 안정적인 알고리즘은 아니라 lidar가 크게 기울어지면 mapping이 전혀 제대로 되지를 않아 lidar pose가 망가지고 이로 인해서 첫번째 단계인 rotation initialization부터 제대로 안된다. 반대로 여기서는 hand-help로 calibration 할 수 있는 세팅으로 해서 그런지 데이터를 길게 뽑아서 사용하는 것을 전제를 하지 않았던듯 싶다. IMU의 dead rekoning의 신뢰할 수 있는 구간이 매우 짧다보니 좀만 긴 구간의 data를 사용하게 되면 imu pose + initial extrinsic 값으로 mapping을 하는 부분에서 사용된 IMU pose가 아무리 b-spline을 통해서 approximation을 했다 하더라도 제대로 나올 수가 없다보니 mapping이 이상하게 되어서 refinement가 제 기능을 하지를 못한다. 한 30~40초 정도가 안정적으로 mapping 할 수 있는 최대 길이인것 같았다. 사실 이정도 길이나 된다는 것이 오히려 의아하긴 했다. On vehicle 환경에서 data를 취득한다는 가정하에 calibration을 하기 위한 유의미한 pose 변화를 주려면 이정도 시간으로는 턱없이 부족한데 이 알고리즘은 on vehicle에서는 사용하기는 어렵고 센서랙만 따로 떼서 hand held로만 사용 가능해 보인다.그리고 나중에 알았는데 올해에 저자가 후속논문으로 OA-Licalib을 냈는데 자세히 볼지 안 볼지는 모르겠는데 일단 가볍게 훑어봤을 때는 기본적인 pipeline이 같아서 이것도 마찬가지로 on vehicle로 돌리기에는 문제가 있지 않을까 싶어 보인다." }, { "title": "Open3D c++ 설치", "url": "/posts/open3d-cpp/", "categories": "Setting", "tags": "Open3D", "date": "2021-10-21 00:00:00 +0900", "snippet": "# clone open3dgit clone --recursive https://github.com/isl-org/Open3Dcd Open3Dgit submodule update --init --recursive# install dependenciesutil/install_deps_ubuntu.sh# buildmkdir buildcd buildcmake -DCMAKE_GLIBCXX_USE_CXX11_ABI=ON ..make -j$(nproc)# installmake installDGLIBCXX_USE_CXX11_ABI=ON 을 해줘야 linker 오류 해결 가능" }, { "title": "Robust Place Recognition Using an Imaging Lidar 정리", "url": "/posts/robus_place_recognition_using_an_imaging_lidar/", "categories": "SLAM", "tags": "SLAM", "date": "2021-08-23 00:00:00 +0900", "snippet": "paper: Robust Place Recoginition using an Imaging LidarIntroduction이 논문은 3D point cloud로부터 얻은 intensity range image를 이용해 robust, real-time place recognition을 했다고 한다. ORB feature를 bag-of-word에 저장하고 이를 place recognition에 사용한다. BoW에 query를 보내 얻은 결과의 validation을 위해 visual feature desciptor matching을 하는데 여기서 PnP를 사용해 outlier rejection을 한다. 이런식으로 카메라와 라이다 방식을 둘다 활용하여 온전하게 rotation-invariant한 place recognition을 할 수 있다고 한다.이 논문의 main contribution은 다음과 같다. 최초로 intensity range image를 활용하여 real-time robust place recognition방법을 찾았다. Sensor의 pose에 invariant한 방법을 제안하였다. 다른 scale, platform, environment의 data에서 검증하여 extensive하게 검증되었다.Methodology전체적인 과정은 다음과 같다. Point cloud로부터 intensity image를 얻고 ORB feature를 추출하여 DBoW에 query하여 유사한 이미지를 추출한다. 그 후 feature matching을 하고 PnP RANSAC을 사용해 outlier rejection을 해서 validation을 한다.Intensity Image우선 point cloud $\\mathbb{P}$를 cylindrical intensity image $\\mathbb{I}$에 projection한다. 각 pixel의 값은 intensity이며 image processing에서 grayscale에서 사용하는 값의 범위와 같은 0~255 사이의 값이 되도록 normalization을 해준다. valid point가 없는 pixel의 경우 0을 부여한다.Feature Extraction이 논문에서는 다양한 scenario에 적용할 수 있도록 sensor의 orientation이 심하게 변한다고 가정하여 연구를 진행했다고 한다. 그래서 rotation invariant한 ORB(Oriented-FAST Rotated BRIEF) feature가 적합하다고 생각해서 이를 선택했다고 한다. Sensor의 움직임으로 인해 object의 scale은 sensor와 object사이의 scale이 되며 object의 orientation은 sensor orientation을 기준으로 한다. 다양한 scale과 orientation에 robust하게 feature extraction을 하기 위해서 각각 1.2의 비율로 8단계로 down sampling을 하여 다른 resolution을 가진 8개의 intensity image를 얻는다. ORB feature는 FAST algorithm으로 찾은 후에 BRIEF를 사용하여 corner feature를 descriptor로 변환한다. 그 결과로 $N_{bow}$개의 ORB feature descriptor를 얻게 되고 이를 $\\mathbb{O}$라 한다.DBoW Query이 논문에서는 ORB feature descriptor $\\mathbb{O}$를 visual vocabulary를 사용해 bag-of-word vector로 변환하며 이는 DBoW database를 만드는데 사용된다. 각각의 bag-of-word vector가 point cloud를 나타내는 것이다. 그래서 새로은 bag-of-word vector가 들어오게 되면 database에서 query하여 database에 존재하는 vector들과의 similarity를 L1 distance를 이용해 구한다. 만약 similarity가 $\\lambda_{bow}$보다 높다면 potential revisit candidate를 찾았다고 본다. 그리고 새로운 bag-of-words vector는 query후에 database에 추가된다.Feature Matching일반적으로 DBoW에 query를 날려 얻는 candidate는 많은 false detection을 포함한다. Detection을 validate하기 위해서 두 frame의 $\\mathbb{O}_i,\\mathbb{O}_j$를 사용해 matching을 한다. 이 descriptor matching은 computationally expensive하고 false match가 많이 발생하기 때문에 corner score의 내림차순으로 $\\mathbb{O}_i$에 순위를 매긴다. $N_s$개의 largest corner score를 가지는 descriptor를 선택하고 이를 $O_i$라 하고 각각 $O_i$와의 best match를 $\\mathbb{O}_j$에서 찾는다. 두 descriptor 사이의 distance는 Hamming distance를 사용해 구한다. 이 Hamming distance의 오름차순으로 matched descriptor들의 순위를 또 매긴다. 최종적으로 false match를 걸러내기 위해 $\\lambda_h$이하의 Hamming distance를 가지는 match들만 다음단계에 사용한다. 여기서 $\\lambda_h$는 smallest Hamming distance의 2배로 잡았다. 여기서 matched descriptor는 $O_i, O_j$로 표기한다. 그러나 이 과정 이후에도 여전히 false positive match들이 많이 존재하며 충분히 작은 $\\lambda_h$를 사용했음에도 불구하고 많은 true positive match들이 reject 되었다고 한다. 그래서 $N_m$개 이상의 match가 남아있는 상황이라면 이후에 PnP RANSAC을 사용해 outlier rejection을 한번 더 해준다.PnP RANSAC이전 단계에서 얻은 candiate를 validate하기 위해서 PnP 문제를 푼다. PnP 문제는 $O_i$에 존재하는 feature들의 3D Euclidean position을 알고 있고 2D image 상에서의 $O_j$에 존재하는 feature들의 position을 알기 때문에 이 correspondence들 사이의 reprojection error를 minimize하는 문제이다. 그러나 PnP는 false match에 취약하기 때문에 robustness를 위해 RANSAC을 사용하여 outlier rejection을 했다. Inlier의 수가 $N_p$ 이상 이라면 이 candidate를 correct detection이라고 판단하며 PnP의 결과로 나온 relative pose를 이용해 frame-to-frame registration에 활용할 수 있다고 한다.Experimentsprecision과 recall 모두 압도적인 성능을 보였다.SLAM 전체 과정에서 Loop-clousure detection에 소모되는 시간이 꽤 되는데 사실 마지막 registration이 지금까지 돌려본 경험 상으로는 i7 10세대 CPU에서 voxel size를 0.2m정도로 해도 거의 1000ms~2000ms 정도 소모되었던 것으로 기억하는데(물론 빨리 수렴하면 더 빠르긴 했지만…) 정확한 relative pose를 추정을 하지 않으면 의미가 없기 때문에 시간 소모는 어느정도 이하기만 하면 크게 의미는 없는 것 같다. Scan Context에 비해서 확실히 느리긴 하지만 Scan Context의 false positive 비중이 거의 사용 불가한 수준인 것을 감안하고 후의 registration 시간까지 합치면 별 차이가 없는 수준이기에 논문대로의 recall과 precision이 나와준다면 확실하게 메리트가 있는 방법인것 같다.64채널까지는 어느정도 성능이 나오는 것을 볼 수 있지만 32채널이나 16채널의 성능은 좀 많이 떨어졌다.총평camera와 lidar-based 방법들을 결합하였다고 주장하지만 사실 그냥 lidar data를 range image로 바꿔서 camera-based 돌린 방법이지 않나 싶긴하나 어쨌든 결과는 상당히 좋아서 눈여겨 볼만하다고 생각한다. Poincloud Descriptor도 BoW에 같이 포함해서 시도할만한 방법들도 떠올려 보면 좋을 것 같다.PnP의 결과로 나온 relative pose를 SLAM에 사용할 수 있을거라 얘기했는데 이에 관한 결과도 첨부 되었으면 좋을것 같은데 이게 없어서 직접 테스트를 해봐야 할 것 같다.그리고 motion distortion을 배제하기 위해서 여기서 취득한 데이터는 매우 천천히 움직였다고 하는데 구체적으로 이런 motion distortion의 영향이 어느 정도인지 이를 해결하기 위해서 range image 변환할때 어떤 처리를 할 수 있을지도 고민해 볼만한 것 같다.그리고 vertical resolution이 일정하지 않을때 이 논문의 방식대로면 sensor의 pose에 따라서 같은 물체도 형태가 달라질 소지가 충분히 있는데 이를 좀 반영할 수 있는 range image building 방법이 필요할 것 같다." }, { "title": "Region Growing Segmentation 정리", "url": "/posts/region-growing-segmentation/", "categories": "Segmentation", "tags": "Machine Learning, Pointcloud", "date": "2021-07-24 00:00:00 +0900", "snippet": "Reference: https://pcl.readthedocs.io/projects/tutorials/en/latest/region_growing_segmentation.htmlPCL에서 사용되는 Region Growing Segmentation의 방법론을 pcl 공식 문서를 참조하여 정리하였다. Algorithm 개요 Inputs: Point cloud=$\\{P\\}$ Point normals=$\\{N\\}$ Point curvatures=$\\{C\\}$ Neighbor finding function $\\Omega(.)$ Curvature threshold $c_{th}$ Angle threshold $\\theta_{th}$ Initialize: Region list $R\\leftarrow\\emptyset$ Available points list $\\{A\\}\\leftarrow\\{1,…,|P|\\}$ Algorithm: While $\\{A\\}$ is not empty do Current region $\\{R_c\\}\\leftarrow\\emptyset$ Current seeds $\\{S_c\\}\\leftarrow\\emptyset$ Point with minimum curvature in $\\{A\\}\\rightarrow P_{min}$ $\\{S_c\\}\\leftarrow\\{S_c\\}\\cup P_{min}$ $\\{R_c\\}\\leftarrow\\{R_c\\}\\cup P_{min}$ $\\{A\\}\\leftarrow\\{A\\}-P_{min}$ for $i=0$ to size $(\\{S_c\\})$ do Find nearest neighbors of current seed point $\\{B_c\\}\\leftarrow\\Omega(S_c\\{i\\})$ for $j=0$ to size $(\\{B_c\\})$ do Current neighbor point $P_j\\leftarrow B_c\\{j\\}$ If $\\{A\\}$ contains $P_j$ and $cos^{-1} ( | ( N \\{ S_c \\{ i \\} \\},N \\{ S_c \\{ j \\} \\} ) | ) &lt; \\theta_{th}$ then $\\{R_c\\}\\leftarrow\\{R_c\\}\\cup P_j$ $\\{A\\}\\leftarrow\\{A\\}-P_j$ If $c\\{P_j\\}&lt;c_{th}$ then $\\{S_c\\}\\leftarrow\\{S_c\\}\\cup P_j$ end if end if end for end for Add current region to global segment list $\\{R\\}\\leftarrow\\{R\\}\\cup\\{R_c\\}$ end while Return $\\{R\\}$ 간단하게 정리하자면 minimum curvature를 가지는 point를 seed point로 삼아 nearest point들을 얻어서 normal의 cosine distance가 threshold 값 이하라면 같은 region에 포함시키는 방식으로 segmentation을 하는 알고리즘이다." }, { "title": "LiDAR IRIS 정리", "url": "/posts/lidar_iris/", "categories": "SLAM", "tags": "SLAM", "date": "2021-07-09 00:00:00 +0900", "snippet": "paper: LiDAR Iris for Loop-Closure DetectionLiDAR Iris는 빠르고 정확한 loop-closure detection을 위해 LiDAR-Iris image에 LoG-Gabor filter와 thresholding을 적용해서 binary signature image를 feature로 사용한다. 두 이미지의 hamming distance를 이용해 두 point cloud사이의 similarity를 fourier transform을 이용해 descriptor level에서 pose invariant하게 구할 수 있다고 한다.기존의 global descriptor와 local descriptor는 각각 pose invariance문제와 descriptive power가 약하다는 문제가 존재했으며 learning-based의 경우 training data가 매우 많아야하고 다른 조건에서 취득된 데이터나 topology가 다양한 데이터에 대해서 generalize가 잘 되지 않는다는 문제가 있다고 해서 이를 해결하려고 한다고 한다.이 방법은 Scan Context와 유사한 방식이지만 크게 세 가지 차이점이 있다고 한다. 주변의 높이 정보를 LiDAR-Iris의 pixel intensity로 삼았다. Loop-closure detection을 위해 LiDAR-Iris image로부터 discriminative한 binary feature map을 얻었다고 한다. Scan Context처럼 brute force matching을 할 필요 없이 LiDAR의 pose에 invariant한 loop-closure detection을 한다고 한다.Lidar IrisGeneration of LiDAR-Iris Image RepresentationBird eye view로 point cloud를 projection하고 range와 yaw 값으로 discretization을 한다. pointcloud를 완벽하게 표현하기 위해서는 각각의 bin에 대해서 height, range, reflection, ring과 같은 정보를 담도록 해야한다. 이를 간단하게 하기 위해 같은 bin에 들어가는 모든 point의 정보를 8 bit로 encoding했다고 한다. 우선 scan context와 마찬가지로 angle, range를 기준으로 영역을 나누고 각 영역마다 값을 부여한다. 각 영역의 값을 구하기 위해 point의 최고, 최저 높이인 $y_h,y_l$을 센서마다 적당히 정해서 그 사이를 8등분 하고 각 영역마다 또 binary로 값을 부여할 수 있게 하는데 이를 $y_k, k\\in[1, 8]$이라 한다. 그리고 $y_k$에 해당하는 영역 내에 point가 있다면 1, 없다면 0을 부여하여 binning을 하여 구한 8 bit의 이진수가 해당 $(r,\\theta)$ 영역의 값이 된다.Iris recognition의 영향을 받아서 위의 그림과 같이 이 이미지를 strip형태로 폈다고 한다.Fourier transfom for a translation-invariant LiDAR IrisTranslation variation이 Lidar Iris Image를 matching할때 성능을 낮추는 요인중 하나가 될 수 있다. 이를 해결하기 위해 두 image사이의 translation을 estimate하기 위해서 Fourier Transform을 사용하였다. Fourier-based scheme에서는 rotation, scaling, translation을 모두 estimate할 수 있다. Frequency domain 상에서 rotation은 horizontal traslation, translation은 vertical translation으로 이어지면서 image pixel의 intensity에 변화를 준다. 이 점을 활용하여 FT를 한 후의 cross power spectrum의 inverse FT가 non-zero가 되는 frequency domain image상의 shift$(\\delta_x, \\delta_y)$ 를 구해 translation과 rotation을 구한다. 식은 다음과 같다.\\[\\hat{I_1}(w_x,w_y)\\dot{e}^{i(w_x\\delta_x+w_y\\delta_y)}=\\hat{I_2}(w_x,w_y)\\]\\[\\hat{Corr}=\\frac{\\hat{I_2}(w_x,w_y)}{\\hat{I_1}(w_x,w_y)}=\\frac{\\hat{I_2}(w_x,w_y)\\hat{I_1}(w_x,w_y)*}{|\\hat{I_2}(w_x,w_y)\\hat{I_1}(w_x,w_y)*|}=e^{-i(w_x\\delta_x+w_y\\delta_y)}\\]\\[Corr(x,y)=F^{-1}(\\hat{Corr})=\\delta(x-\\delta_x,y-\\delta_y)\\]\\[(\\delta_x,\\delta_y)=argmax_{x,y}\\{Corr(x,y)\\}\\]Binary feature extraction with LoG-Gabor filtersRepresenation ability를 높이기 위해 LoG-Gabor filter를 사용해 Lidar Iris image로부터 추가적인 feature를 뽑아낸다. LoG-Gabor filter는 다양한 resolution으로 LiDAR IRIS 영역안에 data를 나타낼 수 있어서 fourier transform에 비해 같은 resolution과 position을 가지는 feature를 매칭하기 유리하다. Fourier transform의 frequency data는 매우 국소적(delta 함수)이기 때문이다. 여기서 1D LoG-Gabor filter를 사용하였다.\\[G(f)=exp(\\frac{-(\\log(f/f_0))^2}{2(log(\\sigma/f_0))^2})\\]$f_0$는 center frequency, $\\sigma$는 bandwidth로 filter의 parameter이다. Iris image의 각 row마다 1D LoG-Gabor filter를 적용하였으며 사용한 fillter는 4개이다. 여러가지 갯수의 filter를 사용해봤으나 4개일때가 결과가 제일 좋았다고 한다.Loop-Closure Detection with Lidar IRISLoop closure detection 과정에서는 log gabor filter를 이용해 구한 binary feature map 사이의 hamming distance를 구해 이 distance의 threshold 이하의 frame을 loop로 삼는다.총평극 좌표계를 사용해서 binning을 하면 어떤 식으로 translation error를 해결을 하나 싶었는데 이 논문에서는 fourier transform을 사용해서 해결하고자 한 것 같은데 여전히 frequency domain상에서의 vertical translation이 실제 translation 차이를 반영하는데 충분한 가에 대해서는 의문이 든다. 코드를 쭉 봤을 때는 보통 loop closure detection이 도는데 시간이 오래 걸려서 2-stage로 돌릴텐데 여기서는 그 부분에 대한 고려가 없이 전체 frame에 대해서 IRIS를 돌리는 것 같아 보였다. 실제 사용하려면 scan context에서 ring key로 candidate를 뽑는 부분 뒤에 scan matching을 하는 부분을 대체해서 써야 할 것 같다." }, { "title": "Closed-form solution of absolute orientation using unit quaternions, orthonormal matrices 정리", "url": "/posts/closed-form-solution-registration/", "categories": "Pointcloud Registration", "tags": "Pointcloud, Rigid", "date": "2020-12-15 00:00:00 +0900", "snippet": "이 논문들은 pointcloud registration의 조상격인 논문이다. 두 좌표계에 존재하는 point들의 registration 문제를 least square 문제로 바꿔서 iterative하게가 아닌 closed form으로 푸는 해법을 제시한 논문이라고 보면 된다.Closed-form solution of absolute orientation using unit quaternionspaper: Closed-form solution of absolute orientation using unit quaternions여기서는 solution을 unit quaternion의 형태로 rotation을 표현하며 4x4 matrix의 최대 eigenvalue의 eigenvector가 desired quaternion이 된다. 본 논문에서는 source와 target을 left와 right coordinate라고 표현하였다. Method 앞으로 나오겠지만 scale factor와 translation은 rotation만 구하면 쉽게 구할 수 있으나 rotation을 구하는 것이 어렵다. Selective Discarding Constraints 여기서는 3개의 point를(논문에서는 triad라 표현) 사용하여 각각의 coordinate를 기준으로 이 point들의 coordinate를 계산하였다. 이 triad point들의 left와 right 좌표계에서의 표현을 $r_{l,1},r_{l,2},r_{l,3},r_{r,1},r_{r,2},r_{r,3}$라 나타내었다.\\[x_l=r_{l,2}-r_{l,1}, y_l=(r_{l,3}-r_{l,1})-[(r_{l,3}-r_{l,1})\\cdot\\hat{x_l}]\\hat{x_l}, \\hat{z_l}=\\hat{x_l}\\times\\hat{y_l} \\\\ x_r=r_{r,2}-r_{r,1}, y_r=(r_{r,3}-r_{r,1})-[(r_{r,3}-r_{r,1})\\cdot\\hat{x_r}]\\hat{x_r}, \\hat{z_r}=\\hat{x_r}\\times\\hat{y_r}\\] 여기서 hat이 붙은 vector들은 unit vector이다. 그리고 이 column vector들을 합쳐서 matrix를 만드는데 다음과 같이 만든다.\\[M_l=|\\hat{x_l}\\hat{y_l}\\hat{z_l}|, M_r=|\\hat{x_r}\\hat{y_r}\\hat{z_r}|\\] 그리고 left coordinate상에서의 vector $r_l$을 이 triad point들의 coordinate로 변환하면 $M_l^Tr_l$이 되고 여기에 $M_r$를 곱하면 이 값들을 right coordinate에 mapping이 된다. 따라서\\[r_r=M_rM_l^Tr_l\\] 이 된다. 이를 통해 rotation은 다음과 같이 주어지는 것을 알 수 있다.\\[R=M_rM_l^T\\] $M_r,M_l$은 만드는 과정에서 알 수 있듯이 orthonormal이므로 $R$ 또한 orthonormal이 된다. 이런식으로 rotation을 구하는 것은 쉽지만 data가 완벽하지 않을 경우 어떤식으로 3개의 point를 선택하느냐에 따라서 rotation matrix가 달라지므로 3개 이상의 point에 대해서는 확장을 하기가 어렵다. 그래서 이를 보완한 optimum rotation을 구하는 방법은 추후에 소개한다고 한다. Finding the Translation n개의 point가 존재한다고 할때 left와 right coordinate 에서의 point를 다음과 같이 나타낸다고 하자.\\[\\{r_{l,i}\\},\\{r_{r,i}\\}\\] 그리고 여기서 우리는 다음을 만족하는 transformation을 찾고자 한다.\\[r_r=sR(r_l)+r_0\\] 여기서 $s$는 scale factor $r_0$는 translational offset, $R(r_l)$은 $r_l$을 rotation한다는 의미이다. 모든 rotation에 대해서 rotation matrix는 orthonormal이므로 다음을 만족한다.\\[||R(r_l)||^2=||r_l||^2\\] 그리고 data가 완벽하지 않다면 최적의 transformation을 한다고 해도 residual error가 존재할 수밖에 없다. 이 residual error은 아래와 같이 나타낸다.\\[e_i=r_{r,i}-sR(r_{r,i})-r_0\\] 그리고 이 residual error의 sum을 minimize하는 것이 registration 문제를 푸는 것과 같다.\\[\\sum_{i=1}^{n}||e_i||^2\\] 앞으로 우선은 translation에 대해서 그 다음은 scale 마지막으로 roataion에 대해서 total error가 달라지는 것을 살펴볼 것이다. Centroids of the Sets of Measurements 각각의 coordinates 상의 point에 대해서 centroid $\\bar{r_l},\\bar{r_r}$를 계산하면 다음과 같다.\\[\\bar{r_l}=\\frac{1}{n}\\sum_{i=1}^{n}r_{l,i}, \\bar{r_r}=\\frac{1}{n}\\sum_{i=1}^{n}r_{r,i}\\] 그리고 이 centroid들을 중심으로 새로운 coordinate를 정의하고 여기서 point들을 나타내면 다음과 같아진다.\\[r^{\\prime}_{l,i}=r_{l,i}-\\bar{r_l}, r_{r,i}^{\\prime}=r_{r,i}-\\bar{r_r} \\\\ \\sum_{i=1}^{n}r_{l,i}^{\\prime}=0, \\sum_{i=1}^{n}r_{r,i}^{\\prime}=0\\] 그리고 residual error도 이 coordinate상으로 바꿔서 표현하면 다음과 같다.\\[e_i=r_{r,i}^{\\prime}-sR(r_{l,i}^{\\prime})-r_{0}^{\\prime} \\\\ r_{0}^{\\prime}=r_{o}-\\bar{r_r}+sR(\\bar{r_l}) \\\\ \\sum_{i=1}^{n}||r_{r,i}^{\\prime}-sR(r_{l,i}^{\\prime})-r_{0}^{\\prime}||^{2} \\\\ \\sum_{i=1}^{n}||r_{r,i}^{\\prime}-sR(r_{l,i}^{\\prime})||^2-2r_{0}^{\\prime}\\cdot\\sum_{i=1}^{n}[r_{r,i}^{\\prime}-sR(r^{\\prime}_{l,i})]+n||r_{0}^{\\prime}||^2\\] 여기서 두번째 term은 centroid를 중심으로 coordinate를 옮겼으므로 0이 될 것이고 첫번째 term의 경우 translation과 상관 없는 term이며 마지막 term은 non-negative이므로 translation에 대해서 $r_0^{\\prime}=0$일때 total error가 minimize되며\\[r_0=\\bar{r_r}-sR(\\bar{r_l})\\] 이 된다. 따라서 translation은 단순히 right centroid와 scaled, rotated left centroid 사이의 difference가 된다. 이 식은 scale과 rotation을 구한 이후에 translational offset을 구할때에 사용된다. 그리고 여기에서 $r_0^{\\prime}=0$이므로 error term은 다음과 같이 나타낼 수 있다.\\[e_i=r_{r,i}^{\\prime}-sR(r_{l,i}^{\\prime}) \\\\ \\sum_{i=1}^{n}||r_{r,i}^{\\prime}-sR(r_{l,i}^{\\prime})||^2\\] Finding the Scale \\(||R(r_{l,i}^{\\prime})||^2=||r_{l,i}^{\\prime}||\\) 이므로 total error를 전개하면\\[\\sum_{i=1}^{n}||r_{r,i}^{\\prime}||^2-2s\\sum_{i=1}^{n}r_{r,i}^{\\prime}\\cdot R(r_{l,i}^{\\prime})+s^2\\sum_{i=1}^{n}||r_{l,i}^{\\prime}||^2\\] 편의성을 위해 위의 식을 간단하게 나타내고 완전제곱식으로 바꾸면 다음과 같다.\\[S_r-2sD+s^2S_l \\\\ =(s\\sqrt{S_l}-D/\\sqrt{S_l})^2+(S_rS_l-D^2)/S_l\\] 이 식을 minimize하기 위해서는 $s=D/S_l$이 되어야 한다.\\[s=\\frac{\\sum\\limits_{i=1}^{n}r_{r,i}^{\\prime}\\cdot R(r_{l,i}^{\\prime})}{\\sum\\limits_{i=1}^{n}||r_{l,i}^{\\prime}||}\\] Symmetry in Scale transformation은 inverse form의 형태로 대칭적으로 다음과 같이 나타낼 수 있다.\\[r_r=sR(r_l)+r_0 \\\\ r_l=\\bar{s}\\bar{R}(r_r)+\\bar{r_0} \\\\ \\bar{s}=1/s, \\bar{r_0}=-\\frac{1}{s}R^{-1}(r_0), \\bar{R}=R^{-1}\\] inverse transform에 대해서 위와 같은 식으로 $\\bar{s}=\\bar{D}/S_r$을 찾아서 구하게 되면 $\\bar{s}\\neq 1/s$ 가 된다. 그 이유는 위의 방식으로 하면 구해지는 scale이 각각의 coordinate에서의 measurement에 depend하기 때문이다. 위의 두 결과가 asymmetric해도 한쪽의 presision이 다른쪽에 비해서 매우 높다면 적절한 결과를 얻은 것이지만 두 measurement에서의 eror가 유사하다면 다음과 같은 symmetric error term을 사용하는 것이 더 합리적이다.\\[e_i=\\frac{1}{\\sqrt{s}}r_{r,i}^{\\prime}-\\sqrt{s}R(r_{l,i}^{\\prime})\\] 위의 term을 사용하면 total error는 다음과 같아지며\\[\\frac{1}{s}\\sum_{i=1}^{n}||r_{r,i}^{\\prime}||^2-s\\sum_{i=1}^{n}r_{r,i}^{\\prime}\\cdot R(r_{l,i}^{\\prime})+s\\sum_{i=1}^{n}||r_{l,i}^{\\prime}||^2 \\\\ =\\frac{1}{s}S_r-2D+sS_l \\\\ =(\\sqrt{s}S_l-\\frac{1}{\\sqrt{s}}S_r)^2+2(S_lS_r-D)\\] $s=S_r/S_l$일때 최소가 된다.\\[s=(\\sum_{i=1}^{n}||r_{r,i}^{\\prime}||^2/\\sum_{i=1}^{n}||r_{l,i}^{\\prime}||^2)^{1/2}\\] 이 symmetrical result의 또 다른 장점은 rotation에 대해서 알 필요가 없이 scale을 구할 수 있다는 점이며 또한 scale의 결과가 rotation을 구하는데에 영향을 주지 않는다는 것이다. 그래서 결국 남은 error term을 minimize하기 위해서는 $D$를 maximize하는 rotation을 찾아야 한다. 즉 아래의 식을 maximize하는 rotation을 찾으면 된다.\\[\\sum_{i=1}^{n}r_{r,i}^{\\prime}\\cdot R({r^{\\prime}_{l,i})}\\] Finding the Best Rotation 우선 본 논문에서는 rotation representation으로 quaternion을 사용했는데 그 이유는 unit quaternion 이라는 constratint가 rotation matrix가 orthonormal이라는 constraint보다 간단하기 때문이라고 한다. 그리고 unit quaternion의 axis와 angle notation geometric 관점에서 더 직관적이기 때문이다. 위의 식을 quaternion notation으로 바꾸면 다음과 같다.\\[\\sum_{i=1}^{n}(\\dot{q}\\dot{r^{\\prime}_{l,i}\\dot{q}^*})\\cdot\\dot{r}^{\\prime}_{r,i}\\] 이렇게 바꾸고 나면 위의 식을 maximize하는 unit quaternion $\\dot{q}$를 찾는 문제가 된다. 그리고 이 식을 변형하면\\[\\sum_{i=1}^{n}(\\dot{q}\\dot{r^{\\prime}_{l,i}\\dot{q}^*})\\cdot\\dot{r}^{\\prime}_{r,i} \\\\ = \\sum_{i=1}^{n}(\\bar{Q}^{T}Q\\dot{r}^{\\prime}_{l,i})^T\\dot{r}^{\\prime}_{r,i} \\\\ =\\sum_{i=1}^{n}(Q\\dot{r}_{l,i}^{\\prime})^T\\bar{Q}\\dot{r}^{\\prime}_{r} \\\\ =\\sum_{i=1}^{n}(\\dot{q}\\dot{r}^{\\prime}_{l,i})\\cdot(\\dot{r}^{\\prime}_{r,i}\\dot{q}) \\\\ = \\sum_{i=1}^{n}(\\bar{\\mathbb{R}}_{l,i}\\dot{q})\\cdot(\\mathbb{R}_{r,i}\\dot{q}) \\\\ = \\sum_{i=1}^{n}\\dot{q}^T\\bar{\\mathbb{R}}_{l,i}^T\\mathbb{R}_{r,i}\\dot{q} \\\\=\\dot{q}^T(\\sum_{i=1}^{n}\\bar{\\mathbb{R}}_{l,i}^T\\mathbb{R}_{r,i})\\dot{q} \\\\=\\dot{q}^T(\\sum_{i=1}^{n}N_i)\\dot{q}\\\\=\\dot{q}^TN\\dot{q}\\] 이고 여기서\\[\\bar{\\mathbb{R}}_{l,i}=\\begin{bmatrix}0 &amp; -x_{l,i}^{\\prime} &amp; -y^{\\prime}_{l,i} &amp; -z^{\\prime}_{l,i} \\\\ x^{\\prime}_{l,i} &amp; 0 &amp; z^{\\prime}_{l,i} &amp; -y^{\\prime}_{l,i} \\\\ y^{\\prime}_{l,i} &amp; -z^{\\prime}_{l,i} &amp; 0 &amp; x^{\\prime}_{l,i} \\\\ z^{\\prime}_{l,i} &amp; y^{\\prime}_{l,i} &amp; -x^{\\prime}_{l,i} &amp; 0 \\end{bmatrix},\\ \\mathbb{R}_{r,i} = \\begin{bmatrix} 0 &amp; -x^{\\prime}_{r,i} &amp; -y ^{\\prime}_{r,i} &amp; -z^{\\prime}_{r,i} \\\\ x^{\\prime}_{r,i} &amp; 0 &amp; -z^{\\prime}_{r,i} &amp; y^{\\prime}_{r,i} \\\\ y^{\\prime}_{r,i} &amp; z^{\\prime}_{r,i} &amp; 0 &amp; -x^{\\prime}_{r,i} \\\\ z^{\\prime}_{r,i} &amp; -y^{\\prime}_{r,i} &amp; x^{\\prime}_{r,i} &amp; 0 \\end{bmatrix} \\\\ N_i=\\bar{\\mathbb{R}}_{l,i}^T\\mathbb{R}_{r,i},\\ N=\\sum_{i=1}^{n}N_i\\] 이며 각각의 $N_i$들이 symmetric이므로 $N$ 또한 symmetric이다. 여기서 계산의 편리성을 위해서 다음과 같은 3$\\times$3 matrix를 도입한다.\\[M=\\sum_{i=1}^{n}r^{\\prime}_{l,i}r^{\\prime \\ T}_{r,i} \\\\ M=\\begin{bmatrix} S_{xx} &amp; S_{xy}&amp;S_{xz}\\\\ S_{yx}&amp;S_{yy}&amp;S_{yz}\\\\ S_{zx}&amp;S_{zy}&amp;S_{zz}\\end{bmatrix},\\ S_{xx}=\\sum_{i=1}^{n}x^{\\prime}_{l,i}x^{\\prime}_{r,i}\\] 그리고 이 matrix $M$의 성분들을 이용해 $N$을 표현하면 다음과 같다.\\[N=\\begin{bmatrix}(S_{xx}+S_{yy}+S_{zz})&amp;S_{yz}-S_{zy}&amp;S_{zx}-S_{xz}&amp;S_{xy}-S_{yx}\\\\ S_{yz}-S_{zy} &amp; (S_{xx}-S_{yy}-S_{zz})&amp;S_{xy}+S_{yx}&amp;S_{zx}+S_{xz}\\\\ S_{zx}-S_{xz} &amp; S_{xy}+S_{yx}&amp;(-S_{xx}+S_{yy}-S_{zz})&amp;S_{yz}+S_{zy}\\\\ S_{xy}-S_{yx}&amp;S_{zx}+S_{xz}&amp;S_{yz}+S_{zy}&amp;(-S_{xx}-S_{yy}+S_{zz})\\end{bmatrix}\\] 이렇게 symmetric matrix $N$의 10개의 independent element를 $M$의 element를 이용해 나타낼 수 있으며 $Tr(N)=0$ 이다. $N$은 4$\\times$4 symmetric matrix이므로 4개의 real eigenvalue를 가지며 이를 크기 순으로 $\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4$라 하면 이와 관련된 unit eigenvector $\\dot{e}_1,\\dot{e}_2,\\dot{e}_3,\\dot{e}_4$에 대해서 다음을 만족한다.\\[N\\dot{e}_i=\\lambda_i\\dot{e}_i,\\ i=1,2,3,4\\] 임의의 quaternion $\\dot{q}$는 이 eigenvector를 이용해 다음과 같은 linear combination으로 나타낼 수 있으며\\[\\dot{q}=\\alpha_1\\dot{e}_1+\\alpha_2\\dot{e}_2+\\alpha_3\\dot{e}_3+\\alpha_4\\dot{e}_4\\] 이와 같은 식으로 $\\dot{q}^TN\\dot{q}$를 계산하면\\[N\\dot{q}=\\alpha_1\\lambda_1\\dot{e}_1+\\alpha_2\\lambda_2\\dot{e}_2+\\alpha_3\\lambda_3\\dot{e}_3+\\alpha_4\\lambda_4\\dot{e}_4 \\\\ \\dot{q}^TN\\dot{q}=\\dot{q}\\cdot(N\\dot{q})=\\alpha_1^2\\lambda_1+\\alpha_2^2\\lambda_2+\\alpha_3^2\\lambda_3+\\alpha_4^2\\lambda_4\\] 그리고 eigenvalue를 크기순으로 $\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3\\ge\\lambda_4$ 이렇게 정했으며 $\\dot{q}$가 unit quaternion 이므로 다음을 만족한다.\\[\\dot{q}^TN\\dot{q}\\le\\alpha_1^2\\lambda_1+\\alpha_2^2\\lambda_1+\\alpha_3^2\\lambda_1+\\alpha_4^2\\lambda_1=\\lambda_1\\] 따라서 $\\dot{q}^TN\\dot{q}$은 $N$의 가장 큰 eigenvalue $\\lambda_1$보다 커질 수 없으며 이를 최대화 하기 위해서는 $\\alpha_1=1,\\ \\alpha_2=\\alpha_3=\\alpha_4=0$이 되도록 해야 하므로 $\\dot{q}=\\dot{e}_1$이 residual error를 minimize하는 unit quaternion이 된다. 그리고 위에 구한 matrix들을 활용하여 이 eigenvector와 eigenvalue를 구하는 식은 다음과 같다.\\[\\det(N-\\lambda I)=0 \\\\ [N-\\lambda_mI]\\dot{e}_m=0\\\\\\lambda^4+c_3\\lambda^3+c_2\\lambda^2+c_1\\lambda+c_0=0\\\\c_3=0, c_2=-2Tr(M^TM),c_1=-8\\det(M),c_0=\\det(N)\\] Closed-form solution of absolute orientation using orthonormal matricespaper: Closed-form solution of absolute orientation using orthonormal matrices이 논문은 같은 저자가 1년뒤에 쓴 논문인데 앞에서 translation과 scale을 구하는 과정까지는 동일하며 rotation을 구하는 부분에서 quaternion representation 대신 orthonormal matrix representation을 사용해서 해결하였다. 따라서 앞부분은 생략하고 residual error를 minimize하기 위해서 maximize 해야하는 $D$를 다시 살펴보면 다음과 같다.\\[\\sum_{i=1}^{n}r_{r,i}^{\\prime}\\cdot R({r^{\\prime}_{l,i})}\\] Dealing with Rodation 여기서 $a^TRb=Tr(R^Tab^T)$임을 이용하여(계산해보면 나온다) 위의 식을 정리하면\\[\\sum_{i=1}^{n}r_{r,i}^{\\prime}\\cdot R({r^{\\prime}_{l,i})}\\\\=\\sum_{i=1}^{n}(r^{\\prime}_{r,i})^TR(r_{l,i}^{\\prime})\\\\=Tr\\begin{bmatrix}R^T\\sum\\limits_{i=1}^{n}r^{\\prime}_{r,i}(r^{\\prime}_{l,i})^T\\end{bmatrix}\\\\=Tr(R^TM)\\] 이고 여기서 $M$은 앞의 unit quaternion에서 구한 $M$과 같다.\\[M=\\sum_{i=1}^{n}r^{\\prime}_{r,i}(r^{\\prime}_{l,i})^T \\\\ M=\\begin{bmatrix} S_{xx} &amp; S_{xy}&amp;S_{xz}\\\\ S_{yx}&amp;S_{yy}&amp;S_{yz}\\\\ S_{zx}&amp;S_{zy}&amp;S_{zz}\\end{bmatrix},\\ S_{xx}=\\sum_{i=1}^{n}x^{\\prime}_{r,i}x^{\\prime}_{l,i}\\] 여기서 $Tr(R^TM)$을 maximize하는 orthonormal matrix $R$을 찾으면 된다. 모든 square matrix는 orthonormal matrix $U$와 positive semidefinite matrix S로 decomposition할 수 있으며 matrix가 nonsingular라면 $U$가 uniquely determined 된다. 그래서 $M$이 nonsingular라면 다음과 같이 나타낼 수 있다.\\[M=US\\\\S=(M^TM)^{1/2},\\ U=M(M^TM)^{-1/2},\\ S^T=S,\\ U^TU=I\\] Symmetric matrix $S$ 우선 $M^TM$을 eigenvalue와 eigenvector를 이용해 표현하면 다음과 같다.\\[M^TM=\\lambda_1\\hat{u}_1\\hat{u_1}^T+\\lambda_2\\hat{u}_2\\hat{u_2}^T+\\lambda_3\\hat{u}_3\\hat{u_3}^T\\] 그리고 $M^TM$은 positive definite이므로 eigenvalue들이 positive이므로 eigenvalue의 square root들은 real value이며 다음과 같은 symmetric matrix $S$는 다음과 같이 나타낼 수 있다.\\[S=\\sqrt{\\lambda_{1}}\\hat{u}_1\\hat{u}_1^T+\\sqrt{\\lambda_{2}}\\hat{u}_2\\hat{u}_2^T+\\sqrt{\\lambda_{3}}\\hat{u}_3\\hat{u}_3^T\\] $S^2=M^TM$임은 쉽게 알 수 있다. 그리고 $S$ 또한 positive definite 임은 다음과 같은 식으로 알 수 있다.\\[x^TSx=\\sqrt{\\lambda_1}(\\hat{u}_1\\cdot x)^2+\\sqrt{\\lambda_2}(\\hat{u}_2\\cdot x)^2+\\sqrt{\\lambda_3}(\\hat{u}_3\\cdot x)^2 &gt; 0\\] Orthonormal matrix U 모든 eigenvalue가 positive라면 다음과 같이 나타낼 수 있다.\\[S^{-1}=(M^TM)^{-1/2}=\\frac{1}{\\sqrt{\\lambda_1}}\\hat{u}_1\\hat{u}_1^T + \\frac{1}{\\sqrt{\\lambda_2}}\\hat{u}_2\\hat{u}_2^T + \\frac{1}{\\sqrt{\\lambda_3}}\\hat{u}_3\\hat{u}_3^T\\] 그리고 다음과 같은 식으로 $U$을 구할 수 있다.\\[U=MS^{-1}=M(M^TM)^{-1/2}\\] 여기에 determinant를 적용하면\\[\\det(U)=\\det(MS^{-1})=\\det(M)\\det(S^{-1})\\] $\\det(S^{-1})&gt;0$ 이므로 $\\det(U)$와 $\\det(M)$의 부호는 같으며 $\\det(M)&gt;0$ 이면 순수 rotation을 나타내고 $\\det(M)&lt;0$ 이면 reflection이 포함된 rotation을 나타낸다. 여기서는 data에 대해 rotation만 일어난다고 가정한다. 그리고 만약 $M$의 rank가 2라면 위의 방식으로 orthonormal matrix를 구할 수 없다. 그래서 대신 다음과 같은 식으로 구한다.\\[U=M\\begin{pmatrix}\\frac{1}{\\lambda_1}\\hat{u}_1\\hat{u}_1^T+\\frac{1}{\\lambda_2}\\hat{u}_2\\hat{u}_2^T\\end{pmatrix}\\pm\\hat{u}_3\\hat{u}_3^T\\] $\\hat{u}_3$는 eigenvalute가 0인 eigenvector이다. 그리고 마지막 term의 부호는 $\\det(U)$가 positive가 되도록 정한다. 다시 본론으로 돌아와 maximize하려는 값은 $Tr(R^TM)$이며 위의 decomposition을 적용하고 식을 정리하면 다음과 같다.\\[Tr(R^TM)=Tr(R^TUS)\\\\=\\sqrt{\\lambda_1}Tr(R^TU\\hat{u}_1\\hat{u}_1^T) + \\sqrt{\\lambda_2}Tr(R^TU\\hat{u}_2\\hat{u}_2^T) + \\sqrt{\\lambda_3}Tr(R^TU\\hat{u}_3\\hat{u}_3^T)\\] 그리고 trace는 commutative property를 만족하므로\\[Tr(R^TU\\hat{u}_i\\hat{u}_i^T)=Tr(\\hat{u}_i^TR^TU\\hat{u_i})=Tr(R\\hat{u}_i\\cdot U\\hat{u}_i)=(R\\hat{u}_i\\cdot U\\hat{u}_i)\\] 이며 $R,U$는 orthonormal이므로 $(R\\hat{u}_i\\cdot U\\hat{u}_i)\\le1$ 이며 equality가 성립하는 경우는 $R\\hat{u}_i=U\\hat{u}_i$ 일 때이다. 따라서 다음과 같은 식이 성립하며\\[Tr(R^TUS)\\le\\sqrt{\\lambda_1}+\\sqrt{\\lambda_2}+\\sqrt{\\lambda_3}=Tr(S)\\] $Tr(R^TUS)$의 maximum value는 $R^TU=I, R=U$가 되도록 $R$을 구함으로써 얻을 수 있다. 그래서 $M$이 nonsingular라면 우리가 구하고자 하는 orthonormal matrix $R$은\\[R=M(M^TM)^{-1/2}\\] 이다. $M$의 rank가 2라면 위에서 구한 방법대로 $R=U$로 구하면 된다. Nearest Orthonormal Matrix 여기서는 $M$에 대해서 nearest orthonormal matrix $R$이 $U$와 같다는 것을 보인다. 이는 아래의 식을 minimize하는 $R$을 구한다는 것과 같다.\\[\\sum_{i=1}^{3}\\sum_{j=1}^{3}(m_{i,j}-r_{i,j})^2=Tr[(M-R)^T(M-R)] \\\\=Tr(M^TM)-2Tr(R^TM)+Tr(R^TR)\\\\ R^TR=I\\] 여기서 첫번째 term과 세번째 term은 $R$과 상관없는 term이므로 이 문제는 결국 $Tr(R^TM)$을 maximize하는 $R$을 찾는 문제와 같아지고 우리가 이제껏 풀었던 문제와 같은 문제이다. 이를 통해서 residual error를 minimize하는 orthonormal matrix는 원래의 matrix $M$에 가장 가까운 orthonormal matrix를 찾는 least-square 문제와 같다는 것을 알 수 있다. Symmetry in the Transformation left to right가 아닌 right to left transformation을 찾고 싶다면 다음 식을 maximize하는 rotation을 찾아야 한다.\\[\\sum_{i=1}^{n}(r_{l,i}^{\\prime})^T\\bar{R}r_{r,i}^{\\prime}\\] 앞에서 구한 rotation의 대칭성을 이용하면 바로 다음처럼 구할 수 있다.\\[\\bar{R}=M^T(MM^T)^{-1/2}\\] 여기서 $\\bar{R}^T=R$이 되어야하는데\\[\\bar{R}^T=(MM^T)^{-1/2}M \\\\ R=M(M^TM)^{-1/2}\\] 위의 두 식이 달라보이지만\\[[M^{-1}(MM^T)^{-1/2}M]^2=M^{-1}(MM^T)^{-1}M=(M^TM)^{-1}\\\\ M^{-1}(MM^T)^{-1/2}M=(M^TM)^{-1/2} \\\\ (MM^T)^{-1/2}M=M(M^TM)^{-1/2}\\] 으로 같음을 알 수 있다. 위의 정보들을 바탕으로 실제로 rotation을 구하는 식은 다음과 같다.\\[M^TM=\\begin{bmatrix}a &amp; d&amp; f \\\\ d&amp; b&amp; e \\\\ f&amp; e&amp; c\\end{bmatrix}\\\\\\det(M^TM-\\lambda I)=0\\\\(M^TM-\\lambda_i I)\\hat{u}_i=0\\\\-\\lambda^3+d_2\\lambda^2+d_1\\lambda+d_0=0\\\\d_2=Tr(M^TM)\\\\d_1=(e^2-bc)+(f^2-ac)+(d^2-ab)\\\\d_0=[\\det(M)]^2\\] " }, { "title": "ICP(Iterative Closest Point) Algorithm 정리", "url": "/posts/icp/", "categories": "Pointcloud Registration", "tags": "Rigid, Pointcloud", "date": "2020-12-06 00:00:00 +0900", "snippet": "Reference: Least-Squares Fitting of Two 3-D Point SetsICP 알고리즘은 fine registration에서 거의 가장 기본이 되는 알고리즘이라고 볼 수 있다.ICP 알고리즘의 과정 자체는 복잡하지 않은데 늘 이해가 안됐던 부분이 center point로 translation을 한 이후에 두 frame상의 point들에 대해 outer product를 해서 나온 matrix의 SVD를 통해서 rotation을 구하는 부분이 ICP외에도 많이 등장하는데 이전부터 이해가 잘 안됐어서 이 부분과 함께 이 참에 정리를 해보려고 한다.ICP의 기본적인 flow는 다음과 같다 Initialize error with inf ($\\epsilon = \\infty$) Calculate correspondence (get closest pair $&lt;q_i, q_i’&gt;)$ Calculate alignment (from $q_i’, q_i$ get $R, T$) Apply alignment ($p = Rp + T)$ Update error $(\\epsilon’ = \\Vert p’ - p \\Vert, \\epsilon = \\epsilon’)$ If($\\epsilon$ &gt; Threshold) {back to 2.} else {end} 자세히 설명을 하자면, error minimize 문제이기 때문에 당연히 error를 infinity로 initialize한다. source frame의 point들을 $p$ 라하고 target frame의 point들을 $p’$ 이라 하면 이 두 frame 각각에 대해서 center point를 구하고 그 center point를 origin으로 하도록 point들을 각각 translation하여 새롭게 $q, q’$ 을 정의하고 이 두 point들 사이의 closest pair $&lt;q_i, q_i’&gt;$를 계산한다. Paper: Least-Squares Fitting of Two 3-D Point Sets 결국 이 ICP문제는 두 frame사이의 rotation $R$과 translation $T$를 구해서 $\\Vert p’ - Rp - T\\Vert$를 minimize 하는 문제인데 여기에서 $T$의 경우 $R$을 구하면 $T = p’ -Rp$ 를 통해서 쉽게 구할 수 있다. 그래서 결국은 center point를 기준으로 frame을 이동하고 $\\Vert q’ - Rq\\Vert$를 minimize하는 문제로 볼 수 있다. 이 문제를 자세히 보면 2에서 구한 pair를 사용하여 $\\Sigma{\\Vert q_i’ - Rq_i\\Vert}^2$$=$ $\\Sigma({q_i’}^tq’_i + q_i^tq_i-2{q_i’}^tRq_i)$가 되고 여기서 첫번째와 두번째 term은 고정값이므로 3번째 term인 $F = \\Sigma {q_i’}^tRq_i$ 를 maximize하는 문제로 바꿀 수 있다. maximization problem의 최종 형태는 $F = Tr(\\Sigma Rq_i{q_i’}^t) = Tr(RH)$ $(H=\\Sigma q_i{q_i’}^t)$ 가 된다. 여기서 한가지 $Lemma$를 소개하면 $Lemma:$ 모든 positive definite matrix $AA^t$, orthonormal matrix $B$에 대해서 $Tr(AA^t) &gt; Tr(BAA^t)$ 를 만족한다. 우선 $H$에 SVD를 적용하면 $H = U\\Lambda V^t$ 로 나타낼 수 있고 여기서 어떤 $X=VU^t$라 하면, $XH=VU^tU\\Lambda V^t=V\\Lambda V^t$가 된다. 이 $XH$는 symmertic이면서 positive definite이므로 위의 $Lemma$에 의해 모든 3x3 orthonormal matrix $B$에 대해서 $Tr(XH)\\ge Tr(BXH)$ 를 만족하므로 이 $X$가 결국 위의 $F$를 maximize하는 $R$값이 된다. 이렇게 구한 $R$ 값으로 앞서 말했듯 $T$도 구했다. (사족: 결국은 outer product자체에 의미가 있다기 보다는 least square problem을 푸는 과정 중에 나오는 term이고 이를 maximize하는 과정중에서 SVD를 활용을 했던 것이었다. 여기서 보다 physical한 의미를 찾으면 좋겠지만 일단은 왜 나오지는지를 이해한걸로 만족하고 넘어가보려 한다.) 3에서 구한 $R, T$를 사용하여 source frame을 transformation($p\\leftarrow Rp+T$)하고 $\\epsilon’ =\\Vert p’-p\\Vert$의 값을 구해서 $\\epsilon’$이 기존 $\\epsilon$보다 작으면 값을 update한다. 5에서 update한 $\\epsilon$이 Threshold(end condition)보다 작으면 ICP를 종료하고 아니면 2로 돌아가서 다시 같은 과정을 계속 반복한다." }, { "title": "group theory, lie group, lie algebra", "url": "/posts/group_theory/", "categories": "Math", "tags": "group thoery", "date": "2020-11-29 00:00:00 +0900", "snippet": "Group TheoryLie Group(리 군)에 대해 알아보기에 앞서 Group의 정의와 표현에 대해서 알아보고자 한다.보통 matrix group에 우리가 관심을 가지는데 $SU_{(N)},O_{(N)},SP_{(N)},\\cdots$ 들이 있는데 특히 $SU_{(2)}$와 $SU_{(3)}$에 대해서 아는것이 중요하다.정의:\\[G=\\{g_1,g_2,\\cdots\\} \\\\ {}^\\exists{\\cdot}:s.t. \\\\ (1) \\ g_1\\cdot g_2\\in G \\ (\\cdot \\ closed)\\\\ (2)\\ (g_1\\cdot g_2)\\cdot g_3=g_1\\cdot(g_2 \\cdot g_3) \\ (associativity) \\\\ (3) \\ {}^\\exists 1, g\\cdot 1 = 1 \\cdot g = g \\\\ (4) {}^\\exists g^{-1},\\ for\\ all\\ g\\in G \\ (x\\cdot g = 1 \\rightarrow x=g^{-1})\\]위의 조건들을 만족하는 집합을 group이라고 부른다.$O(N)$ Group$O_{(N)}$ Group은 $AA^T=1$인 특성을 가지는데 $N$ dimensional vector space에서의 rotation group이다.예를들어 $R\\in3\\times3$ 이면, $R$의 column들은 orthonormal basis 이며 각각의 column들은 orthonormal하다. 즉 $\\hat{r_i}\\cdot\\hat{r_j}=\\delta_{ij}$이며 $\\Vert R\\vec{X}\\Vert=\\Vert\\vec{X}\\Vert$이다.$N$ dimension의 $\\mathbb{R}$(orthonormal group)의 element가 몇개인가 = $\\mathbb{R}$의 independent element가 몇개인가?$\\hat{r_i}\\cdot\\hat{r_j}=\\delta_{ij}$가 constraint이므로 constraint의 갯수는 $N+\\frac{1}{2}N(N-1)$이며 전체 원소의 갯수가 $N^2$이므로 $O_{(N)}$ group의 dimension은 $\\frac{1}{2}N(N-1)$이 된다.$U(N)$ Group$U_{(N)}$ Group은 unitary group이라고 하는데 $UU^{\\dagger}=U^{\\dagger}U=1$을 만족하는 group이다.$U$의 column들을 $\\vec{a_i}$라고 하면 $(\\vec{a_i},\\vec{a_j})=\\vec{a_i^*}\\vec{a_j}=\\delta_{ij}$가 constraint가 된다. 원래는 $N^2$개의 complex parameter가 존재하므로 $2N^2$개의 real parameter가 존재하게 된다. 그런데 constraint가 $i\\neq j$인 경우 complex가 $\\frac{1}{2}N(N-1)$개가 있으므로 parameter수는 2배인 $N(N-1)$ 이고 만약 $i=j$이면 real이 $N$개가 있으므로 총 $N^2$개의 constraint가 있게 된다. 그러므로 $\\dim U_{(N)}=N^2$이다. $\\det(U)=1$이라면 constraint가 추가되므로 이를 $SU_{(N)}$그룹이라 부르고 $\\dim SU_{(N)}=N^2-1$이 된다. 이 추가된 constraint에 대해서 설명하자면\\[UU^\\dagger=1 \\\\ \\det U (\\det U)^* =1 \\\\ \\therefore \\det U=e^{i\\theta}\\]원래 $U$의 determinant는 위와 같은데 $\\theta=0$으로 정해지는 constraint가 생기면서 복소평면의 원 공간이 하나의 점으로 줄어들면서 차원이 하나 줄게된다.Group Homomorphism그룹 $G, G^\\prime$가 있고 함수 $f:G→G^{\\prime}$가 있을때\\[f(g_1)=g_1^\\prime, f(g_2)=g_2^\\prime \\\\ f(g_1)\\cdot f(g_2)=f(g_1\\cdot g_2)\\]위의 조건을 만족하면 Group $G,G^\\prime$가 homomorphic 하다고 하고 $f$를 homomorphism이라고 한다. 연산을 보존한다는 말로 두 그룹을 거의 같다고 봐도 된다는 얘기라고 한다. 만약 $f$가 1:1 대응이라면 isomorphism이라고 하고 두 그룹은 identical 하다고 한다.Homomorphism of SU(2), O(3)Pauli matrix라 불리는 특별한 matrix\\[\\sigma_1 = \\begin{pmatrix}0 &amp; 1\\\\1 &amp;0\\end{pmatrix}, \\sigma_2 = \\begin{pmatrix}0 &amp; -i\\\\i &amp;0\\end{pmatrix}, \\sigma_3 = \\begin{pmatrix}1 &amp; 0\\\\0 &amp;-1\\end{pmatrix}\\]들이 존재하고 다음과 같은 성질이 있다.\\[\\sigma_1\\sigma_2=i\\sigma_3 \\\\ \\sigma_2\\sigma_3=i\\sigma_1 \\\\ \\sigma_3\\sigma_1=i\\sigma_2 \\\\ \\sigma_1^2=\\sigma_2^2=\\sigma_3^2=1\\]$M(\\vec{\\mathsf{x}})=\\mathsf{x}_1\\sigma_1+\\mathsf{x}_2\\sigma_2+\\mathsf{x}_3\\sigma_3=\\vec{\\mathsf{x}}\\cdot\\vec{\\sigma}=\\begin{pmatrix}\\mathsf{x_3} &amp; \\mathsf{x_1}-i\\mathsf{x_2}\\\\mathsf{x_1}+i\\mathsf{x_2} &amp; -\\mathsf{x_3} \\end{pmatrix}$ 라고 하고 $\\vec{\\mathsf{x}}\\in\\mathbb{R}^3$ 라고 하자. $\\vec{\\mathsf{x}^\\prime}=R\\vec{\\mathsf{\\vec{x}}}$ 라는 회전변환(SU(2))이 있고 이 값에 대응하는 $M(\\vec{\\mathsf{x}^\\prime})$ 가 있다고 하면 $M(\\vec{\\mathsf{x}})$에 SU(2)를 곱해서 $M(\\vec{\\mathsf{x}^\\prime})$를 만들 수 있을지 즉, 주어진 $R$에 대해서 적당한 $U$를 찾아 $M^\\prime=M(\\vec{\\mathsf{x}^\\prime})=UM(\\vec{\\mathsf{x}})U^\\dagger$를 만족할 수 있을지를 알아보고자 한다.In general 하게 푸는 것은 어렵기에 가장 간단한 rotation으로 z축으로 $\\alpha$만큼 회전한 matrix $R$이 있다고 하고 $U$를 다음과 같이 정의해 본다.\\[R_z(\\alpha)=\\begin{pmatrix}\\cos{\\alpha} &amp; -\\sin{\\alpha} &amp; 0 \\\\ \\sin\\alpha &amp; \\cos\\alpha &amp;0 \\\\ 0&amp;0&amp;1\\end{pmatrix} \\\\ U=\\begin{pmatrix}e^{i\\beta}&amp;0\\\\0&amp;e^{-i\\beta}\\end{pmatrix}\\]$UMU^\\dagger$를 계산해보면\\[U\\begin{pmatrix}\\mathsf{x_3} &amp; \\mathsf{x_1}-i\\mathsf{x_2} \\\\ \\mathsf{x_1}+i\\mathsf{x_2} &amp; \\mathsf{x_3}\\end{pmatrix}U^\\dagger \\\\=\\begin{pmatrix}e^{i\\beta} &amp; 0 \\\\0&amp;e^{-i\\beta}\\end{pmatrix}\\begin{pmatrix}\\mathsf{x_3} &amp; \\mathsf{x_1}-i\\mathsf{x_2} \\\\ \\mathsf{x_1}+i\\mathsf{x_2} &amp; \\mathsf{x_3}\\end{pmatrix}\\begin{pmatrix}e^{-i\\beta} &amp; 0 \\\\0&amp;e^{i\\beta}\\end{pmatrix}\\\\=\\begin{pmatrix}\\mathsf{x_3} &amp; e^{2i\\beta}(\\mathsf{x_1}-i\\mathsf{x_2})\\\\e^{-2i\\beta}(\\mathsf{x_1}+i\\mathsf{x_2}) &amp; -\\mathsf{x_3}\\end{pmatrix}\\]$RM$을 계산해보면\\[RM=\\begin{pmatrix}\\cos\\alpha&amp;-\\sin\\alpha&amp;0\\\\\\sin\\alpha&amp;\\cos\\alpha&amp;0\\\\0&amp;0&amp;1\\end{pmatrix}\\begin{pmatrix}\\mathsf{x}_1\\\\\\mathsf{x}_2\\\\\\mathsf{x}_3\\end{pmatrix}=\\begin{pmatrix}\\cos\\alpha\\mathsf{x}_1-\\sin\\alpha\\mathsf{x}_2\\\\\\sin\\alpha\\mathsf{x}_1+\\cos\\alpha\\mathsf{x}_2\\\\\\mathsf{x}_3^\\prime\\end{pmatrix}\\]이므로\\[\\begin{pmatrix}\\mathsf{x_3} &amp; e^{2i\\beta}(\\mathsf{x_1}-i\\mathsf{x_2})\\\\e^{-2i\\beta}(\\mathsf{x_1}+i\\mathsf{x_2}) &amp; -\\mathsf{x_3}\\end{pmatrix}=\\begin{pmatrix}\\mathsf{x_3}^\\prime &amp; \\mathsf{x_1}^\\prime-i\\mathsf{x_2}^\\prime \\\\ \\mathsf{x_1}^\\prime+i\\mathsf{x_2}^\\prime &amp; \\mathsf{x_3}^\\prime\\end{pmatrix},\\\\(\\cos2\\beta+i\\sin2\\beta)(\\mathsf{x}_1-i\\mathsf{x}_2)=(\\cos2\\beta\\mathsf{x}_1+\\sin2\\beta\\mathsf{x}_2)+i(\\sin2\\beta\\mathsf{x}_1-\\cos2\\beta\\mathsf{x}_2),\\\\\\cos2\\beta\\mathsf{x}_1+\\sin2\\beta\\mathsf{x}_2=\\cos\\alpha\\mathsf{x}_1-\\sin\\alpha\\mathsf{x}_2, \\sin2\\beta\\mathsf{x}_1-\\cos2\\beta\\mathsf{x}_2=\\sin\\alpha\\mathsf{x}_1+\\cos\\alpha\\mathsf{x}_2 \\\\ \\beta=-\\frac{\\alpha}{2}\\]이렇게 $U$를 구할 수 있고 다음과 같은 대응관계가 성립하고 둘 사이의 homomorphism을 찾은 것이다. $x,y$축에 대해서도 같은 식으로 구할 수 있다.\\[R_z(\\alpha) \\Longleftrightarrow U=\\begin{pmatrix}e^{-i\\frac{\\alpha}{2}}&amp;0\\\\0&amp;e^{i\\frac{\\alpha}{2}}\\end{pmatrix}\\]지금까지 간단한 케이스로 이를 보였지만 $U=e^{i\\alpha_i\\sigma_i}$(여기서는 $i=3$ 에 대해서만 함)의 형태로 나타내고 $\\alpha_i$가 주어지면 결국 이에 대응하는 $R$을 construction 할 수 있다. 여기서 $e^{i\\alpha_i\\sigma_i}$를 taylor expasion을 하면 $\\sum\\frac{(-1)^{n}\\alpha_i^{2n}}{(2n)!}+i\\sum\\frac{(-1)^n\\alpha_i^{2n}}{(2n+1)!}=\\cos\\alpha_i+i\\sigma_i\\sin\\alpha_i$ 이다. $\\sigma_i$는 hermitian matrix이므로 결국 임의의 unitary matrix는 hermitian matrix를 이용해 나타낼 수 있다($U=e^{i\\alpha H}$ ). 정리하자면 unitary matrix를 hermitian matrix의 exponential 형태로 나타낼 수 있고 거기에서 $\\alpha$가 주어지면 이에 대응하는 $R$을 찾을 수 있기에 SU(2)와 SO(3)는 homomorphic 하다고 볼 수 있다.Lie Group, Lie AlgebraLie Group은 노르웨이의 수학자 Sophus Lie가 고안한 이론으로 Lie는 갈로아가 발견한 다항식의 해를 찾는 공식이 존재하는 상황이 그 주어진 다항식의 해의 대칭성에 기인한다는 위대한 대수학의 발경에 감동을 먹고서 미분방정식에서도 대칭성의 이론을 찾아낼 수 있지 않을까하는 접근을 시도 하였다고 한다.이러한 시도로부터 그는 기존의 유한 집합이 아니라 그룹의 원소가 무한대이고 오른쪽 혹은 왼쪽 이동에 대해서 대칭성을 가지고 연속적이며 미분가능한 smooth manifold가 되는 집합을 발견하게 되었고 우리는 이를 Lie Group이라 부른다.그러면 이러한 Lie Group은 어떻게 얻는 것인가 하면 연속변환을 통해서 얻는다고 하는데 좌표변환을 생각해보면 기존의 좌표계에서 새로운 좌표계로의 선형변환을 우리는 주로 사용했는데 이러한 좌표변환이 한번만 일어나는것이 아니라 시간에 따라서 연속적으로 이루어진다고 하면 coordinate가 연속적으로 변하는 상황이 발생하고 이에 따라서 어떤 점 $P(x,y,z)$의 좌표 또한 연속적으로 변하게 된다. 그리고 이러한 변환에 의해 그려지는 $P$의 자취는 Lie Group을 이루게 된다. 예를 들어서 회전변환의 경우는 $[0,2\\pi]$의 원모양의 smooth manifold가 된다. 이러한 관점은 공간에서 물체의 움직임도 이러한 연속변환의 관점에서 바라볼 수 있게 해주며 Lie Group에서의 미분이라는 해석적 접근을 가능하게 한다.Lie Group중에서 특히 unitary 그룹을 예로 들면 $g=e^{i\\epsilon h}$가 group element인데 여기서 $h$를 Lie algebra element라고 한다. 여기서 $\\epsilon$이 매우 작다면 $e^{i\\epsilon h}\\approx 1+i\\epsilon h$ 로 나타낼 수 있다. 그러면 $1+i\\epsilon h$는 1(identity element) 근방의 어떤 값들이고 이 값들은 Lie Group의 identity element 에서의 tangent space 위에 있다고 할 수 있으며 이 tangent space의 원점은 identity element가 되며 이 tangent space위의 모든 점을 Lie Algebra라고 한다고 한다.tangent space의 basis를 generator($S_i$)라고 하는데 $h=\\sum\\epsilon_iS_i$라고 나타낼 수 있고 이러한 generator의 linear combination을 이용해 Lie group을 만들고자 했다. 그리고 $[S_i,S_j]=\\sum c_{ij}^k S_k$ 처럼 $S_i$와 $S_j$의 commutator를 $S_k$들의 linear combination으로 나타낼 수 있는데 그 이유는 $R_i=e^{\\epsilon S_i}$라고 하면 $R_iR_jR^{-1}_iR_j^{-1}$는 $e^{\\sum\\delta_iS_i}$의 형태로 Lie Group의 또 하나의 element가 되는데 infinitesimal로 이 값을 계산하면 $1+\\epsilon_i\\epsilon_j[S_i,S_j]$이 되고 이게 결국 $1+\\delta_iS_i$와 같아지므로 $[S_i,S_j]$를 다른 generator들의 linear combination으로 나타낼 수 있게 된다.그래서 linear combination의 parameter에 의해서 Lie Algebra의 structure가 결정되고 이에 따라 Lie Group의 structure 또한 결정 된다. 따라서 Lie Group 상에서 어떤 문제를 풀기보다는 Lie Algebra 상에서 간단하게 풀어서 exponential을 통해서 Lie Group으로 변환할 수 있기에 Lie Algebra를 배우는 것이다.\\[SU(2)\\ |\\ su(2) \\\\ \\begin{pmatrix}a &amp; b \\\\ c &amp; d\\end{pmatrix} \\ \\ \\ \\ \\ \\ \\sigma_1, \\sigma_2, \\sigma_3 \\\\ \\dim SU(2)=3, \\vec{M}(x)=\\vec{x}\\cdot\\vec{\\sigma}\\]Lie Group의 곱셈 = Lie Algebra에서의 덧셈, su(2)의 generator(basis)는 pauli matrix이다.\\[g \\leftarrow e^{M(x)} \\\\ \\sigma_1\\sigma_2=-\\sigma_2\\sigma_1=i\\sigma_3 \\\\ [\\sigma_1,\\sigma_2] =\\sigma_1\\sigma_2-\\sigma_2\\sigma_1=2i\\sigma_3 \\\\ [\\frac{\\sigma_i}{2},\\frac{\\sigma_j}{2}]=i\\epsilon_{ijk}\\frac{\\sigma_k}{2}\\]여기서 $\\epsilon_{ijk}$는 structure constant라고 부른다. $H=\\Sigma\\alpha_iS_i$ 에서 $S_i=\\frac{\\sigma_i}{2}$이다.지금까지 Lie Group에서 Lie Algebra로 변환하는 방법에 대해서만 배우고 Algebra가 vector space라는 것만 언급했는데 Algebra가 되기 위해서는 몇 가지 조건이 더 필요하다. 그래서 Algebra의 명확한 정의에 대해서 알아보고자 한다.Algebra에서는 곱이 정의될 수 있는데 commutator로 정의된다. 두 element의 commutator를 취한 결과는 여전히 Lie Algebra의 element가 되는걸 앞에서 보였는데 이는 Lie Group의 closed-ness가 Lie Algebra의 곱을 commutator로 정의해 주는 것이다.\\[[S_i,S_j]=ic_{ij}^kS_k\\]Jacobi Identity\\[[S_i,[S_j,S_k]]+[S_j,[S_k,S_i]]+[S_k,[S_i,S_j]]=0 \\\\ [S_i,[S_j,S_k]]=[[S_i,S_j],S_k]+[S_j,[S_i,S_k]] (Leibniz\\ rule)\\]→ group이 associative하기 위해서 algebra가 만족시켜야 하는 ruleSU(2)의 형태\\[su(2)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ M=i\\vec{x}\\cdot\\vec{\\sigma} \\\\ e^{M}=e^{\\vec{x}\\cdot\\vec{\\sigma}}=1+i\\vec{x}\\cdot\\vec{\\sigma} \\\\ g=y^0\\cdot1+i\\vec{y}\\cdot\\vec{\\sigma}=\\begin{pmatrix}iy_3+y_0 &amp; iy_1+y_2 \\\\ iy_1-y_2 &amp; y_0-iy_3\\end{pmatrix}\\]su(2)는 $\\det{g}=1$이므로 $\\det{g}=y^2_o+y^2_3+y^2_1+y^2_2=1$이 되어야한다. 이는 3차원 구 $S^3$의 형태로 생겼고 SU(2)와 SO(3) 또한 3차원 구처럼 생겼다고 봐고 된다." }, { "title": "LOAM, Lego-LOAM 정리", "url": "/posts/LOAM,-Lego-LOAM/", "categories": "SLAM", "tags": "SLAM", "date": "2020-11-24 00:00:00 +0900", "snippet": "LOAM, Lego-LOAMLidar Odometry에서 많이 쓰이는 LOAM 계열의 논문들을 리뷰해 보고자 한다.LOAMpaper: LOAM: Lidar Odometry And Mapping in Real-time (or Low-drift and Real-time Lidar Odometry and Mapping)LOAM은 논문 제목대로 Lidar Odometry And Mapping의 줄임말로 high accuracy ranging과 inertial measurement 없이 Low-drfit, Low-computational complexity를 가진 odometry라고 한다.Odometry는 아주 복잡한 문제인 SLAM 문제의 일부이며 이 odometry를 아래처럼 두 부분으로 나누어서 해결했다고 한다. motion estimation을 위해 높은 frequency로 돌아가지만 신뢰도가 비교적 낮은 odometry 위의 알고리즘에 비해 frequency가 낮지만 fine registration을 할 수 있는 mappingLOAM의 전제 시스템은 다음과 같다. Lidar input이 들어오면 registration을 해서 Lidar odometry를 한다. Odometry의 결과 pose를 이용해 Lidar mapping을 한다. Mapping의 결과와 Odometry의 pose결과를 합쳐서 finagl pose를 얻는다. Lidar Odometry Feature Point Extraction Registration에 사용되는 feature는 edge와 plane이다. 그래서 point마다 edge point인지 planar point인지를 구별하기 위해서 smoothness를 구해서 이를 판별하였다.\\[c=\\frac{1}{|S||X^L_{(k, i)}|}||\\sum_{j\\in S ,\\ j \\neq i}(X^L_{(k, i)}-X^L_{(k, j)})||\\] 여기서 $S$는 표현이 애매한데 내가 이해한 바로는 point $i$와 consecutive한 point들의 set이며 양쪽 side를 반반씩 가지고 있는 set리고 한다. LeGO-LOAM 구현코드에서는 양쪽 point 5개씩 사용한것을 봐서는 결국 양쪽에 n개씩 사용한다는 뜻인것 같다. $\\begin{vmatrix}S\\end{vmatrix},\\begin{vmatrix}X^L_{(k,i)}\\end{vmatrix}$는 거리, set의 크기에 대한 normalize term역할을 한다. 그래서 이 maximum $c$들을 edge point 로 minimum c들을 planar point로 한다고 하는데 식을 보면 edge인 경우에 현재 point를 기준으로 consecutive한 point가 어느 한쪽으로 몰려있기 때문에 $c$의 값이 커지고 planar의 경우 평평할수록 현재 point를 지나는 대칭형태의 line이 되므로 값들이 상쇄되어 0에 가까워진다. 그리고 feature point를 골고루 뽑기 위해 90도씩 4개의 subregion으로 나누고 각각의 region마다 threshold를 만족하는 edge는 최대 2개, planar는 최대 4개를 찾도록 하였다. 그리고 feature point를 골고루 뽑아야 하므로 주변에 feature point가 존재하면 뽑지 않도록 하였고 occluded region의 boudary또한 실제로는 planar인데 edge로 뽑힐 가능성도 있으므로 뽑지 않으며 laser beam에 parallel한 surface는 대체로 unreliable하므로 이또한 뽑지 않도록 한다. 이런 과정을 통해 edge point와 planar point는 아래와 같이 뽑힌다. 노란색: edge 빨간색: planar Finding Feature Point Correspondence Feature를 찾았으면 registration을 위해서 서로 다른 scene의 feature들 간에 correpondence를 생성해야한다. $P_{k+1}$에서 찾은 edge와 planar를 $E_{k+1}, H_{k+1}$이라 하면 이 point들에 대해 $P_k$에 있는 point중 nearest point를 찾아 correspondece를 만든다고 한다. edge point: $i\\in E_{k+1}$에 대해서 $P_k$에서의 nearest point를 $j$라고 하고 edge line을 형성하기 위해서는 2개의 point가 필요하므로 $j$의 consecutive point $l\\in P_k$를 구하고 이 $j, l$에 대해 smoothness를 계산해 둘 다 edge point라면 $(j,l)$이 이루는 edge line과 $i$ 사이에 correspondence를 만들고 다음과 같은 식으로 correspondence의 distance를 구한다.\\[d_{\\epsilon}=\\frac{|(\\tilde{X}^L_{(k+1,i)}-\\bar{X}^L_{(k, j)})\\times(\\tilde{X}^L_{(k+1,i)}-\\bar{X}^L_{(k,l)})|}{|\\bar{X}^L_{(k, j)}-\\bar{X}^L_{(k,l)}|}\\] planar point: $i\\in H_{k+1}$에 대해서 $P_k$에서의 nearest point를 $j$라고 하고 plane을 형성하기 위해서는 3개의 point가 필요하므로 세 점이 한 직선을 만들지 않도록 $j$의 nearest neighbor 2개의 $l,m\\in P_k$를 구하고 이 $j,l,m$에 대해 smoothness를 계산해 셋 모두 planar point라면 $(j,l,m)$이 이루는 plane과 $i$사이에 correspondence를 만들고 다음과 같은 식으로 correspondence의 distance를 구한다.\\[d_H=\\frac{|(\\tilde{X}^L_{(k+1,i)}-\\bar{X}^L_{(k,j)})\\{(\\bar{X}^L_{(k,j)}-\\bar{X}^L_{(k,l)})\\times(\\bar{X}^L_{(k,j)}-\\bar{X}^L_{(k,m)})\\}|}{|(\\bar{X}^L_{(k,j)}-\\bar{X}^L_{(k,l)})\\times(\\bar{X}^L_{(k,j)}-\\bar{X}^L_{(k,m)})|}\\] Motion Estimation 위에서 구한 correspondence를 이용해 두 frame사이의 motion estimation을 해야한다. 그런데 lidar point는 모든 point가 동시에 찍혀 나오는것이 아니라 일정한 속도로 sweep을 하면서 한 frame을 완성하는 것이기에 linear interpolation을 해줘야 한다. 그래서 transformation을 구한 시점 $t$에서의 6-DOF transformation을 $T^L_{k+1}=[t_x,t_y,t_z,\\theta_x,\\theta_y,\\theta_z]^T$이라고 하고 transform 하는 point의 index를 $i$라고 하면 point $i$에 한 transformation은 다음과 같다.\\[T^L_{(k+1,i)}=\\frac{t_i-t_{k+1}}{t-t_{k+1}}T^L_{k+1}\\] motion estimation을 위해서는 두 frame에서 찾은 feature들 사이의 transform matrix를 구해야한다. 즉 아래의 식을 만족하는 transformation을 구해야한다.\\[X^L_{(k+1,i)}=R\\tilde{X}^L_{(k+1,i)}+T^L_{(k+1,i)}(1:3)\\] 여기서 $R$은 optimization을 위해서 $T^L_{(k+1,i)}(4:6)$을 Rodrigues formula를 통해서 구한 $\\omega$의 skew symmetric matrix이다. 위의 문제를 optimization problem으로 풀기 위해 correpondence의 거리가 가까워진다는 것은 제대로 registration(motion estimation)이 이뤄졌다는 의미이므로 아래와 같이 edge correspondence와 planar correspondence를 cost로 삼는다.\\[f_{E}(X^L_{(k+1,i)},T^L_{k+1})=d_E, i\\in E_{k+1} \\\\ f_{H}(X^L_{(k+1,i)},T^L_{k+1})=d_H, i\\in H_{k+1} \\\\ \\rightarrow f(T^L_{k+1})=d\\] optimization에 사용된 알고리즘은 Levenberg-Marquardt이다.\\[T^L_{k+1}\\leftarrow T^L_{k+1}-(J^TJ+\\lambda diag(J^TJ))^{-1}J^Td\\] Lidar Odometry Algorithm lidar odometry에 사용된 알고리즘을 정리하자면 위와 같다. 앞에 소개한 방법론들을 순차적으로 진행하여 얻은 transform을 통해 odometry를 한다. Lidar Mapping lidar mapping은 odometry보다 더 낮은 빈도로 실행되고 sweep이 완성된 후에만 실행된다. odometry에서 구한 motion으로 untwist된 point cloud \\(\\bar{P}_{k+1}\\)을 world coordinate상의 map에 registration하는 과정이다. $Q_k$를 sweep $k$가 끝난 시점에서의 pose주변의 cubic area내에 존재하는 map point들의 set이라 하고 $\\bar{P}_{k+1}$을 mapping을 통해서 가장 최근에 얻은 transformation인 $T^W_k$으로 transform한 point를 \\(\\bar{Q}_{k+1}\\)이라 하면 odometry에서 한 것처럼 feature extraction, finding correspondence, motion estimation을 통해서 \\(\\bar{Q}_{k+1}\\)을 \\(Q_k\\)에 registration을 한다. mapping에서 \\(\\bar{Q}_{k+1}\\)에 대한 feature extration은 이미 odomery에서 했기 때문에 그대로 사용한다. odometry가 10Hz로 돌아가고 mapping이 1Hz로 돌아가기 때문에 10배의 feature를 사용하게 된다. correspondence 생성은 $\\bar{Q}_{k+1}$의 각 feature point마다 주변에 존재하는 $Q_k$의 point의 set $S’$을 구하고 $S$에 대해서 matrix decomposition을 통해 eigenvalue와 eigenvector를 구하고 eigenvalue에서 dominant한 value의 갯수가 2개면 plane, 1개면 edge라고 판별하고 edge line과 planar patch의 position은 $S’$의 geometric center로 삼아서 feature point과 이 center사이의 corresopndence를 생성한다. optimization은 마찬가지로 Levenberg-Marquardt를 사용해서 transformation을 구한다. 그리고 마지막으로 pose integration은 mapping이 상대적으로 fine registration이기에 이미 존재하는 mapping에 odometry pose를 쌓는방식으로 한다. 따라서 실시간으로 얻는 pose의 결과는 $T^L_{k+1}T^W_K$가 되는 것이다. LeGO-LOAMpaper: LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry And Mapping on Variable TerrainLeGO-LOAM은 ground plane의 존재를 이용해 lightweight한 real time 6DOF pose estimation을 했다고 한다.noise filtering을 위해 segmentation을 해서 보다 robust한 결과를 얻었다고 했다.computation expense를 줄였지만 성능은 LOAM과 비슷하거나 더 낫다고 했다. System Overview LeGO LOAM의 전제 시스템은 다섯 부분으로 나뉜다. Segmentation 모듈에서 Point Cloud → Range Image → Segmented Point의 과정을 진행하고 Feature Extration 모듈에서 Segmented Cloud로 부터 Feature를 뽑고 이 feature를 이용해 odometry와 mapping을 하고 두 pose를 integration한다고 한다. Segmentation Point Cloud (그림 a)를 $1800\\times16$ 크기의 range image로 변환하고 range image에서의 pixel value는 point의 sensor로 부터의 euclidean distance로 한다. 그리고 Segmentation 이전에 ground extration을 하고서 segmentation을 한다고 하는데 이 ground extration은 column-wise evaluation방법을 통해서 한다고 하는데 이 방법은 range image 상에서 column-wise slope를 이용해서 threshold 미만이면 ground로 판단해서 ground point들을 뽑아낸다고 했다. 이렇게 ground point들의 index를 제외한 나머지 range image에서 image-based segmentation을 한다. 여기서 robustness를 위해 point 갯수가 30개 미만인 segment들은 사용하지 않는다. 이 결과로 segmented point과 groud point (그림 b)를 얻는다. Feature Extraction 앞에서 추출한 segmented point와 ground point에서 feature를 뽑는 과정이다. LOAM에서와 같은 smoothness를 정의해서 사용한다.\\[c=\\frac{1}{|S|||r_i||}||\\sum_{j\\in S,j\\neq i} (r_j-r_i)||\\] $S$는 range image상에서 같은 row에 있는 연속적인 point를 사용하였고 LeGO-LOAM 구현 코드에서는 앞뒤로 5개의 point를 사용하였다. 그리고 이 smoothness를 가지고 edge와 planar를 구분한다. (자세한것은 LOAM에서의 설명 참조) 다른 점은 edge point로 판별되었지만 ground point일 경우는 feature로 사용하지 않으며 $60^{\\circ}$씩 6개의 sub-image로 나눠서 edge 와 planar point들을 뽑는다. $\\mathbb{F}_e,\\mathbb{F}_p$는 6개의 sub image에 있는 모든 feature들의 set이며 \\(F_e, F_p\\)는 각각의 sub image에 존재하는 feature들의 set이며 \\(n_{F_e}, n_{F_p}, n_{\\mathbb{F}_e},n_{\\mathbb{F}_e}\\)는 각각 2, 4, 40, 80으로 정했다. feature extracion을 통해 얻은 feature들은 위의 그림 c와 d에 나타나 있다. Lidar Odometry Lidar odometry 모듈에서는 두 개의 연속된 scan 사이의 transformation을 feature들간의 correspondence를 이용해서 구한다. 이를 위해서는 $F^t_e,F^t_p$와 $\\mathbb{F}^{t-1}_e,\\mathbb{F}^{t-1}_p$사이에서 correspondence를 구하고 이를 optimizaion 해야한다. Label Matching matching의 효율성을 위해서 모든 feature를 match에 이용하는것이 아니라 segmented point에서는 $F^t_e$와 $\\mathbb{F}^{t-1}_e$ 사이의 correspondence만 찾고 ground point에서는 $F^t_p$와 $\\mathbb{F}^{t-1}_p$사이의 correspondence만 LOAM에서와 같은 방식으로 찾는다. Two-step LM Optimization optimization에서도 속도의 효율성을 높이기 위해서 6-DOF의 transform $[t_x,t_y,t_z,\\theta_{roll},\\theta_{pitch},\\theta_{yaw}]^T$를 한번에 optimization하는 것이 아니라 두 개의 단계로 나눠서 optimization을 진행한다. $[t_Z,\\theta_{roll},\\theta_{pitch}]$를 ground plane을 이용하여 즉 $F^t_p$와 $\\mathbb{F}^{t-1}_p$사이의 correspondence의 distance를 줄이는 방향으로 optimization 한다. $[t_x,t_y,\\theta_{yaw}]$를 $F_e^t$,$\\mathbb{F}^{t-1}_e$ 사이의 distance를 줄이는 방향으로 optimization을 한다. 그리고 각각의 과정에서 optimize하는 parameter외의 나머지 parameter는 constraint로 삼아서 optimize한다. 이렇게 하는 이유는 ground plane만 사용해도 $t_z,\\theta_{roll},\\theta_{pitch}$를 optimize할 수 있으므로 parameter를 3개씩 나눠서 optimization을 진행해도 optimization이 가능하며 parameter수를 줄이면 LM알고리즘의 특성상 search space가 작아지기 때문에 6개의 한번에 optimization하는것보다 3개씩 두개의 과정으로 나누어 optimization하는것이 더 빠르기 때문이다. 이 방법이 accruracy를 높이는데에 도움이 되었을 뿐만 아니라 실제로 35%정도의 computation time이 줄었다고 한다. Lidar Mapping Lidar Mapping 모듈은 낮은 빈도로 돌아가지만 pose transformation을 refine하기 위해 $\\mathbb{F}^t_e, \\mathbb{F}^t_p$의 feature들을 주변의 point cloud map $\\bar{Q}^{t-1}$과 matching하고 L-M 알고리즘을 사용하여 transformaion을 구하는 모듈이다. LeGO-LOAM에서는 LOAM과는 달리 point cloud map을 저장할때 feature set ${\\mathbb{F}^t_e,\\mathbb{F}^t_p}$도 같이 저장한다. 여기서 $M^{t}={{\\mathbb{F}^1_e,\\mathbb{F}^1_p},\\cdots,{\\mathbb{F}^t_e,\\mathbb{F}^t_p}}$라고 하면 각각의 $M^t$에 대응하는 pose를 연결짓는 식으로 저장을 한다. 이 $M^{t-1}$로 부터 $\\bar{Q}^{t-1}$을 얻는 방법은 두 가지가 있다. 현재 pose를 기준으로 저장된 feature들 중에서 주변 100m 이내에 있는 모든 pose들의 feature들을 불러오고 이 모든 feature들을 각 pose로 transform하고 합쳐서 surrounding map $\\bar{Q}^{t-1}$을 얻는다. LeGO-LOAM을 pose-graph SLAM하고 통합해서 사용한다고 하면 sensor의 pose는 graph의 node로, feature set ${\\mathbb{F}^t_e,\\mathbb{F}^t_p}$은 각 node의 measurement로 모델링 할 수 있다. 그리고 lidar mapping의 pose estimation drift가 매우 작으므로 단기적으로는 pose의 drift가 없다는 가정하에 최근의 $k$개의 pose 즉 ${{\\mathbb{F}^{t-k}_e,\\mathbb{F}^{t-k}_p},\\cdots,{\\mathbb{F}^{t-1}_e,\\mathbb{F}^{t-1}_p}}$을 사용해 $\\bar{Q}^{t-1}$을 만든다. 그리고 odometry와 같은 방식으로 correspondence를 생성한 후에 L-M optimization을 해서 transform을 얻는다. 여기에 추가적으로 loop closure detection을 해서 ICP와 같은 registration을 통해 추가적인 contraint를 얻으면 drift를 줄일 수 있을 것이라 한다. " }, { "title": "3D pointcloud descriptors 총 정리", "url": "/posts/A_comprehensive_review_of_3D_point_cloud_descriptors/", "categories": "Feature Descriptor", "tags": "Pointcloud", "date": "2020-08-25 00:00:00 +0900", "snippet": "2D data(image)의 경우에는 ORB, SIFT, HOG등등의 descriptor들을 들어봤는데 3D data에 대해서는 제대로 알아본적이 없어서 A comprehensive review of 3D point cloud descriptors 라는 제목의 review 논문 + 여러 pointcloud 관련 task를 적용한 논문들을 통해서 알아보고자 하였다. 각각의 방법론의 소제목 옆에 있는 것은 논문에서 feature를 사용한 target task를 적어놓은 것이다. 대체로 survey논문을 참고하였지만 설명이 부족하다고 생각하거나 이해가 안되었던 것들은 원래 논문도 참고 하였다.사실 완전하게 다 정리하고서 올리고 싶었으나 여러가지 사정으로 언제 다 정리할 지를 모르겠어서 작성한게 너무 아까우니 여기서 올리고 나중에 수정하려고 한다.3D data의 feature를 얻어내는 descriptor는 local-based descriptor, global-based descriptor, hybrid-based descriptor로 크게 3가지로 나눌 수 있다. 수가 많아서 이에 대한 목차는 아래와 같다.Local DescriptorLocal descriptor는 말그대로 local geometry정보를 사용해 feature를 뽑고자 하는 알고리즘이다. 각각의 point에 대해서 local neighborhood를 이용해서 구한다. 이 방법은 local neighborhood의 변화에 매우 민감한 방법이다.1. Spin Image (SI) - 3D object recognitionPaper: Surface matching for object recognition in complex 3-d scenesspin image방법은 feature를 ($\\alpha$,$\\beta$)로 나타내고 point p 주변의 neighboring point q에 대해서 다음과 같이 나타낸다. $\\alpha=n_q\\cdot(p-q), \\beta=\\sqrt{ {\\Vert p-q\\Vert}^2 - \\alpha^2}$ 이것은 결국 p에 대해서 q를 surface plane과 parallel한 방향으로의 거리 surface normal과 parallel한 방향으로의 거리를 구한 것이다.spin image는 주변의 모든 neighboring point에 대해서 $(\\alpha, \\beta)$ 를 구해서 2D discrete bin안에 넣는 것으로 구할 수 있다. 이것이 spin image인 이유는 결국 surface normal vector 부터 시작하는 half-plane을 정의하면 이를 surface normal을 기준으로 한 바퀴 돌리는 동안 이 half-plane상에 neighbor point가 위치한 지점마다 point를 찍어서 만든 이미지가 되기 때문인것 같다.이 descriptor는 occlusion이나 clutter에 대해선 robust하지만 high-level noise에 대해서 취약하다고 한다.2. 3D Shape Context (3DSC) - 3D object recognitionPaper: Recognizing Objects in Range Data Using Regional Point Descriptors3D shape context는 2D shape context descriptor를 3D로 확장시킨 방법이다. 주어진 point p를 중심으로 한 spherical support region에서 north pole의 방향을 surface normal의 방향으로 정하고 3D bin을 azimuth angle과 elevation angle을 동일하게 나누고 반지름의 간격은 logarithmical하게 나눈 형태이다.최종적으로 3DSC의 feature형태는 저렇게 나눈 3D bin안에 들어있는 point의 weighted sum으로써 나타내게 된다. 즉, point p 주변의 local shape를 나타낸 것이다. 그러나 이 방법은 각각의 feature point마다 reference frame이 없기 때문에 feature를 사용할 때 computation양이 많다고 한다.3. Eigenvalues Based Descriptors - 3D terrain classificationPaper: Natural terrain classification using 3-d ladar dataEigenvalues based descriptor는 saliency feature를 뽑아내는 방법이다. Eigenvalue는 point p 주변의 local support region 내부에 있는 neighboring points의 co-variance matrix를 decomposition해서 얻고 이 eigenvalue를 내림 차순으로 $\\lambda_0 \\geq \\lambda_1 \\geq \\lambda_2$ 정의한다.scattered된 point에 대해서는 dominant한 direction이 없기 때문에 $\\lambda_0 \\simeq \\lambda_1 \\simeq \\lambda_2$ 가 될 것이고 linear structure라면 principal direction이 plane위에 생기므로 $\\lambda_0,\\lambda_1 \\gg \\lambda_2$ 의 형태가 되고 surface 경우라면 principal direction이 surface normal과 나란해지므로 $\\lambda_0 \\gg \\lambda_1,\\lambda_2$ 의 형태가 된다.위의 사실들을 바탕으로 eigenvalue의 linear combination을 이용해 saliencies를 구하면 다음과 같이 된다.$\\lambda_2$ 가 크다는 것은 결국 dominant한 direction이 없다는 것이기에 scatter의 형태에 가깝다는 얘기고 작다는 것은 dominant한 direction들이 있다는 것이고 이는 line이든 surface든 어떤 형태를 이루기 때문에 point-ness가 작다고 말할 수 있다.$\\lambda_0 - \\lambda_1$ 이 크다는 것은 가장 dominant한 direction으로 치우쳐져 있다는 의미이고 이는 curve-ness가 크다고 말할 수 있고 $\\lambda_1 - \\lambda_2$ 가 크다는 것은 가장 dominant하지 않은 direction이 크기가 작다는 얘기이기 때문에 surface의 형태에 가까워지고 이는 surface-ness가 크다고 말할 수 있다.4. Distribution Histogram (DH) - 3D segmentationPaper: Discriminative learning of markov random fields for segmentation of 3d scan dataDistribution histogram은 각각의 point의 주변의 principal plane을 이용한다.특정한 cube를 정의 하고 cube내에서 PCA를 사용해서 처음 두 개의 principal components를 이용해 plane을 얻고 cube를 평면방향으로 bin을 나눈다. 그래서 feature의 형태는 단순하게 이렇게 나눠진 bin안에 들어간 point의 수를 더해서 occupancy voxel grid의 형태가 된다.5. Histogram of Normal Orientation (HNO) - 3D object classification and segmentationPaper: Robust 3d scan point classification using associative markov networks이 방법은 단순히 point $p$ 를 기준으로 주변의 point와의 surface normal의 angle의 차이의 cosine값을 이용하는 방법이다. 이 cosine값을 가지고 histogram을 생성한다.만약 curvature가 심하면 histogram은 uniformly distributed 하게 되고 flat area에서는 뾰족한 형태의 histogram을 갖게 된다.6. Intrinsic Shape Signatures (ISS) - 3D object recognitionPaper: Intrinsic Shape Signatures: A Shape Descriptor for 3D Object RecognitionIntrinsic shape signature는 다음과 같은 방식으로 구할 수 있다. point $p$ 주변의 radius가 r인 구 내부의 point들을 이용하여 LRF(local reference frame)를 구하고 이 LRF를 이용해 bin partitioning을 한다. 여기서 cartesian partition보다 polar partition이 rotation error에 대해 더 robust 하므로 azimuth angle과 elevation angle $(\\theta, \\phi)$로 분할한다. 그런데 이를 균일하게 angle로만 분할을 해버리면 bin의 크기가 일정하지가 않고 pole근처에서의 성능이 저하가 되므로 다른 방식으로 분할을 한다.LRF를 이용해 base octahedron을 정의하고 이 base octahedron의 vertex를 기준이 되는 sphere위에 projection을 시켜서 sphere위에 grid의 center를 만들고 projection된 point에 대해서 nearest center가 point가 속하게 되는 grid의 좌표$(\\theta, \\phi)$가 되며 grid에 속하는 point의 갯수로 feature embedding을 한다. 만약 angluar resolution을 높이고 싶으면 polyhedron에 있는 모든 triangle에 대해 순차적으로 sub triangluation을 해서 resolution을 조정한다. 위의 (c)그림은 이 sub triangulation을 3번 한 결과이다.7. ThrIFT - 3D object recognitionPaper: Thrift: Local 3D Structure RecognitionThrIFT는 SIFT와 SURF로 부터 아이디어를 얻어서 orientation information을 고려한 방법이다. point $p$에 대해서 window를 2개를 잡아서 $w_{small}, w_{large}$의 영역에서 각각 least-square로 plane normal $n_{small}, n_{large}$를 계산한다. feature로 만들어 지는것은 이 두 normal사이의 angle의 histogram이 된다. 이 방법은 noise에 매우 민감하다고 한다.8. Point Feature Histogram (PFH) - 3D registrationPaper: Persistent point feature histograms for 3d point cloudspoint feature histogram은 특정 영역내의 point쌍들의 관계와 surface normal을 사용해 geometric property를 나타낸 방법이다. 모든 point $p$와 그 주변의 neighbor point의 쌍에 대해서 기준 point를 $p_s$ 다른 하나를 $p_t$라 하고 $p_s$에서의 Darboux frame을 다음과 같은 식으로 정의한다.$u = n_s$, $v=u \\times {(p_t-p_s)\\over{\\Vert p_t-p_s\\Vert}_2}$, $w=u\\times v$이 frame 상에서 normal $n_s, n_t$사이의 difference를 $\\alpha, \\Phi, \\theta$ . 즉, 3개의 angular feature로 사용하고 여기다가 point pair의 distance를 feature로 사용한다.PFH의 최종 형태는 이 4가지 feature의 4D-histogram bin의 형태가 된다.9. Fast Point Feature Histogram (FPFH) - 3D registrationPaper: Fast Point Feature Histograms (FPFH) for 3D registrationFPFH는 이름에서 알 수 있듯이 위의 PFH에서 time complexity를 줄이기 위해서 simplifying한 방법이라고 생각하면 된다. 기존의 PFH의 time complexity는 $k$개의 neighbor를 사용할때 $o(nk^2)$인데 FPFH는 $o(nk)$의 time complexity를 가지는 알고리즘이다. FPFH는 2개의 step으로 나뉘는데 SPFH(simplified point feature histogram)을 구하는 과정과 point마다 구한 SPFH를 기준이되는 query point와 neighboring point의 SPFH를 weighted sum 해서 구하는 step으로 나뉜다.첫번째 step은 PFH를 구하는 법과 유사한데 PFH가 $\\alpha, \\Phi, \\theta$ , $d$의 값을 가지는 4D space상의 histogram이라면 SPFH는 단순히 $\\alpha, \\Phi, \\theta$ 에 대해서 각각 1D histogram을 만들고 concatenate하는 식으로 구성한다.두번째 step은 query point의 SPFH와 주변 point들의 SPFH의 weighted sum을 해서 최종 FPFH를 구한다. 식은 아래와 같다.$FPFH(p_q)=SPFH(p_q)+{1\\over k}\\sum_{i=1}^k w_k \\cdot SPFH(p_k)$여기서 $w_k={1\\over\\sqrt{exp{\\Vert p_q - p_k\\Vert }}}$ 로 query point과 neighbor 사이의 distance를 반영하는 weight 이다.10. Radius-based Surface Descriptor (RSD)Paper: General 3D modelling of novel objects from a single viewRSD는 주변 point와의 radial relation을 계산하여 geometric property를 나타낸다고 한다. Radius는 두 point사이의 distance와 normal의 angle차이를 이용해 다음과 같이 modeling한다.$d_\\alpha = \\sqrt2 r \\sqrt{1-cos(\\alpha)} = r\\alpha + r{\\alpha}^3/24 + O(\\alpha^5)$주변의 모든 point에 대해서 위의 equation을 계산하여 그 중 maximum radius와 minimum radius를 사용해 final descriptor를 얻는다. 각각 point마다 feature의 형태는 다음과 같다 $d_i =[r_{max}, r_{min}]$.RSD의 2D histogram이 descriptor는 결국 기준 point과 주변 point가 이루는 surface가 거대한 sphere의 일부라고 가정하고 그 sphere의 radius를 구해서 그 중 가장 작은 radius와 가장 큰 radius를 구한다는 것이다. Radius는 ideal plane의 경우 infinity가 된다.11. Normal Aligned Radial Feature (NARF)Paper: NARF: 3D Range Image Features for Object RecognitionNARF는 효과적으로 interest point 주변영역의 similarity를 비교하는 descriptor라고 한다.우선 interest point를 구하는 방법은(a) range image에서 3D distance를 이용해 background와 foreground를 구분하는 border를 찾고(b) image에서의 모든 pixel에 대해 local neighborhood와의 curvature와 border정보를 이용해 surface score를 부여하고(c) 모든 image pixel에 대해 dominant direction을 구하고 이 direction에 대해 다른 direction과 얼마나 다른지, 얼마나 이 point의 surface가 변화가 있는지를 이용해 interest value를 측정하고(d) smoothing과 non-maximum suppression을 이용해 interest point를 구한다.그리고 앞에서 구한 이 interest point의 주변의 normal aligned range value patch를 계산하고 이 patch에 위의 그림의 초록색 선과 같이 star pattern을 그려서 star에서의 beam이 지나는 pixel의 변화가 final descriptor의 value가 되고 unique한 orientation(빨간 화살표)을 이 descriptor로 부터 뽑아서 이 orientation을 기준으로 descriptor value들을 shift해서 rotation invariant하게 한다.12. Signature of Histogram of Orientation (SHOT)Paper : Efficient 3D object recognition using foveated point cloudsSHOT descriptor는 signature와 histogram의 조합이라고 보면 된다.우선 feature point와 그 주변의 radius $r$내에 있는 point들의 covariance matrix에 대해서 unambiguated eigen value decompisition(EVD)을 사용해 unique하고 unambiguous한 local reference frame(LRF)을 계산한다.그 후에 위의 그림과 같은 isotropic spherical grid를 사용해 주변 point를 $(r,\\phi,\\theta)$에 따라 나눠 signature structure를 정의한다. 각각의 grid영역 마다 feature point의 normal과 grid내부의 neighbodirng point들의 normal의 angle의 차이로 local histogram을 생성한다.SHOT descriptor의 최종 형태는 모든 local histrogram을 juxtapose(concatenate?)한 형태가 된다고 한다. 여기에 texture information을 추가해 accuracy를 향상시킨 CSHOT이라는 descriptor도 있다고 한다.13. Unique Shape Context (USC)Paper: Unique shape context for 3D data descriptionUSC는 3DSC에서 각각의 key point를 비교할 때 feature의 계산을 여러번하는 것을 피하기 위해서 unique, unambiguous한 local reference frame(LRF)를 추가한 3D descriptor라고 한다. query point $p$ 와 이 $p$ 주변의 radius가 $R$ 인 spherical support region에 대해서 weighted covariance $M$은 다음과 같이 정의된다.$M=\\frac{1}{Z}\\sum_{i:d_i\\le R}(R-d_i)(p_i-p)(p_i-p)^T$ 여기서 $Z=\\sum_{i:d_i\\le R}{(R-d_i)}$이다.이 $M$은 결국 distance가 멀 수록 weight를 적게 주는 식으로 해서 query point와 주변 point사이의 covariance matrix들을 더한 형태라 말할 수 있다. LRF의 unit vector는 이 $M$을 EVD해서 얻고 여기서 나온 eigenvector와 연관된 eigenvalue의 크기에 따라 re-orient(첫번쨰와 세번째는 그대로, 두번째는 cross product의 방향)를 했다고 한다. 그래서 최종 feature는 이 LRF를 기준으로 3DSC(local descriptor 2번 참조)를 사용해서 얻는다.14. Depth Kernel Descriptor (TODO)Paper: Depth kernel descriptors for object recognitionkernel descriptor에서 motivation을 얻어서 만든 방법으로 3D point cloud에서 size, shape, edge들을 나타내는 5개의 local kernel descriptor를 derive 했다고 한다.15. Spectral Histogram (SH) - 3D urban classificationPaper: Performance of histogram descriptors for the classification of 3D laser range data in urban environmentsSpectral histogram (SH) descriptor는 SHOT과 Eigenvalues Based Descriptor(EBD)를 합친 방법이다. point p 주변의 radius 내부의 영역의 점들에 대한 covariance matrix를 EVD해서 eigenvalue들을 구하고 크기에 따라서 $\\lambda_0 \\le \\lambda_1 \\le \\lambda_2$ 순서로 정렬하고$\\lambda’_i = \\lambda_i / \\lambda_2$로 normalize를 한다. 그 후에 EBD처럼 $\\lambda_0’, \\lambda_1’-\\lambda_0’, \\lambda_2’ - \\lambda_1’$의 값을 계산한다. 그 후에 SHOT처럼 위의 그림과 같이 영역을 나눠서 영역 안에 있는 point들에 대해서 앞에 계산한 3개의 feature를 이용해 histogram을 만든다.16. Covariance Based Descriptors - 3D classificationPaper: Compact covariance descriptors in 3D point clouds for object recognitionspin image로부터 발전한 방법으로 compact하고 flexible한 representation power가 큰 feature들을 많이 갖고 잇는 descriptor라고 한다. 이 descriptor가 사용하는 geometric relation은 아래의 그림에서의 $\\alpha, \\beta, \\theta,\\rho, \\psi, n’$이다.$\\alpha, \\beta$는 spin image에서와 같고 $\\theta$는 $p$의 surface normal $n$과 $p’-p$가 이루는 각도이고 $\\rho$는 $p$와 $p’$사이의 distance, $\\psi$는 $p$의 surface normal $n$과 $p’$의 surface normal $n’$이 이루는 각도이다.이 방법은 computation이 적고 memory도 적게 사용하고 tuning할 parameter가 필요 없다고 한다. 여기에 r, g, b color를 사용해서 성능을 향상 시켜보려는 시도도 있었고 shape feature와 visual feature를 함축하는 covatiance descriptor를 사용했던 시도도 있다고 한다.17. Surface Entropy - 3D place recognitionPaper: SURE: Surface Entropy for Distinctive 3D Featuressurface entropy는 shape와 color(가능한경우) information을 이용한 local shape-texture descriptor이다.Descriptor에서 feature를 얻는 과정은 우선 surface entropy를 구해서 entropy가 특정 threshold보다 낮은 interest point들을 찾아서 noise인 point를 최대한 제거한다. 그리고 interest point $p$와 neighbor $q$에서의 surfels $(p,n_p), (q, n_q)$를 이용해 local reference frame $(u, v, w)$를 정의한다. 이 값들을 이용해 두 surfel pair의 관계를 다음과 같이 $\\alpha, \\beta, \\gamma, \\delta$로 나타낸다.$\\alpha:=arctan(2(w\\cdot n_q, u\\cdot n_q)), \\beta:=v\\cdot n_q, \\gamma:=u\\cdot \\frac{d}{\\Vert d\\Vert _2}, \\delta:=\\Vert d\\Vert_2$최종 histogram의 형태는 모든 surfel pair에 대해서 위의 값들을 binning한 형태가 된다. 만약 color 정보를 사용할 수 있다면 HSL color space 기반으로 hue값과 saturation여부를 이용해 두개의 histogram을 더 사용하게 된다. 이 descriptor는 view-pose invariant하다.18. 3D Self-Similarity DescriptorPaper: Point cloud matching based on 3D self-similarity이 descriptor는 self-similarity라는 개념을 3D point cloud로 확장해서 만든 descriptor라고 한다. self-similarity란 해당 부분의 data가 다른 부분의 data와 얼마나 닮았는지를 비교하는 property라고 한다.3D self-similarity descriptor는 크게 두가지 단계로 나뉜다. 첫번째 단계에서 두 point사이의 normal similarity, curvature similarity, photometric similarity 이 세가지 similarity를 계산한다.normal similarity: $s(x, y, f_{normal})=[\\pi-cos^{-1}(n(x)-n(y))]/\\pi$curvature similarity: $s(x,y,f_{curv})=1-|f_{curv}(x)-f_{curv}(y)|$photometric similarity: $1-|I(x)-I(y)|$그리고 세개의 similarity들을 하나로 합쳐서 united similarity를 정의한다.united similarity: $s(x,y)=\\sum_{p\\in PropertySet} w_p\\cdot s(x, y, f_p)$이 united similarity를 비교함으로써 self-similarity surface를 만들게 되는 것이다.다음 단계에서는 point를 기준으로 LRF를 구해서 rotation invariant하게 하고 주변의 space를 spherical coordinate에서 quantization을 하고 각각의 bin에 존재하는 주변 point들의 average similarity값을 bin의 값으로 부여하는 식으로 descriptor를 생성한다.이 descriptor는 distinctive geometric signature를 잘 나타낸다고 논문에서 말한다.19. Geometric and Photometric Local Feature (GPLF)Paper: Robust descriptors for 3D point clouds using Geometric and Photometric Local FeatureGPLF는 $f=(\\alpha, \\delta, \\phi, \\theta)$로 표현되는 geometric property들과 photometric characteristic을 온전하게 사용하여 나타낸 descriptor라고 한다.기준 point $p$와 $p$의 normal $n$에 대해 $k$-nearest neighbor $p_i$를 구하고 이 point들을 이용해 몇개의 geometric parameter들을 다음과 같이 유도할 수 있다. $a, b, c$ 식에서 보면 알 수 있지만 $p_i$의 normal 방향을 기준으로 $p$와 $p_i$의 위치관계를 parallel, directly, perpendicular 하게 나타낸 property들이다.\\[b_i=p-p_i, \\ a_i=n_i(n_i\\cdot b_i), \\ c_i=b_i-a_i\\]우선 color feature인 $\\alpha, \\delta$는 HSV color space에서 아래와 같은 식을 이용해 구한다. 아래에 $h, v$는 HSV에서의 hue와 value를 의미한다. $\\delta$는 식을 보면 주변 point와의 normal 기준으로 perpendicular distance와 value값의 차이의 비율을 의미한다. 즉 얼마나 색변화가 심한가를 의미하는 것이다.\\[\\alpha_i = h_i, \\ \\delta_i = \\frac{v_i-v}{\\Vert c_i\\Vert }\\]그리고 angular feature $\\phi, \\theta$는 다음과 같이 정의된다.\\[\\phi_i = arccos(\\frac{d_i\\cdot c_i}{\\Vert d_i\\Vert \\ \\Vert c_i\\Vert }), \\ \\phi_i = \\begin{cases} -\\phi_i \\text{ if} \\ n_i\\cdot(d_i \\times c_i) \\ge 0 \\\\ \\ \\ \\ \\phi_i \\ \\ \\ \\ \\ \\ \\text{ otherwise} \\end{cases}\\]\\[\\theta_i = arccos(\\frac{\\Vert a_i\\Vert }{\\Vert c_i\\Vert }), \\ \\theta_i = \\begin{cases} \\ \\ \\ \\theta_i \\text{ if} \\ n_i\\cdot a_i \\ge 0 \\\\ -\\theta_i \\ \\ \\text{ otherwise} \\end{cases}\\]여기서 $d_i = \\sum_{i=1}^k(\\frac{c_i}{\\Vert c_i\\Vert }) \\delta_ie^{-\\frac{\\Vert e_i\\Vert }{2}}$이다. $d_i$는 일종의 point $p$를 기준으로 주변 point들의 direction들의 dominant direction이라고 보면 된다. 최종 histogram 형태는 neighbor point $p_i$의 거리에 따라서 4개의 subgroup으로 나누고 각각의 subgroup안의 point들의 4개의 feature를 모두 8개의 bin으로 나눠서 총 4x4x8의 128의 크기를 가진 histogram이 된다.20. MCOVPaper: MCOV: A Covariance Descriptor for Fusion of Texture and Shape Features in 3D Point CloudsMCOV는 visual 정보와 3D shape 정보를 융합한 covariance descriptor다. 주어진 point $p$와 이 point의 주변의 radius $r$ 이내의 neighborhood $p_i$에 대해서 feature selection function $\\Phi(p, r)$은 다음과 같다.\\[\\Phi(p,r) = \\{ \\phi_{p_i}, \\forall p_i \\ \\ \\text{s.t.} |p-p_i| \\le r \\}\\]$\\phi_{p_i}=(R_{p_i}, G_{p_i}, B_{p_i}, \\alpha_{p_i}, \\beta_{p_i}, \\gamma_{p_i})$는 각 point $p_i$에서 얻은 random variable의 vector이다. 여기서 앞의 3개의 값은 $p_i$의 R,G,B값과 관련된 값이며 texture information을 포함하고자 했고 뒤의 3개의 값은 각각 $\\langle n_p, (p_i-p)\\rangle, \\langle n_{p_i}, (p_i,-p) \\rangle, \\langle n_p, n_{p_i} \\rangle$ 이다. $\\langle \\rangle$의 의미는 두 vector사이의 angle이다. 모든 feature는 scale invariance를 위해 normalized된 값을 사용한다. 주어진 point $p$에 대해서 radius $r$이내의 covariance descriptor는 다음과 같이 나타낸다.\\[C_r(\\Phi(p,r))=\\frac{1}{N-1}\\sum^N_{i=1}(\\phi_{p_i}-\\mu)(\\phi_{p_i}-\\mu)^T\\]여기서 $\\mu$는 $\\phi_{p_i}$의 평균 값이며 이 식은 결국 neighboring point들의 feature 값들의 correlation을 나타낸다. MCOV의 output은 결국 6x6 symmetric matrix인 이 $C_r$이다.21. Histogram of Oriented Principal Components (HOPC)Paper: HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for Action RecognitionHOPC는 viewpoint variation과 noise의 영향을 줄이기 위해 고안된 descriptor이다.우선 keypoint를 중심으로 PCA를 해서 eigenvector와 eigenvalue들을 구한다. 그리고 이 eigenvector들을 $regular \\ m-sided \\ polyhedron$으로 부터 나온 $m$ direction들에 projection하고 eigenvalue을 이용해 scale한다. 마지막으로 projected된 eigenvector를 eigenvalue의 decreasing order에 따라서 concatenate한 3xm 크기의 matrix가 HOPC descriptor의 최종형태이다.22. Height Gradient Histogram (HGIH)Paper: Height gradient histogram (high) for 3d scene labeling(a)는 point p의 spherical 영역을 나타내었고 (b)는 sub-region으로 나눈 예시 (c)는 각 point와 sub-region에 따른 2D histogram의 예시이다HIGH는 $f(p)=p_x$ ($x$방향이 height)로 3D point cloud $p=(p_x, p_y, p_z)$에서 height dimension data를 추출해 활용한 descriptor라고 한다.우선 이렇게 height 값을 추출한 후에 linear gradient reconstruction method를 사용해서 $p$에 대해 $p$주변의 point들과의 height gradient $\\nabla f(p)$를 계산한다.두 번째로 point $p$에 주변의 spherical 영역을 $K$개의 sub-region들로 나눠서 각 sub-region내부의 point들의 gradient 방향이 histogram의 형태로 변환된다.마지막으로 이 HIGH feature descriptor는 모든 subregion의 histogram을 concatenate한 형태가 된다. 이 방법은 small object를 표현하기에는 적합하지 않다고 한다.23. Equivalent Circumstance Surface Angle Descriptor (ECSAD)Paper: Geometric edge description and classification in point cloud data with application to 3d object recognitionECSAD는 3D point cloud에서 geometric edge를 찾기 위해 개발된 descriptor라고 한다. feature point $p$의 주변 region을 radial, azimuth를 축으로 몇몇 cell로 나눈다. 그리고 각각의 cell 내부의 주변 point들에 대해서 $p$에서의 normal $n$과 $p-p_i$ 사이의 angle의 평균을 구한다. 여기서 empty bin은 주변 bin을로 부터 interpolation을 이용해서 구한다. 논문에서 사용한 ECSAD의 dimension은 4개의 radial level을 사용했고 level당 azimuth을 1, 2, 3, 4로 나눠서 총 60개의 cell이 나오게 되는데 이 descriptor는 edge를 찾는것이 목적이기 때문에 center를 기준으로 opposing bin의 값을 이용을 하는데 여기서 opposing bin 해당하는 두 cell 내부의 angle의 평균을 사용함으로써 dimension이 절반이 된다. 그래서 최종적으로 ECSAD의 descriptor 크기는 30이 된다.24. Binary SHOT (B-SHOT)Paper: B-SHOT : A Binary Feature Descriptor for Fast and Efficient Keypoint Matching on 3D Point CloudsB-SHOT은 이름 그대로 SHOT의 값들을 0 또는 1로 대체한 형태라고 보면 된다. Descriptor를 구성하는 과정은 SHOT와 같은 절차로 되어있으면서 5개의 possibillity를 기준으로 binary value를 결정한다고 한다. 이 기준을 설명하기에 앞서 notation을 정리하면 SHOT descriptor의 값은 ${S_0,S_1,S_2,S_3}$이고 B-SHOT으로 변환된 descriptor의 값은 ${B_0,B_1,B_2,B_3}$로 표현할 것이다. 그리고 $S_{sum} = S_0+S_1+S_2+S_3$이다.Case A: 모든 $S_i$가 0일 경우에 모든 $B_i$ 또한 0이다.Case B: Case A를 만족하지 않을 경우 $S_i$중 하나가 $S_{sum}$의 값의 90% 이상을 차지한다면 해당하는 $i$번째 값으로 one-hot encoding을 한다. (i.e. ${S_0,S_1,S_2,S_3}={0.85, 0.01, 0.01,0.01}$이면${B_0,B_1,B_2,B_3}={1,0,0,0}$이다.Case C: Case A와 Case B를 만족하지 않을 경우 $S_i$들 중 두 값의 합($S_i+S_j$)이 $S_{sum}$의 90%이상일 경우 $B_i, B_j$의 값을 1로 하고 나머지 두 값을 0으로 한다.Case D: Case A, B, C 를 만족하지 않을 경우 $S_i$들 중 세 값의 합($S_i+S_j+S_k$)이 $S_{sum}$의 90%이상일 경우 $B_i, B_j, B_k$의 값을 1로 하고 나머지 값을 0으로 한다.Case E: 위의 어느 조건도 만족하지 않을 경우 ${B_0,B_1,B_2,B_3}={1, 1, 1, 1}$이다.이 B-SHOT을 사용하면 SHOT에 비해 memory도 덜 사용하고 매우 빠르다고 한다.25. Rotation, illumination, Scale Invariant Appearance and Shape Feature (RISAS)Paper: RISAS: A Novel Rotation, Illumination, Scale Invariant Appearance and Shape FeatureRISAS는 spatial distribution, intensity information, geometrical information의 3가지 statistical histogram을 포함한 feature vector이다.Spatial distribution을 구하기 위해서는 spherecal support 영역을 $n_{pie}$개의 일정한 크기의 영역으로 나누고 각각의 sector안에 있는 position과 depth value를 이용해 spatial histogram을 생성한다. Sector수가 늘어날수록 descriptor가 discriminative해진다.Intensity information은 absolute intensity대신 relative intensity를 사용해 $n_{bin}$개의 bin들로 나눠서 intensity histogram을 구성한다.Geometric information은 기준 point $p$와 그 neiboring point $q$의 normal의 각도 차이 $\\rho_i=|\\langle n_p, n_q\\rangle|$를 이용해 $n_{vec}$개의 bin으로 나눠서 geometric histogram을 구성한다. 여기서 대부분의 $\\rho$는 1에 가까우므로 threshold $\\bar{\\rho}$ 이상의 값을 하나의 카테고리로 묶고 나머지를 $n_{vec}$개로 나누는 방식으로 하므로 실제로는 $n_{vec}+1$개의 bin이 된다.최종 discriptor의 dimension은 $n_{pie} \\times n_{bin} \\times (n_{vec} + 1)$이 된다.26. Colored Histograms of Spatial Concentric Surflet-Pairs (CoSPAIR)Paper: Cospair: Colored histograms of spatial concentric surflet-pairs for 3d object recognitionPFH와 FPFH와 유사하게 CoSPAIR는 surflet-pair relation을 기반으로 한 descriptor다.주어진 point $p$와 그 주변 point $q$에 대해서 LRF와 세가지 angle을 구하는 법은 PFH와 같은 방법으로 구하고 radius에 따라서 spherical shell들로 영역을 나눈다.각각의 shell에 대해서 shell 내부의 point들의 3개의 angular feature를 이용해 3개의 histogram들을 생성한다. 원래 SPAIR는 모든 shell의 histogram들을 concatenate 한다.그런데 CoSPAIR는 마지막으로 CIELab color space의 채널들로 각각의 shell마다 histogram들을 만들고 SPAIR descriptor의 histogram들과 함께 써서 CoSPAIR를 만든다. 이 방법은 빠르고 간단한 방법이라고 알려져 있다고 한다.27. Signature of Geometric Centroids (SGC)Paper: Signature of geometric centroids for 3d local shape description and partial shape matchingSGC는 PCA기반으로 unique한 LRF를 중심으로한 최초의 descriptor라고 한다.$p$의 주변의 spherical support region인 $S_p$을 LRF를 구해서 align하고 이 $S_p$를 둘러싸는 edge의 길이가 $2R$인 cubical volume을 구하고 이를 $K \\times K \\times K$개의 voxel로 나눈다.마지막으로 descriptor는 이 voxel안에 들어간 point의 수와 중심 point를 기준으로 한 position을 concatenate해서 원래라면 $4 \\times K \\times K \\times K$ dimension이어야 하지만 3개의 positional value를 $C = (Z \\times L + Y)\\times L + X$의 식$(L=2R)$을 통해 일종의 L-bit의 값으로 compress하여서 $2 \\times K \\times K \\times K$로 storage공간 사용을 줄였다.28. Local Feature Statistics Histograms (LFSH) - 3D registrationPaper: A fast and robust local descriptor for 3D point cloud registrationLocal feature statistics histogram은 3개의 local shape geometry 이용해 feature를 추출하는 방식이다. Local shape geometry는 local depth, point density, angles between normals의 3가지 feature로 구성되어 있다.point $p$ 를 중심으로한 반지름이 $r$ 인 sphere 내부에 속하는 point들을 neighboring point $p_i$ 라 하고 이 sphere에 접하고 normal vector가 $p$ 의 surface normal 인 plane을 L이라고 하고 $p_i$ 를 $L$에 projection 시킨 point 들을 ${p_i}’$ 이라고 하면 feature는 다음과 같이 구할 수 있다. local depth: $d = r - n \\cdot ({p_i}’- p_i)$ deviation angle between normals: $\\theta = arccos(n_{p’}, n_p)$ point density: $\\rho = \\sqrt{ {\\Vert p’ - {p_i}’\\Vert}^2 - (n \\cdot (p’ - {p_i}’))^2}$local depth 는 결국 point $p$ 의 depth를 기준으로 즉, 0으로 한 neighboring point의 depth라고 볼 수 있고 deviation angle between normal 는 말 그대로 neighboring point와 기준 point 사이의 normal vector의 angle의 차이를 구하는 것이고 point density 는 plane L상에 projection된 point들의 horizontal한 거리를 나타낸 것으로 horizontal distance를 사용해서 histogram에서 normalization을 하면 거리별로 존재할 probability를 나타내는 셈이 되고 각각의 bin의 값은 그 구간에서의 density를 표현하게 된다.feature histogram의 최종 형태는 이 3개의 feature의 sub-histogram을 concatenation한 형태가 되며 이 방식은 low dimension을 가지며 computational complexity가 낮고 여러 nuisance에 robust하다고 한다.29. 3D Histogram of Point Distribution (3DHoPD)Paper: 3DHoPD: A Fast Low Dimensional 3D Descriptor3DHoPD는 다음과 같은 과정을 통해 descriptor를 구성한다.우선 기준 point에 대해서 rotational invariance를 위해 LRF를 구하고 이를 기준으로 align한다. 다음은 $x$축을 기준으로 $x$의 최솟값과 최댓값사이의 범위를 D개의 bin으로 나눠서 histogram들을 만들고 각각의 bin에 해당하는 point들을 넣는다. $y, z$에 대해서도 같은 방식으로 한다. 3DHoPD는 이 histogram들과 기준 point의 좌표를 concatenate해서 만든다. 이 방법의 장점은 computation이 빠르다는 것이다.Global DescriptorGlobal descriptor는 전체 3D scene에서의 geometric information을 표현하는 방법이다. local descriptor에 비해서 상대적으로 computation량이 적고 memory 사용량도 적은 편이다. 주로 object recognition, shape retrieval에 쓰인다고 한다.1. Point Pair Feature (PPF)Paper: Model globally, match locally: Efficient and robust 3d object recognitionSurflet-pair feature와 유사하게 주어진 두 point $p_1, p_2$와 그 normal을 $n_1, n_2$라 하면 PPF는 다음과 같이 정의된다.\\[F=(\\Vert p_1-p_2\\Vert _2,\\angle(n_1, p_2-p_1),\\angle(n_2,p_2-p_1),\\angle(n_1, n_2))\\]여기서 각의 범위는 $[0,\\pi]$이다. feature를 discretize하고 비슷한 값을 가진 point pair는 위의 우측의 그림과 같이 서로 합쳐서 저장한다. 이 global descriptor의 최종형태는 이렇게 PPF space의 값들을 mapping한 형태가 된다.2. Global RSD (GRSD)Paper: Hierarchical Object Geometric Categorization and Appearance Classification for Mobile ManipulationGRSD는 local RSD descriptor의 global version이라고 보면 된다. Input point cloud를 voxelize하고 나서 5개의 surface primitive중 하나로 labeled된 voxel의 smallest, largest radius를 각각 구한다. 그리고 이 annotated된 voxel들의 relationship을 이용해서 특정한 task를 수행할 수 있다고 한다.3. Viewpoint Feature Histogram (VFH)Paper: fast 3d recognition and pose using the viewpoint feature histogramVFH는 FPFH에서 추가적으로 viewpoint variance를 고려한 방법이라고 한다. VFH는 viewpoint direction component와 surface shape component로 구성된다.Viewpoint direction component는 central viewpoint direction과 각각 point의 surface normal의 angle difference $\\beta = arcccos(n_p\\cdot (v-p) / ({\\Vert v-p\\Vert}_2))$ 의 histogram의 형태이고,Shape component는 FPFH처럼 centroid point기준으로 모든 point 와의 $\\alpha, \\phi, \\theta$ angle을 각각 45개의 bin갯수를 가진 3개의 sub histogram으로 나타낸 형태이다.VFH는 $O(n)$의 complexity를 가지며 recognition에서 높은 performance를 보이지만 noise와 occlusion에 취약했다.4. Clustered Viewpoint Feature Histogram (CVFH)Paper: CAD-model recognition and 6DOF pose estimation using 3D cuesCVFH는 VFH의 확장버전이라고 보면 되는데 high curvature를 가진 point를 제거하고서 region growing algorithm을 사용함으로써 얻은 stable object region을 사용해서 이를 개선하려고 했다고 한다. Region set $S$ 에 대해서 region $s_i$가 있다고 하면 $s_i$에 대해서 FPH에서 처럼 centroid $p_c$ , normal $n_c$를 이용해서 darvoux frame$(u_i,v_i,w_i)$을 생성한다. 그리고 VFH에서 처럼 region마다 centroid를 기준으로 angular information $(\\alpha, \\phi, \\theta, \\beta)$을 4개의 histogram에 binning하고 여기에 Shape Distribution Component (SDC)를 계산한다. SDC를 계산하는 식은 다음과 같다. $SDC= \\frac{(p_c-p_i)^2}{max((p_c-p_i)^2)}$ 그래서 CVFH descriptor의 최종 형태는 5개의 histogram$(\\alpha, \\phi, SDC, \\theta, \\beta)$을 concatenate해서 각각 45, 45, 45, 45, 128 dimension을 가지고 있으므로 dimension이 308인 histogram이 된다. CVFH는 좋은 성능을 내긴 하지만 euclidean space에 대한 정보가 없어서 feature에 적절한 spatial description이 포함되지 않을 수 있다고 한다.5. Oriented, Unique and Repeatable Clustered Viewpoint Feature Histogram (OUR-CVFH)Paper: OUR-CVFH – Oriented, Unique and Repeatable Clustered Viewpoint Feature Histogram for Object Recognition and 6DOF Pose EstimationOUR-CVFH는 CVFH와 같이 surface shape distribution component와 viewpoint component를 사용하지만 viewpoint component의 dimension을 64로 줄였다고 한다. 거기에다가 CVFH의 shape distribution component를 대체하여 surface에 있는 point들과 centroid사이의 distance의 값을 사용해 원래의 $xyz$-axis를 SGURF로 구한 RF에 align하고 $(x^-,y^-,z^-)…(x^+,y^-,z^-)…(x^+,y^+,z^+)$의 영역으로 point들을 나눠서 이 point들의 distance를 이용해 bin size가 13인 8개의 histogram들을 추가했다고 한다. OUR-CVFH의 최종 형태는 $45\\times 3+13\\times 8 + 64=303$ dimension의 concatenate된 histogram이다.6. Global Structure Histograms (GSH)Paper: Improving Generalization for 3D Object Categorization with Global Structure HistogramsGSH는 point cloud의 global, structural property를 나타내는 descriptor이다. GSH는 3개의 stage로 나뉜다.첫째는 local descriptor를 각각의 point에 적용한뒤 k-means clustering을 사용한 후 Bag of Words model을 사용해 여러개의 surface class로 point들을 labeling한다.두번째는 object자체를 3D상의 non-empty volume을 triangulation을 통해서 surface form에 따라 다른 class들 사이의 관계를 결정한다.마지막으로 이전 과정에서 생성한 class들의 같은 class사이의 surface상의 shortest path의 distance로 histogram을 만들어 GSH를 만든다. 이 descriptor는 variation이 낮고 global geometry나 structure를 잘 설명한다고 한다.7. Shape Distribution on Voxel Surfaces (SDVS)Paper: Shape distributions on voxel surfaces for 3d object classification from depth imagesShape function이라는 개념을 기반으로 SDVS는 실제 surface에 가깝게 approximate된 voxel grid를 생성하고 임의로 sample한 두 개의 point의 distance로 두 point를 잇는 line의 location(inside, outside, mixture)에 따라서 3개의 histogram으로 binning해서 feature histogram을 생성한다. 이 방법은 간단하지만 복잡한 shape를 처리하기에는 적합하지 않다고 한다.8. Ensemble of Shape Functions (ESF)Paper: Ensemble of shape functions for 3D object classificationESF는 real time application을 목표로 나온 방법론이다. ESF는 3개의 angle histogram, 3개의 area histogram, 3개의 distance histogram, 1개의 distance ratio histogram으로 이루어져 있고 각 histogram은 64개로 bin되어 총 640개의 parameter를 가지는 histogram이다.angle, area, distance histogram은 각각 A3(sample된 3개의 point에 의해 생성된 2개의 line에 의한 angle), D3(sample된 3개의 point에 의해 생성된 area), D2(sample된 point쌍이 이루는 line의 길이) shape function의 값을 기준으로 ON, OFF, MIXED로 classifying하여 histogram을 생성한다.D2 function에서 만들어지는 histogram ON(green), OFF(red), MIXED(blue)D2 function에서 ON은 point들을 잇는 line이 surface위에 있을 경우, OFF는 endpoint만이 surface위에 있을 경우, MIXED는 부분적으로 ON일 경우이다.distance ratio histogram그리고 여기에서 line마다 ON인 부분의 ratio를 이용해 distance ratio hitogram을 구한다.A3 fucntion에서 만들어지는 histogramA3 fucntion에서 ON, OFF, MIXED는 angle의 맞은편의 line에 대해 D2와 같은 방식으로 classification을 한다.D3 function에서 만들어지는 histogramD3 function에서는 A3와 같은 방식으로 classification을 한다.9. Global Fourier Histogram (GFH)Paper: Performance of global descriptors for velodyne-based urban object recognitionGSF는 origin이 $O$인 oriented point를 이용해 생성하는 descriptor라고 한다. global reference frame을 정하기 위해 편의상 $O$에서의 normal은 $z$-axis $(z=[0, 0,1]^T)$로 정한다. 그리고 $O$를 중심으로 한 cylindrical support region을 azimuth, radial , elevation을 균등하게 나눠서 bin을 생성한다. GFH descriptor는 각각의 bin 안에 존재하는 point 수를 합해서 생성한 3D histogram이 된다. 여기서 robustness를 증가시키기 위해서 1D FFT를 3D histogram에 적용해서 azimuth dimension에서의 frequency domain으로 transform한다. 이 descriptor는 Spin Image 방법의 단점을 보완할 수 있다고 한다.10. Position-related Shape, Object Height along Length, Reflective Intensity Histogram (PRS, OHL and RIH)Paper: Robust vehicle detection using 3D Lidar under complex urban environmentPRS, OHL and RIH는 vehicle detection rate를 크게 증가시키기 위해서 PRS, OHL, RIH라는 새로운 세가지 feature들을 사용해 vehicle과 다른 object를 robust하게 구별했다고 한다.PRS는 shape feature(width-length, witdth-height ratio)와 position information(distance, orientation, angle of view)을 사용해 orientation과 angle of view에 따른 variance를 처리했다고 한다.OHL은 vehicle의 bounding box를 length에 따라 몇 개의 block으로 나눠서 block들의 average height를 feature vector에 추가해서 discrimination 성능을 추가로 향상 시켰다.마지막으로 25개의 bin을 가진 RIH는 vehicle의 characteristic intensity distribution을 이용해 계산한다고 한다.11. Global Orthographic Object Descriptor (GOOD)Paper: GOOD: A global orthographic object descriptor for 3D object recognition and manipulationGOOD는 처음에 unique하고 repeatable한 LRF를 PCA를 통해서 구한다. 그리고 point cloud를 $xy$, $yz$, $zx$ 이 세개의 plane에 orthographic하게 projection한다. 그리고 각각의 plane을 몇개의 bin들로 나누고 이 bin안에 속하는 point의 수를 세서 distribution matrix를 생성한다. 그 후에 이 matrix에 대해 entropy와 variance 이 두 가지의 statistical feature들의 distribution vector를 구한다. 이 vector들을 concatenate해서 전체 object에 대해 single vector의 형태로 feature가 나오게 된다. GOOD는 scale, pose에 대해 invariant하고 expressiveness와 computational cost 사이의 trade-off가 있다고 한다.12. Globally Aligned Spatial Distribution (GASD) (TODO)Paper: An Efficient Global Point Cloud Descriptor for Object Recognition and Pose EstimationGASD는 두 가지 step으로 나뉜다. 우선 전체 model에서 PCA를 통해 RF를 구하고 이 RF에 point cloud를 align한다. 그 후에 axis-aligned된 point cloud의 bounding cube를 $m_s\\times m_s \\times m_s$의 cell로 나눈다. 최종 histogram은 이 각각의 grid에 들어가 있는 point의 수를 합해서 만든 histogram을 concatenate해서 얻는다. discriminative power를 증가시키기 위해서 HSV space기반의 color13. Scale Invariant Point Feature (SIPF) (TODO)Paper: Scale invariant point feature (SIPF) for 3D point clouds and 3D multi-scale object detectionSIPF는14. Global Fast Point Feature Histogram (GFPFH) (TODO)Paper: Detecting and Segmenting Objects for Mobile ManipulationHybrid Descriptor1. Bottom-Up and Top-Down Descriptor (TODO)Paper: Object Detection from Large-Scale 3D Datasets Using Bottom-Up and Top-Down Descriptors2. Local and Global Point Feature Histogram (LGPFH) (TODO)Paper: Real-time object classification in 3d point clouds using point feature histograms3. Local-to-Global Signature Descriptor (LGS) (TODO)Paper: Local-to-global signature descriptor for 3d object recognition4. Point-Based Descriptor (TODO)Paper: A Multiscale and Hierarchical Feature Extraction Method for Terrestrial Laser Scanning Point Cloud Classification5. FPFH + VFH (TODO)Paper: 3D Object Recognition Based on Local and Global Features Using Point Cloud LibraryDeep Learning Based DescriptorFeature based representationPPFNetVoxelization representationMulti-view representationKD-tree representationPoint cloud inputGraph-based representationMulti-sensors" }, { "title": "Histogram Comparison 방법들", "url": "/posts/histogram-comparison/", "categories": "Metric", "tags": "Machine Learning", "date": "2020-08-22 00:00:00 +0900", "snippet": "서로 다른 두 histogram H1과 H2의 차이(혹은 유사도)를 측정할때 주로 쓰이는 metric 4가지를 알아보고자 한다. Cross Correlation\\[d(H_1,H_2) = \\frac{\\sum_I (H_1(I) - \\bar{H_1}) (H_2(I) - \\bar{H_2})}{\\sqrt{\\sum_I(H_1(I) - \\bar{H_1})^2 \\sum_I(H_2(I) - \\bar{H_2})^2}}\\] where\\[\\bar{H_k} = \\frac{1}{N} \\sum _J H_k(J)\\] 이 식의 형태는 cross-covariance를 normalization한 식과 일치한다. 즉 sliding inner product를 통해서 두 histogram의 분포가 얼마나 유사한지를 나타내는 식이다. 값의 범위는 0~1이고 1에 가까울 수록 유사한 것이고 0에 가까울 수록 분포가 다른 것이다. Chi-Square\\[d(H_1,H_2) = \\sum _I \\frac{\\left(H_1(I)-H_2(I)\\right)^2}{H_1(I)}\\] chi-square는 한국어로는 카이제곱이라고 부르는데 기준이 되는 histogram $H_1(I)$ 의 분포와 비교해서 이러한 분포가 실제로 일어날 법한 일인지에 대한 척도라고 하는데, 우리가 기록해서 보유하고 있는 histogram을 기준으로 현재 이러한 histogram 나타날 확률이 얼마나 되는지를 나타낸다고 보면 될 것 같다. Intersection\\[d(H_1,H_2) = \\sum _I \\min (H_1(I), H_2(I))\\] 이 방법은 가장 직관적으로 알기 쉬운 방법인데 말 그대로 두 histogram이 겹치는 넓이를 구하는 방법이다. 보통 histogram을 normalize해서 사용하는데 normalize한다는 가정 하에는 1에 가까울수록 유사하고 0에 가까울수록 분포가 다르다는 것을 의미한다. Bhattacharyya distance\\[d(H_1,H_2) = \\sqrt{1 - \\frac{1}{\\sqrt{\\bar{H_1} \\bar{H_2} N^2}} \\sum_I \\sqrt{H_1(I) \\cdot H_2(I)}}\\] 이 읽기 힘들게 생긴 이름을 가진 식은 한국어로는 바타차야 거리라고 읽는다고 한다. 대부분 이 식을 두고 bhattacharyya distance라고 하는데 실제로 식은 Helinger distance에 해당한다. 왜 이렇게 명명했는지는 잘 모르겠지만 helinger distance는 bhattacharyya distance와 term을 공유하는 변형된 형태지만 triangle inequality를 만족해서 이러한 형태를 쓰는 것으로 보인다. 이 방법은 두 확률 분포의 distance를 유사도를 측정하는 방법이라고 하는데 histogram을 histogram의 전체 크기로 나눠서 합이 1로 만들어 확률분포 처럼 만들어서 histogram의 분포의 유사도를 구한다고 보면 된다. 식을 통해서 의미를 살펴보면 두 histogram의 bin이 중첩되는 영역의 크기를 모두 더한 뒤 1에서 뺴주는 형태의 식으로 1에 가까울 수록 중첩되는 영역이 적은 것이고 0에 가까울 수록 중첩되는 영역이 커지는 것이다. 따라서 0~1의 값을 가지고 1에 가까울 수록 불일치에 가깝고 0에 가까울 수록 일치에 가까운 것을 의미한다. " }, { "title": "FlowNet3D 정리", "url": "/posts/flownet-3d/", "categories": "Pointcloud Registration", "tags": "Non-rigid, Deep Learning", "date": "2019-09-28 00:00:00 +0900", "snippet": "paper: FlowNet3D: Learning Scene Flow in 3D Point Cloudscode: https://github.com/xingyul/flownet3d이 논문은 dynamic envorionment에서 point의 3D motion(scene flow)를 찾기 위해 제안된 논문이다. 그래서 FlowNet3D를 제안해 end-to-end로 sceneflow를 학습하였다. FlowNet3D는 두 개의 새로운 layer를 제안해 hierarchical 한 feature와 point의 motion을 representation하는 것을 배운다.과거에 실제로 사용해봤던 논문인데 radius이내의 주변 point에 대해서만 학습을 하다보니 rotation이나 difference가 조금만 심해도 못 찾는 모습을 보였다. non rigid registration이라 볼 수 있는 알고리즘이다보니 결과가 point의 3D flow라서 SVD를 통해서 rotation을 한번 구하는 과정도 거쳐야 transformation을 구할 수 있다.이 논문의 main contribution은 다음과 같다. 연속된 point cloud에서 end-to-end로 scene flow를 구할 수 있는 FlowNet3D를 제안하였다. 두 point cloud사이를 연관짓는 flow embedding layer와 하나의 point set에서 다른 set으로 feature를 propagating하는 set upconv layer를 제안하였다. Method Problem definition 이 논문에서 풀고자 하는 문제를 정의하자면 source cloud \\(P= \\{ x_i|i=1,...,n_1 \\}\\)가 있고 target cloud \\(Q= \\{ y_j|j=1,...,n_2 \\}\\)가 있어서 $P$에 있는 모든 point가 $Q$로 scene이 바뀔때 어떤 motion을 가지는지 $P$의 point들의 motion \\(D= \\{ d_i|i=1,...,n_1 \\}\\)를 구하는 문제다. FlowNet3D Architecture FlowNet3D는 point의 feature를 학습하고, 두 scene의 point를 합쳐서 flow embedding을 하고, flow를 모든 point로 propagating하는 3개의 key module로 이루어져 있다. Hierarchical Point Cloud Feature Learning PointNet++의 구조를 차용했으며 위의 그림의 맨 왼쪽에 해당한다. Farthest point sampling으로 centroid들을 뽑고 centroid들을 중심으로한 local coordinate값을 input으로 넣어서 feature를 추출한다. 여기서 max는 element-wise max이다.\\[f^{\\prime}_j=\\max\\limits_{\\{i|||x_i-x_j^{\\prime}||\\le r\\}}\\{h(f_i,x_i-x^{\\prime}_j)\\}\\] Point Mixture with Flow Embedding Layer 두개의 point cloud 합쳐서 flow를 embedding하기 위해서 flow embedding layer를 제안하였으며 위의 그림의 가운데에 해당한다. 실제로는 frame $t$와 frame $t+1$ 사이에 정확한 correspondence가 존재하지 않을 수 있지만 frame $t+1$에 존재하는 point들을 이용해 weighted decision을 내리는 아이디어를 채택하여 이 layer를 고안했다. $P$의 point를 $x_i$, feature를 $f_i$, $Q$의 point를 $y_j$, feature를 $g_j$, embed된 flow를 $e_i$라고 하고 마찬가지로 PointNet++를 사용하여 모든 $x_i$에 대해서 주변 radius이내에 존재하는 $y_j$들을 찾고 둘 사이의 difference와 $x_i$와 $y_j$의 feature를 input으로 넣어서 주변 point들의 feature의 distance를 이용해 difference 값들의 weight를 학습해서 flow를 embedding을 할 수 있도록 하였다. 여기서 max는 element-wise max이다.\\[e_i=\\max\\limits_{\\{j|||y_j-x_i||\\le r\\}}\\{h(f_i,g_j,y_j-x_i)\\}\\] 그리고 이후에 set conv를 거쳐서 spatial smoothing을 하였다. Flow Refinement with Set Upconv Layer 이 모듈은 flow embedding된 결과를 모든 point로 upsampling 하는 부분으로 이를 위해 set upconv를 제안했으며 위의 그림의 오른쪽에 해당한다. set upconv는 set conv를 거의 그대로 사용하는데 local region sampling 방법이 다르다고 한다. FPS를 통해 뽑은 centroid 대신에 upsampling을 적용할 target point를 기준으로 한다. 단순한 3D interpolation \\((f^{\\prime}_j=\\sum_{\\{i|||x_i-x_j^{\\prime}||\\le r\\}}w(x_i,x_j^{\\prime})f_i)\\) 과 비교했을때 set upconv layer에서 flow embedding layer에서 처럼 주변 feature들을 weight하는 방법을 학습을 하기 때문에 더 나은 효과를 보였다고 한다. Network Architecture Training loss with cycle-consistency regularization FlowNet3D의 학습에는 smooth $L_1$ loss(huber loss)를 사용하는데 여기에 cycle-consistency regularization을 적용했다고 한다. 여기서 forward flow를 $d_i$, backward flow를 $d_i^{\\prime}$, ground truth flow를 $d_i^*$라 하면 학습한 model을 이용해 forward flow와 원래의 point에 forward flow를 더한 point들에서 원래 point로 돌아가는 backward flow를 모두 구해서 loss를 다음과 같이 계산한다.\\[L(P,Q,D^*,\\Theta)=\\frac{1}{n_1}\\sum_{i=1}^{n_1}\\{||d_i-d_i^*||+\\lambda||d_i^\\prime+d_i||\\}\\] 위의 loss를 통해서 ground truth와 forward flow의 차이를 줄이는 동시에 forward flow로 옮겨진 point가 다시 원래대로 돌아오는 backward flow를 구하도록 학습을 한다. Inference with random re-sampling Inference를 할때 FlowNet3D를 적용하는 과정중에 발생하는 down-sampling이 prediction의 noise를 발생시킬 수 있어서 이를 해결하기 위해서 random하게 resampling해서 inference를 여러번하고 그 값의 평균을 flow로 출력한다고 한다. " }, { "title": "Stand-Alone Self-Attention in Visual Models 정리", "url": "/posts/stand_alone_self_attention/", "categories": "Model Reasoning", "tags": "Deep Learning, Vision", "date": "2019-06-09 00:00:00 +0900", "snippet": "paper: Stand-Alone Self-Attention in Visual ModelsAbstract현대 컴퓨터 비전에서 convolution은 fundamental building block으로 역할을 수행해 왔다. 최근 몇몇 연구에서 long range dependency 문제를 해결하기 위해 convolution을 넘어서야 한다는 주장들이 있었다. 이를 위해서 보통 self-attention과 같은 content-based interaction을 증가시키는 방식의 연구들이 진행되어 왔다. 여기서 한가지 의문은 attention이 convolution 위에 쓰이는 것이 아니라 Stand-Alone(독립적으로) 쓰일 수 있느냐이다. 본 논문에서 직접 pure self-attention vision model을 만들고 테스트한 결과 효과적인 Stand-Alone layer로 만들 수 있었다고 했다. Spatial convolution의 모든 요소들을 대체한 stand-alone self-attention은 기존의 attention 모델보다 더 적은 연산, 더 적은 parameter 수를 가지고 더 좋은 성능을 보였다고 한다. Introduction Dataset과 computing resource가 풍부해짐에 따라 CNN이 많은 컴퓨터 비전에 주요 backbone으로 쓰여왔고 convolution의 translation equivariance property가 CNN이 주류가 된 핵심이었다. 그러나 CNN은 단순히 주변 값과 고정된 값은 filter의 inner product 계산이어서 large receptive field에 대해 long range interaction이 scale이 커질수록 효율이 떨어진다고 했다.이 long range interaction에 대한 문제는 attention이라는 개념을 사용하여 주로 존재하는 CNN 모델에 global attention layer를 추가함으로써 해결해왔는데 이러한 global하게 적용하는 형태는 input의 모든 spatial location에 적용되므로 original image로부터 많은 downsampling을 해야 하는 small input에만 사용할 수 있는 제한이 있다.본 논문에서는 content-based interaction이 convolution에 추가되는 형태가 아닌 vision model의 primary primitive가 될 수 있는지에 대해 의문을 던졌다. 그래서 마지막 부분에 small input과 large input에 모두 사용할 수 있는 simple local attention layer를 만들었다고 한다. 이 stand-alone attention layer를 기초로 삼아 convolutional baseline보다 뛰어난 성능을 가진 fully attentional vision model도 만들었다고 한다. 더 나아가 stand-alone attention에 대해 이해하기 위해 여러 실험들을 더 진행해 보았다고 한다. Background Convolutions Convolution의 연산방식은 위의 그림으로 간단하게 정리할 수 있다. 하나의 pixel을 기준으로 주변에 있는 pixel의 값들과 학습된 filter의 weight를 inner product한 값으로 업데이트 하는 방식의 연산이다. 지금까지 machine learning은 수 많은 분야에서 이 convolution의 영향을 받아서 발전해 왔다. 이러한 convolution을 넘어서 새로운 방식을 찾기 위해 convolution을 재구성하는 여러 연구들도 진행이 되었었다. Self-Attention Attention은 variable length source의 information을 content-based summarization 할 수 있는 encoder-decoder의 형태로 적용되어 왔다. Attention은 context에서 중요한 region에 집중하는 법을 학습할 수 있어서 neural transduction model에서 중요한 요소가 되었다. Representation learning에서 recurrence가 self-attention으로 완전히 대체되어 주요한 메커니즘으로 사용 되었다. Self-attention의 정의는 single context에 적용된 attention이며 self-attention의 long-distance interaction과 parallelizability로 인해 다양한 task에서 SOTA의 성능을 보였다.그리고 몇몇 vision task에도 convolution에 self-attention을 추가하는 방식이 좋은 성능을 보였고 본 논문에서는 convolution을 없애고 전체 network에 local self-attention을 적용함으로써 이러한 방식을 넘어서고자 하였다. 이와 같은 생각이 다른 현존하는 연구 중에 존재했는데 local relation network라는 제목의 논문은 본 논문과 유사한 생각의 흐름에서 새로운 content-based layer를 제시 하였었고 본 연구팀이 제시한 현존하는 vision model에 사용되는 self-attention의 영향력에 집중한 연구와 상호보완적일 것이라 했다. Attention layer에 관한 연구는 기존의 존재하는 연구들을 재사용하여 간단하게 만드는 것에 초점이 맞춰져 있었는데 본 연구팀은 새로운 attention의 형태를 만들기 위해 이러한 convolution scheme을 버리고 대체하고자 했다. 이제부터 구체적으로 무엇을 했는지에 대해서 살펴보도록 하자. Stand-alone self-attention layer는 convolution과 유사하게 주어진 pixel을 기준으로 주변 local region의 pixel들을 뽑는다. 그리고 이를 memory block이라고 부른다. 이러한 형태의 local attention은 기존의 연구들이 모든 pixel에 대해서 global attention을 수행하여 attention영역을 찾는 방식과 차이가 있다. Global attention은 computationally expensive하기 때문에 크게 downsampling을 하여야만 사용할 수 있었다. 이 local attention의 pixel output을 계산하는 single-headed attention은 다음과 같이 계산되며 위의 왼쪽 그림에 표현되어 있다. query, key, value를 각각 q, k, v로 나타내고 각각의 값들은 pixel을 input으로 받아 linear transformation한 것이다.(query는 기준 pixel, key와 value는 주변 pixel) 그리고 softmax를 통해 주변의 모든 pixel에 대해서 계산된 logit을 내보낸다. Local self-attention model은 주변의 pixel의 spatial information을 합친다는 면에서 convolution과 유사하지만 aggregation 결과는 value vector와 weight(contents interaction을 나타낸)의 convex combination이 된다. 이 계산은 모든 pixel에 대해 적용되고 이 multiple attention head는 input의 여러 distinct representation을 학습하는데 쓰인다. 이 연산은 pixel feature를 N개의 channel로 나누어 각각의 그룹에 대해 single-headed attention을 각각 적용하고 output을 concatenate하여 크기를 보존한다. 지금까지 서술한 것에 따르`면 position에 대한 정보가 attention에 들어가 있지 않아서 permutation equivariant해지고 vision task에서의 표현의 다양성을 제한한다. 이미지에서의 pixel의 위치에(absolute position) 대해 sinusoidal embedding을 하는 방식도 있었으나 relative positional embedding으로 진행한 연구결과들이 더 성능이 좋았다. 2D relative position embedding을 통해 attention을 하는 대신에 relative attention을 사용하였다. 우선 기준 픽셀과 주변 픽셀 사이의 relative distance에 대해서 정의를 하면 위의 그림과 같이 dimension마다 계산하므로 row offset과 column offset에 대해 2개의 distance를 얻을 수 있다. Dimension이 2이기 때문에 각각 1/2크기의 dimension으로 embedding되며 결과는 concatenate된다. 결국은 relative distance에 대해서 그냥 query와 inner product한단 얘기 같다. 그래서 spatial-relative attention은 아래와 같이 정의된다. 기존의 term에 relative distance term을 더한 형태이다. 그러므로 logit은 query에 대해서 주변 pixel들과 거리가 가까운 정도와 어떤 content를 가졌느냐에 따른 similarity를 계산한 것이다. 여기서 self-attention에 infusing 함으로써 self-attention은 convolution과 유사하게 translation equivariance를 가지게 된다.Attention에 사용되는 parameter의 수는 크기와 독립적이지만 convolution은 quadratically 증가한다. 그리고 attention의 computational cost 또한 convolution에 비해 느리게 증가한다. 예를 들면 din = dout = 128일 때, spatial extent가 3인 convolution과 spatial extent가 19인 attention layer의 computational cost가 같았다. Fully Attentional Vision Models Local attention layer를 primitive로 사용할 때 어떻게 fully attentional architecture를 사용하는 가가 여기서 의문이다. 본 논문에서는 두 가지 스텝으로 이를 구현했다고 한다. Replacing Spatial Convolutions Spatial convolution은 spatial extent가 1보다 큰 convolution이라는 정의를 갖고 있다. 이는 각각의 pixel에 대해 독립적으로 적용되는 fully connected layer인 1 x 1 conv를 제외한 정의이다. 왜냐면 이 1 x 1 conv는 그냥 matrix multiplication이기 때문이다. 본 논문에서는 fully attentional vision model을 만들기 위해 간단한 전략을 탐색해 보았다. 기존의 convolutional architecture를 채택하여 모든 spatial convolution의 요소들을 attention layer로 대체하였다. Downsampling이 필요한 경우, 2 stride로 2 x 2 average pooling이 attention layer 다음에 쓰였다. 이 작업은 ResNet의 family architecture에 적용되었고 ResNet의 core building block은 1 x 1 down-projection conv, 3 x 3 spatial conv, 1 x 1 up-projection conv로 구성되었고 input block 과 output block은 residual connection이 붙어있는데 이게 곧 bottleneck block이다. 이 block은 ResNet에서 여러 번 반복된 형태로 구성되었고 이 block들 내부의 3 x 3 spatial conv를 전부 self-attention layer로 대체한다. 이를 제외한 layer의 수, 어디에 spatial downsampling이 적용되는 지와 같은 모든 다른 구조는 그대로 둔다. 이러한 변환은 매우 간단하지만 suboptimal 일 수 있다. Attention을 핵심 요소로 하여 architecture search와 같은 방법으로 architecture를 찾는다면 더 좋은 architecture를 찾을 수 있을 것이다. Replacing the Convolutional Stem CNN의 시작 layer를 stem이라고 부르기도 하는데, 뒤에 있는 layer들이 global object를 찾는데 사용하는 edge와 같은 local feature를 학습하는데 매우 중요한 역할을 한다. Input image가 크기 때문에 stem은 core block과 다르게 downsampling과 같은 lightweight operation에 초점을 맞추기도 한다. 예를 들면 ResNet에서 stem은 stride 2로 7 x 7 conv와 stride 2로 3 x 3 max pooling을 한다.Stem layer에서 pixel의 contents는 RGB이며 각각으로는 별 의미가 없으며 강하게 spatially correlated되어 있다. 이러한 특징이 self-attention 과 같은 content-based 메커니즘으로는 학습하기 어려운 edge detector와 같은 유용한 feature를 학습할 수 있게 한다. 본 논문의 실험에서 self-attention을 stem에 쓰는 것은 ResNet에서 convolution은 stem에 쓰는 것에 비해 더 안 좋은 성능을 보였다고 했다.Convolution에서 distance based weight parametrization은 higher layer에서 필요한edge detector를 비롯한 다른 local feature의 학습을 쉽게 할 수 있게 해주었다. Convolution과 self-attention 사이의 차이를 computation을 증가시키지 않으면서 좁히기 위해 spatially-varying한 linear transformation인 distance based information을 1 x 1 conv를 통해 추가하였다. 새로운 value transformation은 여러 개의 value matrix과 neighborhood pixel과 convex combination을 이루며 아래의 식으로 표현할 수 있다. 이러한 위치에 기반한 요소들은 pixel location에 dependent한 scalar weight를 학습한다는 점에서 convolution과 유사하며 이 stem은 이후에 max pooling을 하는 것으로 구성되어있다. 간단하게 말하면 attention receptive field는 max pooling window와 align 되어 있다. Experiments ImageNet에서 classification을 COCO dataset에서 object detection을 진행하였다. Where is stand-alone attention most useful? Fully attentional model의 인상적인 performance는 stand-alone attention이 vision model에서 대체 가능한 primitive라는 것을 보여줬다. 이 부분에서는 네트워크에서 어떤 부분에서 stand-alone attention이 가장 효과를 나타내는지 보고자 하였다. Stem 우선 attention stem와 conv stem을 ResNet에 적용한 것을 비교해 보았다. 모든 다른 spatial conv는 stand-alone attention으로 대체 되었다. Classification에서 conv stem이 attention stem보다 거의 유사하거나 좋은 성능을 보였다. Object detection에서 conv stem이 detection head와 FPN이 conv일 때 더 좋은 성능을 보였지만, 모든 부분들을 fully attentional 하게 해도 비슷한 성능이 나왔다. 이러한 결과는 conv가 여전히 stem에서는 더 효과적이라는 것을 보였다. Full Network 다음은 conv와 stand-alone attention을 conv stem을 적용한 ResNet의 다른 layer 그룹에 사용하였다. 가장 좋은 성능을 보인 모델은 conv를 앞부분, attention을 뒷부분에 구성한 것들 이었다. 이 모델들은 fully attentional model과 거의 유사한 FLOPS와 parameter를 가진다. 이와 반대로 attention이 앞부분에 쓰이고 convolution이 뒷부분에 쓰이면 parameter수는 증가하지만 성능은 감소하였다. 이는 conv가 low level feature를 더 잘 찾아내고 stand-alone attention은 global information을 합치는 데에 효과적이라는 것을 나타낸다.이 두 가지 결과는 conv와 stand-alone attention의 장점을 결합해서 architecture design을 해야 한다는 것을 보여준다. Which components are important in attention? 이 부분에서는 local attention layer의 여러 요소들의 contribution을 이해하고자 ablation study를 하였다. 별다른 언급이 없다면 모든 attention model은 conv stem을 사용하였다. Effect of spatial extent of self-attention Spatial extent k는 각각의 pixel이 주의를 기울이는 범위를 나타내는 것인데 위의 표는 그 효과를 나타낸다. k=3 과 같이 k가 작다면 performance를 오히려 저하시키고 성능향상은 k=11과 같이 큰 k 주변에서 최고점을 보인다. 최고점의 k값은 feature size나 attention head의 수와 같은 여러 hyperparameter에 따라서 달라진다. Importance of positional information 위의 표는 다른 타입의 positional encoding을 때와 positional encoding 하지 않았을 때의 결과를 담았다. 우선 positional encoding을 쓰는 것 자체는 쓰지 않은 것보다 좋은 성능을 보였고 relative position이 absolute position에 비해 더 좋은 성능을 보였다. 또한 content-content interaction을 나타내는(q • k)부분을 없애고 relative interaction만 사용하면 0.5%의 정확도만 떨어진다. Positional information은 중요성은 positional information의 새로운 용도나 parameterization을 찾는 연구를 통해서 attention의 성능을 향상시킬 수 있다는 것을 보여준다. Importance of spatially-aware attention stem 위의 표는 stem에서의 stand-alone attention에서 spatially-aware value를 쓴 것과 쓰지 않은 것을 비교하였고 나아가 value에 spatial convolution을 적용한 것을 비교하였다. Spatially-aware value를 사용한 것이 성능이 제일 좋았고 spatial conv를 적용한 것은 FLOPS수는 높아졌지만 성능은 오히려 약간 떨어졌다. Stem에 쓰이는 spatially-aware attention과 network안에서 쓰이는 attention을 하나로 통합하는 연구도 이루어 질 수 있을 거라 하였다. Discussion 본 논문에서는 content-based interaction이 vision model의 primary primitive의 역할을 할 수 있다는 것을 확인하였다. Stand-alone local self-attention layer로만 이루어진 fully attentional network가 적은 parameter수와 FLOPS로 더 좋은 성능을 낸다는 것도 보였다. 더 나아가 attention은 특히 후반부의 layer에서 효과적이라는 것도 보였다.또한 이 network의 성능 향상의 여지도 찾았다. 첫째는 우선 attention 메커니즘은 geometry를 더 잘 잡아내는 방법을 개발함으로써 향상시킬 수 있고 둘째로, 단순히 기존의 모델의 변형이 아닌 architecture search를 이용해 더 나은 architecture를 찾아서 향상시킬 수 있고 마지막으로, 앞부분의 layer에서 효과를 보여서 low level feature를 잘 잡아낼 수 있는 새로운 attention의 형태에 대해 추가적인 연구를 할 수 있다고 했다. Attention based architecture에서 training 효율과 computation에 관한 수요가 기존의 convolution에 맞춰져 있어서 결과 network가 실제로는 느리다. 이러한 차이가 생기는 이유는 다양한 하드웨어 가속기에 쓸 수 있는 optimize된 kernel이 없기 때문이다. 이 분야에서 attention이 성공적인 방법으로 여겨져서 가속화 프로그램들이 생긴다면 training과 inference의 실제 실행속도도 빨라질 것이다. 이 연구에서는 vision task에서 content-based interaction의 장점에만 초점을 맞췄는데 나중에 convolution과 self-attention의 각각의 고유한 장점을 결합하고 싶다고 했다. 이러한 content-based interaction의 성공으로부터 다른 cnn을 사용했던 vision task에 attention이 어떻게 적용될 수 있는지 추가적인 연구가 진행되는 것을 기대할 수 있었다. " }, { "title": "PointNet, PointNet++ 정리", "url": "/posts/pointnet_pointnetpp/", "categories": "Object Detection", "tags": "Pointcloud, Deep Learning", "date": "2018-12-06 00:00:00 +0900", "snippet": "PointNetpaper: PointNet: Deep Learning on Point Sets for 3D Classification and Segmentationcode: https://github.com/charlesq34/pointnetpoint cloud는 geometric data 구조중에서 중요한 type이지만 unordered라는 특성 때문에(좌표순처럼 sorting이 된것이 아닌 measurement 순서) image처럼 사용하기 위해서 기존에는 3D voxel grid형태의 data로 바꿔서 사용해왔다. 이러한 과정으로 인해 data 본래의 특성이 일부 소실될 가능성이 있어서 PointNet은 permutational invariance한 neural network를 제안해 point cloud를 input으로 직접 사용할 수 있도록 하였다. classification, segmentation 같이 여러 task에서도 좋은 성능을 보였다고 한다. 그리고 왜 이 network가 perturbation과 corruption에도 강건한지에 대한 ablation study도 진행하였다. Method Properties of Point Sets in $\\mathbb{R}^n$ Unordered: image에서의 pixel array와 다르게 point set은 특정한 order가 존재하지 않으므로 크기가 $N$인 point set이 있다면 $N!$ permutation에 대해서 모두 같은 결과가 나오도록 network를 구성해야 한다. Interaction among points: point들은 주변 point들과 묶여서 meaningful한 subset을 이루므로 network model이 주변 point들의 local structure, combinatorial interaction을 학습할 수 있어야 한다. Invariance under transformations: 단순히 rotating, translating된 point에 대해서 결과가 달라져서는 안된다. PointNet Architecture 전체 network구조는 위의 그림과 같으며 크게 3개의 key module로 이루어져 있다. 모든 point의 information을 통합하는 symmetric function으로 사용되는 max pooling layer, local &amp; global infomation을 통합하는 모듈, input point와 feature에 대해서 align을 하는 joint alignment network. Symmetry Function for Unordered Input model을 input의 permutation에 대해 invariant하게 하려면 input을 정해진 order로 sorting하는 방법, input을 sequence로 생각해서 RNN에 학습하며 모든 permutation을 고려하도록 augmentation을 해서 학습하는 방법, information을 통합하기 위해서 simple symmetric function을 사용하는 방법이 있다. 첫번째 방법이 쉬워보이지만 high dimensional space에서는 ordering이 perturbation에 대해서 stable하지 않으며 두번째 방법은 결국 RNN이 input order에 invariant하도록 학습이 될거라는 기대하에 제안된 아이디어지만 OrderMatters라는 논문에 실제로 이 order를 무시를 할 수 없다는 것을 보였는데 결국은 RNN이 작은 수의 input에 대해서는 어느정도 robustness를 보여줄 수 있지만 point cloud처럼 수만개의 data에 대해서는 좋은 성능을 보여줄수 없었다. 그래서 본 논문에서는 symmetric function을 사용하였다. 본 논문에서 찾고자 하는 symmetric function을 general하게 나타내면 다음과 같다.\\[f(\\{x_1,...,x_n)\\}\\approx g(h(x_1),...,h(x_n)) \\\\ f:2^{\\mathbb{R}^N} \\rightarrow \\mathbb{R},\\ h:\\mathbb{R}^N\\rightarrow \\mathbb{R}^K,\\ g:\\mathbb{R}^K\\times\\cdots\\times\\mathbb{R}^K\\rightarrow\\mathbb{R}\\] 실제로 위의 수식을 통해 본 논문에서 사용하고자 하는 module을 설명하면 매우 간단하다. 여기서 $h$가 multi-layer perceptron network, $g$가 max pooling function이 된다. $h$의 collection의 max pooling이 point set의 특징을 잘 나타내도록 학습하는 것이다. Local and Global Information Aggregation Segmentation 문제에서는 point의 local과 global 정보의 combination이 필요하다. 그래서 이 모듈을 도입하였는데 위의 그림에도 나와있듯이 중간 layer에 있는 local feature에 point마다 global feature를 그저 concatenate함으로써 Joint Alignment Network point cloud에 transformation이 일어나도 target task의 결과가 달라져서는 안 되는데 이를 위해서 T-net이라는 mini network를 사용해 affine transformation matrix를 predict하고 이 transformation을 input point에 적용하는 것으로 이 문제를 해결하였다고 한다. 이 T-net의 구조는 max pooling과 fc layer로 이루어져있고 feature extraction network와 닮아있다. 그리고 이 alignment를 euclidean space, feature space에 둘 다 적용한다. 근데 feature space에서의 transformation은 feature의 dimension이 커지면 optimization이 매우 어려워지므로 orthogonal transformation이 input의 information을 잘 보존하므로 이 방향으로 학습하기 위해 feature transformation matrix가 orthogonal matrix에 가까워지도록 loss에 다음과 같은 regularization term을 추가한다.\\[L_{reg}=||I-AA^T||^2_F\\] 여기서 $A$가 T-net으로 predict하는 feature alignment matrix이다. Theoretical Analysis Universal approximation 이 부분은 논문에서 제시한 neural nwtwork의 continuous set function에 대한 approximation ability에 대해서 알아보는 부분이다. 우선 직관적으로 set function의 continuity로 인해 작은 perturbation은 function value에 큰 변화를 일으키지 않을 것이라 생각할 수 있다. 이를 수식으로 표현하면 Theorem 1. 만약 $f:X\\rightarrow\\mathbb{R}$이 Hausdorff distance $d_H(\\cdot,\\cdot)$에 대해 continuous set function이면 $S\\in X$에 대해 $\\forall\\epsilon&gt;0$ , $\\exists$ continuous function $h$, symmetric function $g(x_1,…,x_n)=\\gamma\\circ\\max$ 이다.\\[|f(S)-\\gamma(\\max\\limits_{x_i\\in S}\\{h(x_i)\\})|&lt;\\epsilon\\] 여기서 max는 vector의 element-wise maximum 이다. 결국에는 완벽한 continuous set function $f$와 bounded error를 가진 function을 neural network를 통해서 구할 수 있다는 말이다. Bottleneck dimension and stability PointNet의 expressiveness는 max pooling layer의 dimension $K$에 크게 영향을 받는다고 한다. 그래서 이론적으로 model의 stability에 영향을 끼치는 특징들에 대해서 분석을 했다고 한다. Thoerem 2. 만약 $\\mathbb{u}:X\\rightarrow\\mathbb{R}^K, \\mathbb{u}=\\max\\limits_{x_i\\in S}{h(x_i)}, f=\\gamma\\circ\\mathbb{u}$라 하면\\[(a)\\ \\forall S,\\exists C_S,N_S\\subseteq X,f(T)=f(S)\\ ifC_S\\subseteq T\\subseteq N_S \\\\ (b)\\ |C_S|\\leq K\\] (a)는 $f(S)$에 실제로 영향을 주는 point들 $C_s$만 보존되면 $f(S)$가 $N_S$까지 noise가 추가되어도 변하지 않는다는 의미로 corruption에도 robust하다는 것을 보여주는 것이고 (b)는 $f(S)$를 결정하는 ciritical point set $C_S$가 max pooling output dimension $K$에 의해 bounded된다는 것이므로 $K$를 bottleneck dimenstion이라고 부른다고 한다. 이 두 theorem들을 통해 model이 perturbation, corruption에 어느정도 robust하다는 것을 보였으며 이를 통해 pointnet이 sparse한 key point의 set으로 shape를 summarize하는 것을 잘 학습하였다는 것을 직관적으로 알 수 있다. PointNet++paper: PointNet++: Deep Hierarchical Feature Leaning on Point Sets in a Metric Spacecode: https://github.com/charlesq34/pointnet2PointNet++는 PointNet 저자의 후속 논문으로 PointNet이 point가 존재하는 metric space의 정보를 local sturcture를 학습할 때 반영하지 못한 점을 보완하여 작은 pattern에 대한 인식 능력이나 complex scene에 대한 generalizability를 높이고자 하였다. 이 논문에서는 이를 input point set을 nested partitioning을 하고 이 구조에 PointNet을 recursive하게 적용하는 hierarchical neural network를 도입하여 해결하였다. metric space의 distance 정보를 활용했기에 이 논문에서는 contextual scaling이 향상된 local feature를 학습할 수 있다고 하며 density에 invariant하도록 하기 위해 여러 scale의 feature들을 adaptive하게 결합하기 위해서 새로운 set learning layer도 제안하였다고 한다.이 논문의 주요한 contribution은 다음과 같다. 여러 scale에서의 neighborboods를 이용해 robustnest와 detail한 학습을 이뤄냈다고 한다. Training과정에서 random input dropout을 통해 network가 adaptive하게 다양한 scale에서 얻은 pattern에 weight을 주고 결합하도록 학습시켰다. Method Hierarchical Point Set Feature Learning PointNet은 전체 point set을 통합하기 위해서 한번만 max pooling을 했던 반면 이 논문에서는 point를 hierarchical하게 grouping을 하고 이 hierarchy에 따라서 점점 넓은 영역의 feature를 추상화한다. Hierarchical structure는 여러개의 set abstraction 층으로 구성되어 있고 각각의 층은 세개의 key layer로 구성된다. 그리고 set abstraction의 각 level은 $N\\times(d+C)$의 matrix를 input으로 한다. 여기서 $d$는 point가 존재하는 space의 coordinate의 dimension, $C$는 feature space의 dimension이다. output으로는 $N^{\\prime}\\times(d+C^{\\prime})$의 matrix 형태를 가진다. Sampling layer 말그대로 point를 sampling하는 layer로 farthest point sampling(FPS)를 사용해 최대한 point들이 서로 멀리 떨어져 있도록 뽑는다. FPS를 사용하면 전체 point set에서 골고루 뽑게 되어서 receptive field를 만들어내는 효과도 보인다. Grouping layer Input data는 $N\\times(d+C)$형태의 data와 sampling한 point의 수가 $N^\\prime$이므로 sampling된 point의 set인 $N^\\prime\\times d$ 형태의 data이며 output data의 형태는 sampled point의 특정 distance 이내의 $K$개의 주변 point로 group을 만들어 $N^\\prime\\times K\\times(d+C)$가 되고 $K$는 group마다 상이할 수 있다. PointNet layer Input data는 $N^\\prime\\times K\\times(d+C)$형태의 data가 들어오고 각각의 centroid에 대해서 feature를 구해 $N^\\prime \\times(d+C^\\prime)$형태의 output data를 얻을 수 있다. 이를 위해서 PointNet을 사용하였으며 centroid point를 중심으로 coordinate를 옮긴 후에 network에 input으로 넣어준다. 이렇게 relative coordinate를 사용해 local region에서 point들간의 관계를 학습하고자 하였다. Robust Feature Learning under Non-Uniform Sampling Density Point set의 feature를 학습하는데에 있어서 멀리있는 물체와 가까이 있는 물체의 point의 density가 다른 문제(non uniformity)는 매우 중요한 문제다. 이상적으로는 point가 dense한 region에서는 매우 상세한 feature들을 학습할 수 있지만 sparse한 영역에서는 sampling 된 point가 부족하다보니 local pattern 자체가 corrupt 되어있다. 이런 경우에는 보다 넓은 영역에서의 pattern을 찾아봐야하는데 이렇게 density에 따라서 adaptive하게 학습하게 하기 위한 point layer를 도입하였다. 이렇게 density adaptive PointNet layer를 가진 hierarchical network를 PointNet++라고 부른다고 한다. 이 논문에서는 grouping하는 local region의 관점에서와 여러 scale에서 온 feature를 합치는 관점에서 두 가지 형태의 density adaptive layer를 제안하였다. Multi-scale grouping (MSG) MSG에서는 여러 scale의 pattern을 얻기 위해서 위의 왼쪽 그림과 같이 다양한 크기로 grouping하는 layer를 거친 이후에 각 scale의 feature를 뽑기 위해 PointNet을 적용하였고 그 후에 concatenate한다. 그리고 이 multi-scale feature를 잘 학습하기 위해서 random input dropout을 사용해 다양한 density와 nonuniformity한 데이터에 대해서도 학습하였다. Multi-resolution grouping (MRG) MSG는 모든 centroid point에 대해서 넒은 영역의 neighborhood를 input으로 한PointNet을 사용하기 때문에 computationally expensive하다. 그래서 computation cost를 낮추면서 adaptive하게 information을 합치는 성능을 보존하기 위해 MRG를 제안했다. 위의 오른쪽 그림과 같이 level $L_i$ feature의 형태가 두 개의 vector가 concatenate된 것을 볼 수 있다. 여기서 왼쪽 vector는 이전 level $L_{i-1}$에서 온 feature들을 summarize한다는데 이 부분은 어떻게 하는지는 코드에도 없고 나와있지를 않아서 모르겠다 아마 단순히 concatenate한 것이 아닌가 싶은 생각이 든다. 그리고 오른쪽 vector는 해당 local region내에 있는 모든 point에 대해서 PointNet을 사용해 feature를 뽑은 것이다. 그래서 local region의 density가 낮으면 앞의 vector가 뒤의 vector에 비해 신뢰도가 낮아지므로 뒤의 vector에 weight를 더 주고 반대의 경우는 앞의 vector에 weight를 더 준다고 한다. MRG가 MSG에 비해서 낮은 level에서 large scale의 neighborhood에 대해서 feature extraction을 할때 특히 computationally efficient하다고 한다. Point Feature Propagation for Set Segmentation Set abtraction layer에서 원래 point가 subsample 되는데 segmentation task에서는 모든 point의 label을 얻어야 하기 때문에 이를 위해서는 그냥 모든 point를 사용하는 방법과 subsample된 point에서 original point들로 feature를 propagation을 하는 방법이 있다고 한다. 전자는 당연히 cost가 매우 크기 때문에 이 논문에서는 후자로 해결을 하였다. 이 propagation은 distance 기반으로 level간에 interpolation을 하는 방법인데 feature propagation level에서는 level $l-1$에 있는 point들의 feature를 level $l$에 있는 point들을 이용해 $k$NN 기반의 distance weighted interpolation을 한다. 그리고 이 feature와 set abstraction level의 feature를 skip link로 가져와서 concatenate한다. 이 feature에 1$\\times$1 convolution과 유사한 unit pointnet을 사용해 기존 feature와 interpolated 된 feature를 합친다.\\[f^{(j)}(x)=\\frac{\\sum_{i=1}^kw_i(x)f^{(j)}_i}{\\sum_{i=1}^kw_i(x)},\\ where \\ w_i(x)=\\frac{1}{d(x,x_i)^p},\\ j=1,...,C\\] " } ]
